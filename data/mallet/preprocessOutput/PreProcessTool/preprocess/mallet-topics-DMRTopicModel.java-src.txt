topics optimize limited memory b f g s optimize optimization types classify max ent pipe pipe pipe noop gnu trove t hash map io i o io print stream io d m r topic model l d a hyper max ent dmr = num features feature index pipe parameter pipe = alpha cache alpha sum cache d m r topic model number topics number topics estimate iterations round i o num features = data get 0 instance get target alphabet size + 1 feature index = num features 1 num docs = data size t o d o consider beginning sub sampling? alpha cache = num docs num topics alpha sum cache = num docs start time = current time millis max iteration = iterations so far + iterations round iterations so far <= max iteration iterations so far++ iteration start = current time millis show topics interval != 0 iterations so far != 0 iterations so far % show topics interval == 0 out print top words out words per topic save state interval != 0 iterations so far % save state interval == 0 print state state filename + + iterations so far + gz iterations so far > burnin period optimize interval != 0 iterations so far % optimize interval == 0 train regression learn loop over every document in corpus doc = 0 doc < num docs doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence dmr != set appropriate alpha set alphas data get doc instance sample topics one doc token sequence topic sequence ms = current time millis iteration start ms > 1000 out print math round ms 1000 + s out print ms + ms iterations so far % 10 == 0 out < + iterations so far + > print log likelihood out model log likelihood out flush seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds use only features to set topic prior use no document features set alphas = dmr get alpha sum = 0 0 smoothing only mass = 0 0 use only features to set topic prior use no document features topic=0 topic < num topics topic++ alpha topic = math exp topic num features + feature index alpha sum += alpha topic smoothing only mass += alpha topic beta tokens per topic topic + beta sum cached coefficients topic = alpha topic tokens per topic topic + beta sum sets alphas a hypothetical document that contains a single non feature set alphas feature index = dmr get alpha sum = 0 0 smoothing only mass = 0 0 use only features to set topic prior use no document features topic=0 topic < num topics topic++ alpha topic = math exp topic num features + feature index + topic num features + feature index alpha sum += alpha topic smoothing only mass += alpha topic beta tokens per topic topic + beta sum cached coefficients topic = alpha topic tokens per topic topic + beta sum set alpha based on features in an instance set alphas instance instance we can t use standard score functions from max ent since our features are currently in target feature vector features = feature vector instance get target features == set alphas = dmr get alpha sum = 0 0 smoothing only mass = 0 0 topic = 0 topic < num topics topic++ alpha topic = topic num features + feature index + matrix ops row dot product num features topic features feature index alpha topic = math exp alpha topic alpha sum += alpha topic smoothing only mass += alpha topic beta tokens per topic topic + beta sum cached coefficients topic = alpha topic tokens per topic topic + beta sum learn create a fake pipe features in data and a trove hashmap topic counts in target parameter pipe == parameter pipe = noop parameter pipe set data alphabet data get 0 instance get target alphabet parameter pipe set target alphabet topic alphabet instance list parameter instances = instance list parameter pipe dmr == dmr = max ent parameter pipe num features num topics doc=0 doc < data size doc++ data get doc instance get target == feature counter counter = feature counter topic alphabet topic data get doc topic sequence get features counter increment topic put real target in data field and topic counts in target field parameter instances add instance data get doc instance get target counter to feature vector d m r optimizable optimizable = d m r optimizable parameter instances dmr optimizable set regular gaussian prior variance 0 5 optimizable set intercept gaussian prior variance 100 0 limited memory b f g s optimizer = limited memory b f g s optimizable optimize once optimizer optimize optimization e step size too small restart a fresh initialization to improve likelihood optimizer optimize optimization e step size too small dmr = optimizable get classifier doc=0 doc < data size doc++ instance instance = data get doc instance feature sequence tokens = feature sequence instance get data instance get target == num tokens = tokens get length sets alpha and alpha sum set alphas instance now cache alpha values topic=0 topic < num topics topic++ alpha cache doc topic = alpha topic alpha sum cache doc = alpha sum print top words print stream out num words using lines dmr != set alphas print top words out num words using lines write parameter i o dmr != print stream out = print stream parameter dmr print out out close serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 i o instance list training = instance list load 0 num topics = length > 1 ? parse 1 200 instance list testing = length > 2 ? instance list load 2 d m r topic model lda = d m r topic model num topics lda set optimize interval 100 lda set topic display 100 10 lda add instances training lda estimate lda write dmr lda print state dmr state gz 