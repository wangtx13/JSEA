2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e author aron culotta <a href= mailto culotta >culotta edu< a> limited memory b f g s described in byrd nocedal and schnabel representations quasi newton matrices and their use in limited memory optimize logging linked list optimize back track line search optimize line optimizer optimize optimizable types matrix ops logger limited memory b f g s optimizer logger logger = logger get logger base ml maximize limited memory b f g s converged = optimizable gradient value optimizable max iterations = 1000 xxx need a more principled stopping point tolerance = 0001 tolerance = 0001 gradient tolerance = 001 eps = 1 0e 5 number corrections used in b f g s update ideally 3 <= m <= 7 larger m means more cpu time memory m = 4 line search function line optimizer gradient line maximizer limited memory b f g s optimizable gradient value function optimizable = function line maximizer = back track line search function optimizable get optimizable optimizable converged converged sets line optimizer gradient to use in l b f g s optimization line opt line optimizer l b f g s set line optimizer line optimizer gradient line opt line maximizer = line opt state search g = gradient s = list m previous values y = list m previous g values rho = intermediate calculation g oldg direction old linked list s = linked list linked list y = linked list linked list rho = linked list alpha step = 1 0 iterations optimizer evaluator gradient eval = p a l added set tolerance newtol tolerance = newtol set evaluator optimizer evaluator gradient eval eval = eval get iteration iterations optimize optimize m a x v a l u e optimize num iterations initial value = optimizable get value logger fine entering l b f g s optimize initial value= +initial value g == first time through logger fine first time through l b f g s iterations = 0 s = linked list y = linked list rho = linked list alpha = m i=0 i<m i++ alpha i = 0 0 = optimizable get num old = optimizable get num g = optimizable get num oldg = optimizable get num direction = optimizable get num optimizable get arraycopy 0 old 0 length optimizable get value gradient g arraycopy g 0 oldg 0 g length arraycopy g 0 direction 0 g length matrix ops abs normalize direction == 0 logger info l b f g s initial gradient zero saying converged g = converged = logger fine direction 2norm + matrix ops two norm direction matrix ops times equals direction 1 0 matrix ops two norm direction make initial jump logger fine before initial jump direction 2norm + matrix ops two norm direction + gradient 2norm + matrix ops two norm g + 2norm + matrix ops two norm test maximizable test value and gradient in direction maxable direction step = line maximizer optimize direction step step == 0 0 could not step in direction give up and say converged g = reset search step = 1 0 optimization line search could not step in current direction + not necessarily cause alarm sometimes happens close to maximum + where function may be very flat optimizable get optimizable get value gradient g logger fine after initial jump direction 2norm + matrix ops two norm direction + gradient 2norm + matrix ops two norm g iteration count = 0 iteration count < num iterations iteration count++ value = optimizable get value logger fine l b f g s iteration= +iteration count + value= +value+ g two norm + matrix ops two norm g + oldg two norm + matrix ops two norm oldg get difference between previous 2 gradients and sy = 0 0 yy = 0 0 i=0 i < old length i++ inf inf = 0 inf inf = 0 infinite i infinite old i i old i > 0 old i = 0 0 old i = i old i infinite g i infinite oldg i g i oldg i > 0 oldg i = 0 0 oldg i = g i oldg i sy += old i oldg i si yi yy += oldg i oldg i direction i = g i sy > 0 invalid optimizable sy = +sy+ > 0 gamma = sy yy scaling factor gamma > 0 invalid optimizable gamma = +gamma+ > 0 push rho 1 0 sy these arrays are now differences between and gradient push s old push y oldg s size == y size s size + s size + y size + y size next section where we calculate direction first work backwards from most recent difference vectors i = s size 1 i >= 0 i alpha i = rho get i value matrix ops dot product s get i direction matrix ops plus equals direction y get i 1 0 alpha i scale direction ratio s y and y y matrix ops times equals direction gamma now work forwards from oldest to newest difference vectors i = 0 i < y size i++ beta = rho get i value matrix ops dot product y get i direction matrix ops plus equals direction s get i alpha i beta move current values to last iteration buffers and negate search direction i=0 i < oldg length i++ old i = i oldg i = g i direction i = 1 0 logger fine before linesearch direction gradient dotprod + matrix ops dot product direction g + direction 2norm + matrix ops two norm direction + 2norm + matrix ops two norm test whether gradient ok test maximizable test value and gradient in direction maxable direction a line search in current direction step = line maximizer optimize direction step step == 0 0 could not step in direction g = reset search step = 1 0 xxx temporary test passed o k test maximizable test value and gradient in direction maxable direction optimization line search could not step in current direction + not necessarily cause alarm sometimes happens close to maximum + where function may be very flat optimizable get optimizable get value gradient g logger fine after linesearch direction 2norm + matrix ops two norm direction value = optimizable get value test terminations 2 0 math abs value value <= tolerance math abs value + math abs value + eps logger info exiting l b f g s on termination #1 value difference below tolerance old value + value + value + value converged = gg = matrix ops two norm g gg < gradient tolerance logger fine exiting l b f g s on termination #2 gradient= +gg+ < +gradient tolerance converged = gg == 0 0 logger fine exiting l b f g s on termination #3 gradient==0 0 converged = logger fine gradient = +gg iterations++ iterations > max iterations err too many iterations in l b f g s continuing current converged = illegal state too many iterations end iteration call evaluator eval != ! eval evaluate optimizable iteration count logger fine exiting l b f g s on termination #4 evaluator converged = resets previous gradients and values that are used to approximate hessian n o t e link optimizable modified externally should be called to avoid illegal state exceptions reset g = pushes a onto queue l l linked list queue matrix obj s toadd matrix to push onto queue push linked list l toadd l size <= m l size == m remove oldest matrix and add newest to end list to make more efficient actually overwrite memory oldest matrix overwrites oldest matrix last = l get 0 arraycopy toadd 0 last 0 toadd length ptr = last readjusts pointers in list i=0 i<l size 1 i++ l set i l get i+1 l set m 1 ptr = toadd length arraycopy toadd 0 0 toadd length l add last pushes a onto queue l l linked list queue obj s toadd value to push onto queue push linked list l toadd l size <= m l size == m pop old and add l remove first l add last toadd l add last toadd 