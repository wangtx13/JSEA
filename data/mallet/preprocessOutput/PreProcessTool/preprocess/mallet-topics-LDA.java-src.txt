2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics arrays io types utils randoms latent dirichlet allocation author andrew mc callum deprecated use parallel topic model instead think about support incrementally adding more documents i think means we might want to use feature sequence directly we will also need to support a growing vocabulary!public l d a serializable num topics number topics to be fit alpha dirichlet alpha alpha over topics beta prior on per topic multinomial over words t alpha v beta instance list ilist data field instances expected to hold a feature sequence topics indexed <document index sequence index> num types num tokens doc topic counts indexed <document index topic index> type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> l d a number topics number topics 50 0 0 01 l d a number topics alpha sum beta num topics = number topics alpha = alpha sum num topics beta = beta estimate instance list documents num iterations show topics interval output model interval output model filename randoms r ilist = documents shallow clone num types = ilist get data alphabet size num docs = ilist size topics = num docs doc topic counts = num docs num topics type topic counts = num types num topics tokens per topic = num topics t alpha = alpha num topics v beta = beta num types start time = current time millis initialize random assignments tokens to topics and finish allocating topics and tokens topic seq len feature sequence fs di = 0 di < num docs di++ fs = feature sequence ilist get di get data cast e err l d a and other topic models expect feature sequence data not feature vector data + text2vectors you can obtain such data keep sequence or keep bisequence e seq len = fs get length num tokens += seq len topics di = seq len randomly assign tokens to topics si = 0 si < seq len si++ topic = r next num topics topics di si = topic doc topic counts di topic ++ type topic counts fs get index at position si topic ++ tokens per topic topic ++ estimate 0 num docs num iterations show topics interval output model interval output model filename r 124 5 seconds 144 8 seconds after using feature sequence instead tokens 121 6 seconds after putting on feature sequence get index at position 106 3 seconds after avoiding lookup in inner loop a temporary variable add documents instance list additional documents num iterations show topics interval output model interval output model filename randoms r ilist == illegal state must already have some documents first instance inst additional documents ilist add inst ilist get data alphabet == additional documents get data alphabet additional documents get data alphabet size >= num types num types = additional documents get data alphabet size num docs = additional documents size num old docs = topics length num docs = num old docs+ num docs expand various arrays to make space data topics = num docs i = 0 i < topics length i++ topics i = topics i topics = topics rest will be initialized below doc topic counts = num docs num topics i = 0 i < doc topic counts length i++ doc topic counts i = doc topic counts i doc topic counts = doc topic counts rest will be initialized below type topic counts = num types num topics i = 0 i < type topic counts length i++ j = 0 j < num topics j++ type topic counts i j = type topic counts i j further populated below feature sequence fs di = num old docs di < num docs di++ fs = feature sequence additional documents get di num old docs get data cast e err l d a and other topic models expect feature sequence data not feature vector data + text2vectors you can obtain such data keep sequence or keep bisequence e seq len = fs get length num tokens += seq len topics di = seq len randomly assign tokens to topics si = 0 si < seq len si++ topic = r next num topics topics di si = topic doc topic counts di topic ++ type topic counts fs get index at position si topic ++ tokens per topic topic ++ perform several rounds gibbs sampling on documents in given range estimate doc index start doc index length num iterations show topics interval output model interval output model filename randoms r start time = current time millis iterations = 0 iterations < num iterations iterations++ iterations % 10 == 0 out print iterations out print out flush show topics interval != 0 iterations % show topics interval == 0 iterations > 0 out print top words 5 output model interval != 0 iterations % output model interval == 0 iterations > 0 write output model filename+ +iterations sample topics docs doc index start doc index length r seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds one iteration gibbs sampling across all documents sample topics all docs randoms r topic weights = num topics loop over every word in corpus di = 0 di < topics length di++ sample topics one doc feature sequence ilist get di get data topics di doc topic counts di topic weights r one iteration gibbs sampling across all documents sample topics docs start length randoms r start+length <= doc topic counts length topic weights = num topics loop over every word in corpus di = start di < start+length di++ sample topics one doc feature sequence ilist get di get data topics di doc topic counts di topic weights r assign topics test tokens random r test topics = test tokens length test topic counts = num topics num tokens = matrix ops sum test tokens topic weights = num topics randomly assign topics to words and incorporate document in global counts topic si = 0 si < test tokens length si++ topic = r next num topics test topics si = topic analogous to topics test topic counts topic ++ analogous to doc topic counts type topic counts test tokens si topic ++ tokens per topic topic ++ repeatedly sample topic assignments words in document iterations = 0 iterations < num tokens 2 iterations++ sample topics one doc test tokens test topics test topic counts topic weights r remove document from global counts and also fill topic weights an unnormalized over topics whole doc arrays fill topic weights 0 0 si = 0 si < test tokens length si++ topic = test topics si type topic counts test tokens si topic tokens per topic topic topic weights topic ++ normalize over topics whole doc ti = 0 ti < num topics ti++ topic weights ti = test tokens length topic weights sample topics one doc feature sequence one doc tokens one doc topics indexed seq position one doc topic counts indexed topic index topic weights randoms r current type topic counts type old topic topic topic weights sum doc len = one doc tokens get length tw iterate over positions words in document si = 0 si < doc len si++ type = one doc tokens get index at position si old topic = one doc topics si remove token from all counts one doc topic counts old topic type topic counts type old topic tokens per topic old topic build a over topics token arrays fill topic weights 0 0 topic weights sum = 0 current type topic counts = type topic counts type ti = 0 ti < num topics ti++ tw = current type topic counts ti + beta tokens per topic ti + v beta one doc topic counts ti + alpha doc len 1+t alpha constant across all topics topic weights sum += tw topic weights ti = tw sample a topic assignment from topic = r next discrete topic weights topic weights sum put that topic into counts one doc topics si = topic one doc topic counts topic ++ type topic counts type topic ++ tokens per topic topic ++ get doc topic counts doc topic counts get type topic counts type topic counts get tokens per topic tokens per topic print top words num words use lines word prob comparable wi p word prob wi p wi = wi p = p compare to o2 p > word prob o2 p 1 p == word prob o2 p 0 1 word prob wp = word prob num types ti = 0 ti < num topics ti++ wi = 0 wi < num types wi++ wp wi = word prob wi type topic counts wi ti tokens per topic ti arrays sort wp use lines out topic +ti i = 0 i < num words i++ out ilist get data alphabet lookup wp i wi to + + wp i p out print topic +ti+ i = 0 i < num words i++ out print ilist get data alphabet lookup wp i wi to + out print document topics f i o print document topics print writer writer f print document topics print writer pw print document topics pw 0 0 1 print document topics print writer pw threshold max pw #doc source topic proportion doc len topic dist = topics length di = 0 di < topics length di++ pw print di pw print ilist get di get source != pw print ilist get di get source to pw print source pw print doc len = topics di length ti = 0 ti < num topics ti++ topic dist ti = doc topic counts di ti doc len max < 0 max = num topics tp = 0 tp < max tp++ maxvalue = 0 maxindex = 1 ti = 0 ti < num topics ti++ topic dist ti > maxvalue maxvalue = topic dist ti maxindex = ti maxindex == 1 || topic dist maxindex < threshold pw print maxindex+ +topic dist maxindex + topic dist maxindex = 0 pw print state f i o print writer writer = print writer writer f print state writer writer close print state print writer pw alphabet a = ilist get data alphabet pw #doc pos typeindex type topic di = 0 di < topics length di++ feature sequence fs = feature sequence ilist get di get data si = 0 si < topics di length si++ type = fs get index at position si pw print di pw print pw print si pw print pw print type pw print pw print a lookup type pw print pw print topics di si pw write f output stream oos = output stream output stream f oos write oos close i o e err writing + f + + e serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write ilist out write num topics out write alpha out write beta out write t alpha out write v beta di = 0 di < topics length di ++ si = 0 si < topics di length si++ out write topics di si di = 0 di < topics length di ++ ti = 0 ti < num topics ti++ out write doc topic counts di ti fi = 0 fi < num types fi++ ti = 0 ti < num topics ti++ out write type topic counts fi ti ti = 0 ti < num topics ti++ out write tokens per topic ti read input stream in i o not found features length = in read ilist = instance list in read num topics = in read alpha = in read beta = in read t alpha = in read v beta = in read num docs = ilist size topics = num docs di = 0 di < ilist size di++ doc len = feature sequence ilist get di get data get length topics di = doc len si = 0 si < doc len si++ topics di si = in read doc topic counts = num docs num topics di = 0 di < ilist size di++ ti = 0 ti < num topics ti++ doc topic counts di ti = in read num types = ilist get data alphabet size type topic counts = num types num topics fi = 0 fi < num types fi++ ti = 0 ti < num topics ti++ type topic counts fi ti = in read tokens per topic = num topics ti = 0 ti < num topics ti++ tokens per topic ti = in read instance list get instance list ilist recommended to use bin vectors2topics instead i o instance list ilist = instance list load 0 num iterations = length > 1 ? parse 1 1000 num top words = length > 2 ? parse 2 20 out data loaded l d a lda = l d a 10 lda estimate ilist num iterations 50 0 randoms should be 1100 lda print top words num top words lda print document topics 0 + lda 