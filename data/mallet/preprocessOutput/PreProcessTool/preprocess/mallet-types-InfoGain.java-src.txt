2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e gain absence precense each feature note that we aren t attending to feature s value and m a l l e t doesn t currently have any support at all categorical features author andrew mc callum <a href= mailto >mccallum edu< a> types info gain ranked feature vector xxx d i s g u s t i n g l y non thread safe base entropy label vector base label xxx yuck figure out how to remove not strictly a list feature info gains but convenient and efficient ml classify decision tree base entropy label vector base label calc info gains instance list ilist log2 = math log 2 num instances = ilist size num classes = ilist get target alphabet size num features = ilist get data alphabet size infogains = num features target feature count = num classes num features feature count sum = num features target count = num classes target count sum = 0 flv feature location value fli feature location index count populate target feature count et al i = 0 i < ilist size i++ instance inst = ilist get i labeling labeling = inst get labeling feature vector fv = feature vector inst get data instance weight = ilist get instance weight i below relies on label weights summing to 1 over all labels! label weight sum = 0 ll = 0 ll < labeling num locations ll++ = labeling index at location ll label weight = labeling value at location ll label weight sum += label weight label weight == 0 count = label weight instance weight fl = 0 fl < fv num locations fl++ fli = fv index at location fl xxx right? what should we about negative values? whatever decided here should also go in decision tree split fv value at location fl > 0 target feature count fli += count feature count sum fli += count target count += count target count sum += count math abs label weight sum 1 0 < 0 0001 target count sum == 0 base entropy = 0 0 xxx should instead infinite? base label = label vector label alphabet ilist get target alphabet target count infogains target count sum > 0 target count sum p = num classes calculate overall entropy labels ignoring features base entropy = 0 out print target count vector print target count out target count sum = +target count sum = 0 < num classes li++ p = target count target count sum = p p <= 1 0 p p != 0 base entropy = p math log p log2 base label = label vector label alphabet ilist get target alphabet out total entropy = +static base entropy calculate info gain each feature fi = 0 fi < num features fi++ feature present entropy = 0 norm = feature count sum fi norm > 0 = 0 < num classes li++ p = target feature count fi norm p <= 1 00000001 p p != 0 feature present entropy = p math log p log2 ! na n feature present entropy fi norm = target count sum feature count sum fi feature absent entropy = 0 norm > 0 = 0 < num classes li++ p = target count target feature count fi norm p <= 1 00000001 p p != 0 feature absent entropy = p math log p log2 ! na n feature absent entropy fi alphabet dictionary = ilist get data alphabet out feature= +dictionary lookup symbol fi + present weight= + feature count sum fi target count sum + absent weight= + target count sum feature count sum fi target count sum + present entropy= +feature present entropy+ absent entropy= +feature absent entropy infogains fi = base entropy feature count sum fi target count sum feature present entropy target count sum feature count sum fi target count sum feature absent entropy ! na n infogains fi fi infogains info gain instance list ilist ilist get data alphabet calc info gains ilist base entropy = base entropy base label = base label info gain alphabet vocab infogains vocab infogains get base entropy base entropy label vector get base label base label factory ranked feature vector factory factory ranked feature vector ranked feature vector instance list ilist info gain ilist 