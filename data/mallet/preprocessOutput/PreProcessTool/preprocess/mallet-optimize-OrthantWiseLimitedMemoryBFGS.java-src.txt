optimize linked list logging logger types matrix ops logger orthant wise limited memory quasi newton optimizing convex l1 regularized objectives scalable training l1 regularized log linear models galen andrew and jianfeng gao in i m l 2007 details an adaptation freely available c++ on galen s webpage author kedar bellare orthant wise limited memory b f g s optimizer logger logger = logger get logger orthant wise limited memory b f g s get name converged = optimizable gradient value optimizable name optimizable value output opt name max iterations = 1000 tolerance = 0001 gradient tolerance = 001 eps = 1 0e 5 l1 weight number corrections used in b f g s update ideally 3 <= m <= 7 larger m means more cpu time memory m = 4 state optimizer search old value = value before line search value = value after line search old value value y dot y grad = gradient grad old grad direction steepest descent direction old s = list m previous difference in values y = list m previous difference in grad values linked list<double > s y rho = intermediate calculation linked list< double> rhos alphas iterations orthant wise limited memory b f g s optimizable gradient value function function 0 0 orthant wise limited memory b f g s optimizable gradient value function l1wt optimizable = function l1 weight = l1wt parts = optimizable get get name split \\ opt name = parts parts length 1 initialize optimizer state iterations = 0 s = linked list<double > y = linked list<double > rhos = linked list< double> alphas = m matrix ops set all alphas 0 0 y dot y = 0 num = optimizable get num get initial = num optimizable get get initial value value = eval l1 get initial gradient grad = num eval gradient initialize direction direction = num steepest descent direction = num initialize backups old = num old grad = num optimizable get optimizable optimizable converged converged get iteration iterations optimize optimize m a x v a l u e optimize num iterations logger fine entering o w l b f g s optimize l1 weight= + l1 weight + initial value= + value iter = 0 iter < num iterations iter++ create descent direction make steepest desc dir adjust curvature map dir inverse hessian y dot y fix direction signs fix dir signs backup and gradient then perform line search store src in dest old store src in dest grad old grad back tracking line search update gradient after line search eval gradient check termination conditions check value termination condition logger info exiting o w l b f g s on termination #1 logger info value difference below tolerance old value + old value + value + value converged = check gradient termination condition logger info exiting o w l b f g s on termination #2 logger info gradient= + matrix ops two norm grad + < + gradient tolerance converged = update hessian approximation y dot y = shift iterations++ iterations > max iterations logger info too many iterations in o w l b f g s + continuing current converged = evaluate value make it a minimization problem eval l1 val = optimizable get value sum abs wt = 0 l1 weight > 0 infinite sum abs wt += math abs l1 weight logger info get value + opt name + get value = + val + + |w|= + sum abs wt + = + val + sum abs wt val + sum abs wt evaluate gradient make it a descent direction eval gradient optimizable get value gradient grad adjust grad infinite params grad matrix ops times equals grad 1 0 creates steepest ascent direction from gradient and l1 regularization make steepest desc dir l1 weight == 0 i = 0 i < grad length i++ direction i = grad i i = 0 i < grad length i++ i < 0 direction i = grad i + l1 weight i > 0 direction i = grad i l1 weight grad i < l1 weight direction i = grad i l1 weight grad i > l1 weight direction i = grad i + l1 weight direction i = 0 store src in dest direction steepest descent direction adjust grad infinite params d i = 0 i < length i++ infinite i d i = 0 adjusts direction based on approximate hessian inverse y dot y y^ t y in b f g s calculation map dir inverse hessian y dot y s size == 0 count = s size i = count 1 i >= 0 i alphas i = matrix ops dot product s get i direction rhos get i matrix ops plus equals direction y get i alphas i scalar = rhos get count 1 y dot y logger fine direction multiplier = + scalar matrix ops times equals direction scalar i = 0 i < count i++ beta = matrix ops dot product y get i direction rhos get i matrix ops plus equals direction s get i alphas i beta fix dir signs l1 weight > 0 i = 0 i < direction length i++ direction i steepest descent direction i <= 0 direction i = 0 dir deriv l1 weight == 0 matrix ops dot product direction grad val = 0 0 i = 0 i < direction length i++ direction i != 0 i < 0 val += direction i grad i l1 weight i > 0 val += direction i grad i + l1 weight direction i < 0 val += direction i grad i l1 weight direction i > 0 val += direction i grad i + l1 weight val shift next s = next y = list size = s size list size < m next s = length next y = length next s = s remove first next y = y remove first rhos remove first rho = 0 0 y dot y = 0 0 i = 0 i < length i++ infinite i infinite old i i old i > 0 next s i = 0 next s i = i old i infinite grad i infinite old grad i grad i old grad i > 0 next y i = 0 next y i = grad i old grad i rho += next s i next y i y dot y += next y i next y i logger fine rho= + rho rho < 0 invalid optimizable rho = + rho + < 0 + invalid hessian inverse + gradient change should be opposite parameter change s add last next s y add last next y rhos add last rho update old params and grad store src in dest old store src in dest grad old grad y dot y store src in dest src dest arraycopy src 0 dest 0 src length backtrack line search back tracking line search orig dir deriv = dir deriv orig dir deriv >= 0 invalid optimizable l b f g s chose a non ascent direction check your gradient! alpha = 1 0 backoff = 0 5 iterations == 0 norm dir = math sqrt matrix ops dot product direction direction alpha = 1 0 norm dir backoff = 0 1 c1 = 1e 4 store old value old value = value logger fine starting line search iter= + iterations logger fine iter + iterations + value at start line search = + value update and gradient get next point alpha find value value = eval l1 logger fine iter + iterations + using alpha = + alpha + value = + value + |grad|= + matrix ops two norm grad + |x|= + matrix ops two norm value <= old value + c1 orig dir deriv alpha alpha = backoff get next point alpha i = 0 i < length i++ i = old i + direction i alpha l1 weight > 0 not allow to cross orthant boundaries using l1 regularization old i i < 0 i = 0 0 optimizable set termination conditions check value termination condition 2 0 math abs value old value <= tolerance math abs value + math abs old value + eps check gradient termination condition matrix ops two norm grad < gradient tolerance 