io i o io input stream io output stream io serializable bit set random logging logger types instance list types matrix ops optimize limited memory b f g s optimize optimizable optimize optimization optimize optimizer logger a r f trainer that can combine multiple objective functions each represented a optmizable value gradient r f trainer value gradients transducer trainer transducer trainer optimization logger logger = logger get logger r f trainer label likelihood get name r f crf gsc keep instead classnames will give more flexibility to user to setup r f optimizable and then pass them directly in constructor so r f optimizable inner no longer creates r f optimizable optimizable gradient value optimizable value gradient optimizable value gradient classes optimizable r f ocrf optimizer opt iteration count = 0 converged gsc removing these options user ought to set weights before creating trainer use sparse weights = gsc use unsupported trick = various values from r f acting indicators when we need to cached value weights stamp = 1 re calculate expectations and values to get value because weights values changed cached gradient weights stamp = 1 re calculate to get value gradient because weights values changed gsc removing because user will call set weights dimensions in cached weights structure stamp = 1 re allocate crf weights expectations constraints because states transitions use mcrf training set to when we need to re allocate crf weights expectations constraints because we are using a different training list than last time gsc number times to reset optimizer and training when could not step in current direction occurs d e f a u l t m a x r e s e t s = 3 max resets = d e f a u l t m a x r e s e t s r f trainer value gradients r f crf optimizable gradient value optimizable value gradient crf = crf optimizable value gradient = optimizable value gradient transducer get transducer crf r f get r f crf optimizer get optimizer opt training converged otherwise converged converged training converged otherwise finished training converged get iteration iteration count gsc optimizable gradient value get optimizable gradient value optimizable value gradient an optimizable r f that contains a collection objective functions <p> one doesn t then creates one and sets optimizer to optimizable r f get optimizable r f instance list training set gsc user should call set weights dimensions in before optimizable and trainer are created cached weights structure stamp != crf weights structure change stamp use sparse weights crf set weights dimension in training set use unsupported trick crf set weights dimension densely ocrf = cached weights structure stamp = crf weights structure change stamp ocrf == || ocrf training set != training set ocrf = optimizable r f crf training set opt = ocrf a l b f g s optimizer creating one doesn t <p> also creates an optimizable r f required optimizer get optimizer instance list training set get optimizable r f training set will set mcrf necessary opt == || ocrf != opt get optimizable opt = limited memory b f g s ocrf alternative opt = conjugate gradient 0 001 opt trains a r f until convergence train incremental instance list training train training m a x v a l u e trains a r f until convergence or specified number iterations whichever earlier <p> also creates an optimizable r f and an optmizer required train instance list training set num iterations num iterations <= 0 training set size > 0 get optimizable r f training set will set mcrf necessary get optimizer training set will set opt necessary num resets = 0 converged = logger info r f about to train +num iterations+ iterations i = 0 i < num iterations i++ gsc timing each iteration start time = current time millis converged = opt optimize 1 logger info r f finished one iteration maximizer i= +i+ + + current time millis start time 1000 + secs iteration count++ run evaluators optimization e gsc resetting optimizer specified number times e print stack trace logger info catching num resets < max resets reset optimizer and get a one logger info resetting optimizer ++num resets opt = get optimizer training set logger info catching saying converged converged = logger info saying converged converged = converged logger info r f training has converged i= +i converged train a r f on various sized subsets data typically used to accelerate training quickly getting to reasonable on only a subset first then on progressively more data training training instances num iterations per proportion maximum number maximizer iterations per training proportion training proportions non train on increasingly larger portions data e g 0 2 0 5 1 0 can sometimes speedup convergence be sure to end in 1 0 you want to train on all data in end training has converged train instance list training num iterations per proportion training proportions training iteration = 0 training proportions length > 0 converged = i = 0 i < training proportions length i++ training proportions i <= 1 0 logger info training on +training proportions i + % data round training proportions i == 1 0 converged = train training num iterations per proportion converged = train training split random 1 training proportions i 1 training proportions i 0 num iterations per proportion training iteration += num iterations per proportion converged gsc comment in get optimizable r f set use sparse weights b use sparse weights = b get use sparse weights use sparse weights gsc set use unsupported trick b use unsupported trick = b get use unsupported trick use unsupported trick gsc change max number times optimizer can be reset before throwing could not step in current direction sets max number times optimizer can be reset before throwing an <p> value <tt> d e f a u l t m a x r e s e t s< tt> set max resets max resets max resets = max resets an optimizable r f that contains a collection objective functions optimizable r f optimizable gradient value serializable instance list training set cached value = 123456789 cached gradie bit set infinite values = r f crf optimizable gradient value opts optimizable r f r f crf instance list ilist set up crf = crf training set = ilist opts = optimizable value gradient cached gradie = crf get num factors cached value weights stamp = 1 cached gradient weights stamp = 1 optimizable r f r f crf instance list ilist set up crf = crf training set = ilist cached gradie = crf get num factors parameter types = r f instance list i = 0 i < optimizable value gradient classes length i++ constructor = optimizable value gradient classes i get constructor parameter types opts i = optimizable gradient value instance crf ilist e illegal state couldn t contruct optimizable gradient value cached value weights stamp = 1 cached gradient weights stamp = 1 t o d o move these implementations into r f and put here stubs that call them! get num crf get num factors get buffer crf get buffer get parameter index crf get parameter index set buff crf set buff crf weights value changed set parameter index value crf set parameter index value crf weights value changed log probability training sequence labels and prior over get value crf weights value change stamp != cached value weights stamp cached value not up to date it was calculated a different set r f weights starting time = current time millis cached value = 0 i = 0 i < opts length i++ cached value += opts i get value cached value weights stamp = crf weights value change stamp cached value now no longer stale logger info get value loglikelihood = +cached value logger fine inference milliseconds = + current time millis starting time cached value get value gradient buffer prior gradient parameter gaussian prior variance gradient constraint expectation + prior gradient == expectation constraint prior gradient gradient points up hill i e in direction higher value cached gradient weights stamp != crf weights value change stamp get value will fill in expectation updating it necessary matrix ops set all cached gradie 0 b2 = buffer length i = 0 i < opts length i++ matrix ops set all b2 0 opts i get value gradient b2 matrix ops plus equals cached gradie b2 cached gradient weights stamp = crf weights value change stamp arraycopy cached gradie 0 buffer 0 cached gradie length serialization maximizable r f serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write training set out write cached value out write cached gradie out write infinite values out write crf read input stream in i o not found in read training set = instance list in read cached value = in read cached gradie = in read infinite values = bit set in read crf = r f in read serialization r f trainer value gradient serial u = 1 u r r e n t s e r i a l v e r s i o n = 1 n u l l i n t e g e r = 1 need to check pointers write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write feature index out write cached gradient weights stamp out write cached value weights stamp out write cached weights structure stamp out write use sparse weights illegal state not yet complete read input stream in i o not found in read feature index = in read use sparse weights = in read illegal state not yet complete 