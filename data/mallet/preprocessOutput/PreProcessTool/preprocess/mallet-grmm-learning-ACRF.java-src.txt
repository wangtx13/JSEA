2003 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e logging logger io regex pattern gnu trove jdom document jdom element jdom j dom jdom input s a x builder inference types labels assignment models optimize optimizable pipe pipe types utils logger arbitrary r fs these are r fs completely arbitrary graphical structure user passes in a list instances a r f clique factory which get to look at sequence and decide what author <a href= mailto casutton > charles sutton< a> $ a r f v 1 1 2007 10 22 21 37 43 exp $ a r f serializable logger logger = logger get logger a r f get name template templates list fixed ptls = list 0 graph post processor graph processor alphabet input alphabet inferencer global inferencer = t r p inferencer viterbi = t r p create max product feature index pipe input pipe cache unrolled graphs = map graph cache = t hash map gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e d e f a u l t g a u s s i a n p r i o r v a r i a n e = 10 0 size scale = create a a r f a 1 d sequence needs an templates a r f pipe input pipe template tmpls illegal argument input pipe = input pipe templates = tmpls input alphabet = input pipe get data alphabet feature index = input alphabet size tidx = 0 tidx < templates length tidx++ templates tidx index = tidx accessors alphabet get input alphabet input alphabet get feature index feature index inferencer get inferencer global inferencer set inferencer inferencer inf global inferencer = inf inferencer get viterbi inferencer viterbi set viterbi inferencer inferencer inf viterbi = inf size scale size scale set size scale size scale size scale = size scale sets all templates a r f to use supported features only b all templates will use supported features only otherwise all unsupported features will be used set supported only b i = 0 i < templates length i++ templates i set supported only b cache unrolled graphs cache unrolled graphs set cache unrolled graphs cache unrolled graphs cache unrolled graphs = cache unrolled graphs set fixed potentials template fixed fixed ptls = arrays list fixed tidx = 0 tidx < fixed length tidx++ fixed tidx index = 1 add fixed potentials template tmpls i = 0 i < tmpls length i++ template tmpl = tmpls i tmpl set trainable fixed ptls add tmpl tmpl index = 1 template get templates templates pipe get input pipe input pipe template get fixed templates template fixed ptls to template fixed ptls size add fixed potential template tmpl tmpl set trainable fixed ptls add tmpl tmpl index = 1 get gaussian prior variance gaussian prior variance set gaussian prior variance gaussian prior variance gaussian prior variance = gaussian prior variance set graph processor graph post processor graph processor graph processor = graph processor making global transformations to an unrolled graph after it has been generated example directed models can be simulated selectively normalizing potentials graph post processor serializable process unrolled graph graph instance inst a type clique in model each type clique assumed to have same number possible outcomes and same set weights t o d o make an implement log linear template fixed template template serializable s o m e u n s u p p o r t e d t h r e s h o l d = 0 1 unsupported weights added = adds all instiated cliques an instance called a graph being unrolled an instance graph graph that cliques will be added to instance instance to unroll grpah subclasses are free to specify what types they expect in instance s slots add instantiated cliques unrolled graph graph instance instance modifies a factor computed from template useful templates that wish to implement special normalization etc does nothing < p> w a r n i n g you implement it likely that you will change derivative factor respect to weights means that you will not be able to use <tt> a r f trainer< tt> template unrolled graph graph in which factor sits clique set nodes which are domain factor ptl factor to modify modify potential unrolled graph unrolled graph unrolled var set clique table factor ptl sparse vector weights bit set assignments present supported only = supported only supported only sets whether template will use supported features only set supported only supported only supported only = supported only unsupported weights added unsupported weights added bit set get assignments present assignments present weights clique template each possible assignment to clique can in general have a different set weights so an sparse vectors w where w i are weights assignment i sparse vector get weights weights set weights sparse vector w weights != w length != weights length illegal argument weights length changed was +weights length+ now +w length weights = w initializes weight vectors to appropriate size a set training data number weights created init weights instance list training logger info template +this+ weights + supported only ? n o a l l + unsupported features supported only init sparse weights training init dense weights training init dense weights instance list training numf = training get data alphabet size total = 0 handle weights size = clique size from instance training total += allocate weights size and regular weights sparse vector weights = sparse vector size i = 0 i < size i++ weights i = sparse vector numf weights != weights i plus equals sparse weights i total += numf logger info a r f template +this+ weights +i+ num features +numf logger info a r f template +this+ total num weights = +total weights = weights total init sparse weights instance list training check clique size consistent training debug total = 0 build bitsets that tell us what weights occur in data size = clique size from instance training bit set weights present = bit set size i = 0 i < size i++ weights present i = bit set assignments present = bit set size collect weights present training weights present weights != add in current weights weights present we can allocate weights now total += allocate weights size use those to allocate sparse vectors sparse vector weights = sparse vector size total += allocate weights weights present weights logger info a r f template +this+ total num weights = +total weights = weights total allocate weights bit set weights present sparse vector weights total = 0 i = 0 i < weights present length i++ create a sparse vector allowable indices specified in advance num locations = weights present i cardinality indices = num locations j = 0 j < num locations j++ indices j = weights present i next set bit j == 0 ? 0 indices j 1 +1 out a r f +this+ +i+ has index +indices j weights i = hashed sparse vector indices num locations num locations num locations weights != weights i plus equals sparse weights i total += num locations num locations != 0 logger info a r f template +this+ weights +i+ num features +num locations total assumes weights already initialized add some unsupported weights instance list training add debugging marker unsupported weights added = size = weights length bit set weights present = bit set size i = 0 i < size i++ weights present i = bit set collect some unsupported weights training weights present add in current weights weights present sparse vector weights = sparse vector size num added = allocate weights weights present weights logger info this+ some supported weights added = +num added weights = weights num added collect some unsupported weights instance list training bit set weights present ii = 0 ii < training size ii++ instance inst = training get ii unrolled graph unrolled = unrolled graph inst template list iterator it = unrolled unrolled var set iterator it has next unrolled var set vs = unrolled var set it next factor f = vs get factor factor nrmed = f normalize assignment iterator assn it = nrmed assignment iterator assn it has next nrmed value assn it > s o m e u n s u p p o r t e d t h r e s h o l d add present features weights present assn it index current assn vs fv assn it advance allocate weights size sparse vector newdefault weights = sparse vector size weights != newdefault weights plus equals sparse weights weights = newdefault weights size clique size from instance instance list training max weight = 0 i = 0 i < training size i++ instance instance = training get i unrolled graph unrolled = unrolled graph instance template iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next clique tmpl == weight = clique weight weight > max weight max weight = weight max weight == 0 logger warning a r f t know size +this+ never needed in training data max weight debugging function check clique size consistent instance list training weight = 1 i = 0 i < training size i++ instance instance = training get i unrolled graph unrolled = unrolled graph instance template iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next clique tmpl == weight != clique weight err weight change clique +clique+ template +this+ old = +weight+ +clique weight vi = 0 vi < clique size vi++ variable var = clique get vi err var+ +var get num outcomes weight == 1 weight = clique weight illegal state on instance +instance+ template +this+ clique +clique+ strange weight was +weight+ now +clique weight add in current weights bit set weights present assn = 0 assn < weights length assn++ j = 0 j < weights assn num locations j++ weights present assn set weights assn index at location j collect weights present instance list ilist bit set weights present inum = 0 inum < ilist size inum++ instance inst = ilist get inum unrolled graph unrolled = unrolled graph inst template collect transitions present graph unrolled collect weights present graph unrolled weights present collect transitions present graph unrolled graph unrolled iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next clique tmpl == assn no = clique lookup assignment number assignments present set assn no collect weights present graph unrolled graph unrolled bit set weights present iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next clique tmpl == assn = clique lookup assignment number add present features weights present assn clique fv add present features bit set wp feature vector fv i = 0 i < fv num locations i++ index = fv index at location i wp set index table factor compute factor unrolled var set clique matrix phi = create factor matrix clique sparse vector weights = get weights out unrolled clique +clique out f v +clique fv loc = 0 loc < phi num locations loc++ idx = phi index at location loc idx < weights length instantiating +this+ on +clique+ clique has too many + assignments # weights = +weights length+ clique weight = +clique weight sparse vector w = weights idx out weights +idx+ +w w print dp = w dot product clique fv dp += get weight idx phi set value at location loc dp table factor ptl = log table factor clique ptl set values phi ptl creates an empty matrix use in storing factor values when template unrolled overriding subclasses may enforce that factors generated be sparse clique an empty matrixn matrix create factor matrix unrolled var set clique szs = clique var dimensions matrixn szs index sparse vector weights get weight i weights value i sparse vector get weights weights set weights sparse vector w weights = w set weight i w weights set value i w trainable = trainable trainable set trainable tr trainable = tr i hate serialization serial u = 727618747254644076 l 8830720632081401678 l read input stream in i o not found in read assignments present == assignments present = bit set weights length assignments present flip 0 assignments present size assignment compute assignment assignment graph assn var set vs assignment graph assn marginalize vs templates that expect a feature vector sequence labels sequence their instances sequence template template adds all instiated cliques an instance called a graph being unrolled an instance graph graph that cliques will be added to fvs input features instance to unroll cliques lblseq label sequence instance being unrolled add instantiated cliques unrolled graph graph feature vector sequence fvs labels assignment lblseq add instantiated cliques unrolled graph graph instance instance feature vector sequence fvs = feature vector sequence instance get data labels assignment lblseq = labels assignment instance get target add instantiated cliques graph fvs lblseq potentials that have no weights but that know how te construct a potential fixed factor template template init weights instance list training 0 sparse vector get weights sparse vector 0 sparse vector get weights sparse vector trainable set trainable tr tr illegal argument template never trainable table factor compute factor unrolled var set clique a clique in unrolled graphical model an instantiation some template contains a pointer to its corresponding template and a feature vector unrolled var set hash var set template tmpl template that generated clique feature vector fv features clique variable vars factor factor factor compute clique unrolled graph graph last change cache graphs change in var set s factor since last grad call var dimensions dims = size i = 0 i < size i++ dims i = get i get num outcomes dims unrolled var set unrolled graph graph template tmpl variable vars feature vector fv vars graph = graph vars = vars tmpl = tmpl fv = fv assignment get assignment number assn sizes = var dimensions indices = sizes length matrixn single to indices assn indices sizes assignment vars indices lookup assignment number assignment mine = lookup assignment mine single index assignment lookup assignment tmpl compute assignment graph get assignment lookup number assignment assignment assn sizes = var dimensions indices = sizes length i = 0 i < indices length i++ indices i = assn get vars i matrixn single index sizes indices template get template tmpl feature vector get fv fv factor get factor factor set factor factor f factor != last change = factors dist linf table factor f table factor factor factor = f get last change last change unrolled graph undirected model t o d o unrolled graph factor graph compactible variables containing all nodes in model list all vars = list containing all instantiated cliques unrolled clique in model list cliques = list number label in each labels num slices cached = instance instance feature vector sequence fvs assignment assignment output label alphabet output alphabets a r f acrf list all templates factors added = t hash map uvs map = t hash map unrolled graph instance inst template templates template fixed inst templates arrays list fixed unrolled graph instance inst template templates list fixed inst templates fixed creates a graphical model a given instance called unrolling a dynamic model unrolled graph instance inst template templates list fixed setup potentials initial capacity inst instance = inst fvs = feature vector sequence inst get data assignment = assignment inst get target all templates = list fixed != all templates add all fixed all templates add all arrays list templates setup graph setup potentials compute p fs guesses how much cache undirected model should have space initial capacity instance inst inst get data == 8 feature vector sequence fvs = feature vector sequence inst get data t = fvs size 3 t setup graph iterator it = all templates iterator it has next template tmpl = template it next tmpl add instantiated cliques instance setup graph add clique unrolled var set clique cliques add clique compute p fs factors added = t list resid tmp = t list iterator it = cliques iterator it has next unrolled var set clique = unrolled var set it next table factor ptl = clique tmpl compute factor clique add factor internal clique ptl clique tmpl modify potential clique ptl uvs map put ptl clique sigh log table factor unif = log table factor clique resid tmp add factors dist linf unif ptl last resids = resid tmp to adds f a t o r to graph but maintaining invariant that every set variables has at most one factor over exactly that domain given f a t o r has a domain that already used some other factor p r e v then p r e v replaced a factor graph containing p r e v and f a t o r clique factor factor to add add factor internal unrolled var set clique factor factor clique set factor factor factor prev factor = factor factor var set prev factor == add factor factor prev factor factor graph prev factor multiply factor divide prev factor add factor factor graph factor factor prev factor last resids recompute factors last resids = factors size iterator it = cliques iterator it has next unrolled var set clique = unrolled var set it next table factor old f = table factor clique get factor table factor f = clique tmpl compute factor clique dist = factors dist linf table factor old f duplicate normalize table factor f duplicate normalize last resids get index old f = dist old f set values f get log value matrix clique tmpl modify potential clique old f get last resids last resids accessors get max time fvs size get num factors output alphabets length an assignment that corresponds to label sequence which graph was unrolled assignment get assignment assignment xxx these should be refactor to undirected model and automatically add evidence potentials t hash map observed vars = t hash map observed variable var observed vars contains var set observed variable var outcome observed vars put var outcome observed value variable var observed vars get var iterator unrolled var set iterator cliques iterator unrolled var set get unrolled var set cnum unrolled var set cliques get cnum get index var set vs cliques index vs variable get idx factors added get idx variable all vars get idx get index variable var factors added get index var all vars index var get log num assignments total = 0 i = 0 i < num variables i++ variable var = get i total += math log var get num outcomes total convenience variable var index t j labels assignment lblseq = labels assignment instance get target lblseq var index t j num slices labels assignment lblseq = labels assignment instance get target lblseq num slices computes residual each factor without actually changing unrolled graph compute current resids last resids = factors size iterator it = cliques iterator it has next unrolled var set clique = unrolled var set it next table factor old f = table factor clique get factor table factor f = clique tmpl compute factor clique dist = factors dist linf old f f last resids get index old f = dist last resids unrolled var set get unrolled var set factor f unrolled var set uvs map get f optimizable gradient value get maximizable instance list ilst maximizable a r f ilst list best assignment instance list lst list ret = list lst size i = 0 i < lst size i++ ret add best assignment lst get i ret assignment best assignment instance inst compute m a p assignment unrolled graph unrolled = unroll inst models best assignment unrolled viterbi list get best labels instance list lst list ret = list lst size i = 0 i < lst size i++ ret add get best labels lst get i ret labels sequence get best labels instance inst assignment assn = best assignment inst labels assignment gold = labels assignment inst get target gold to labels sequence assn unrolled graph unroll instance inst unrolled graph g cache unrolled graphs graph cache contains key inst g = unrolled graph graph cache get inst g recompute factors g = unrolled graph inst templates fixed ptls graph processor != graph processor process g inst cache unrolled graphs graph cache put inst g g unrolled graph unroll structure only instance inst unrolled graph g cache unrolled graphs graph cache contains key inst g = unrolled graph graph cache get inst g recompute factors g = unrolled graph inst templates fixed ptls graph processor != graph processor process g inst cache unrolled graphs graph cache put inst g g report on graph cache logger info number cached graphs = +graph cache size maximizable a r f optimizable gradient value serializable instance list train data cached value = 123456789 cached gradient bit set infinite values = cached value stale cached gradient stale num total nodes = 0 print gradient = an unrolled a r f unrolled graph graph inferencer inferencer = global inferencer duplicate vectors that contain counts features observed in training data maps clique template x feature number => count sparse vector constraints vectors that contain expected value over labels all features have seen training data but not training labels sparse vector expectations sparse vector constraints sparse vector expectations init weights instance list training tidx = 0 tidx < templates length tidx++ num += templates tidx init weights training initialize constraints and expectations to have same dimensions weights but to be all zero init constraints expectations defaults first constraints = sparse vector templates length expectations = sparse vector templates length tidx = 0 tidx < templates length tidx++ sparse vector defaults = templates tidx get weights constraints tidx = sparse vector defaults clone matrix zeroed expectations tidx = sparse vector defaults clone matrix zeroed and now others constraints = sparse vector templates length expectations = sparse vector templates length tidx = 0 tidx < templates length tidx++ template tmpl = templates tidx sparse vector weights = tmpl get weights constraints tidx = sparse vector weights length expectations tidx = sparse vector weights length i = 0 i < weights length i++ constraints tidx i = sparse vector weights i clone matrix zeroed expectations tidx i = sparse vector weights i clone matrix zeroed set all expectations to 0 after they ve been initialized reset expectations tidx = 0 tidx < expectations length tidx++ expectations tidx set all 0 0 i = 0 i < expectations tidx length i++ expectations tidx i set all 0 0 maximizable a r f instance list ilist logger finest initializing maximizable a r f allocate weights constraints and expectations train data = ilist init weights train data init constraints expectations num instances = train data size cached gradient = num cached value stale = cached gradient stale = cache unrolled graphs unrolled graphs = unrolled graph num instances logger info number training instances = + num instances logger info number = + num logger info feature index = + feature index describe prior logger fine computing constraints collect constraints train data describe prior logger info using gaussian prior variance +gaussian prior variance not tested maximizable d r f maximizable a r f maxable instance list ilist logger finest initializing maximizable a r f train data = ilist init constraints expectations constraints = maxable constraints these can be shared num instances = train data size these must occur after init weights num = num weights cached gradient = num cached value stale = cached gradient stale = cache unrolled graphs unrolled graphs = unrolled graph num instances get num num negate initial value and value because are in weights not values get buf buf length != num illegal argument argument not + correct dimensions idx = 0 tidx = 0 tidx < templates length tidx++ template tmpl = templates tidx sparse vector defaults = tmpl get weights values = defaults get values arraycopy values 0 buf idx values length idx += values length tidx = 0 tidx < templates length tidx++ template tmpl = templates tidx sparse vector weights = tmpl get weights assn = 0 assn < weights length assn++ values = weights assn get values arraycopy values 0 buf idx values length idx += values length set params params length != num illegal argument argument not + correct dimensions cached value stale = cached gradient stale = idx = 0 tidx = 0 tidx < templates length tidx++ template tmpl = templates tidx sparse vector defaults = tmpl get weights values = defaults get values arraycopy params idx values 0 values length idx += values length tidx = 0 tidx < templates length tidx++ template tmpl = templates tidx sparse vector weights = tmpl get weights assn = 0 assn < weights length assn++ values = weights assn get values arraycopy params idx values 0 values length idx += values length functions unit tests to get constraints and expectations i m too lazy to make a deep copy callers should not modify these sparse vector get expectations cnum expectations cnum sparse vector get constraints cnum constraints cnum print weights print buf = num get buf len = buf length w = 0 w < len w++ out print buf w + out get parameter index 0 0 set parameter index value log probability training sequence labels get value cached value stale cached value = compute log likelihood cached value stale = cached gradient stale = save num++ % save period == 0 out saving a r f a r f write weights weight out done logger info get value loglikelihood = + cached value na n cached value logger warning value na n cached value = 0 cached value compute log likelihood retval = 0 0 num instances = train data size start = current time millis unroll time = 0 marginals time = 0 instance values must either always or never be in total values we can t just sometimes skip a value because it infinite that off total values we only allow an instance to have infinite value it happens from start we t compute value instance after first round any other instance has infinite value after that it an initializing infinite values = infinite values == we could initialize bitset one slot every instance but it probably cheaper not to taking time hit to allocate space a bit becomes necessary infinite values = bit set initializing infinite values = clear sufficient statistics that we are about to fill reset expectations fill in expectations each instance i = 0 i < num instances i++ instance instance = train data get i compute marginals each clique unroll start = current time millis unrolled graph unrolled = unroll instance unroll end = current time millis unroll time += unroll end unroll start unrolled num variables == 0 happens all nodes are pruned inferencer compute marginals unrolled marginals time += current time millis unroll end unrolled dump save expected value each feature when we compute gradient collect expectations unrolled inferencer add in joint prob labeling assignment joint assn = unrolled get assignment value = inferencer lookup log joint joint assn infinite value initializing infinite values logger warning instance + instance get name + has infinite value skipping infinite values set i !infinite values get i logger warning infinite value on instance +instance get name + returning infinity n e g a t i v e i n f i n i t y print debug info unrolled illegal state instance + instance get name + used to have non infinite + value but now it has infinite value na n value out na n on instance +i+ +instance get name print debug info unrolled illegal state value na n in a r f get value instance +i logger warning value na n in a r f get value instance +i+ + returning infinity n e g a t i v e i n f i n i t y retval += value size scale retval = retval train data size incorporate gaussian prior on means that each weight we will add w^2 2 variance to log probability prior denom = 2 gaussian prior variance tidx = 0 tidx < templates length tidx++ sparse vector weights = templates tidx get weights j = 0 j < weights length j++ fnum = 0 fnum < weights j num locations fnum++ w = weights j value at location fnum weight valid w tidx j retval += w w prior denom cache unrolled graphs report on graph cache end = current time millis logger info a r f inference time ms = + end start logger info a r f marginals time ms = +marginals time logger info a r f unroll time ms = +unroll time logger info get value loglikelihood = +retval retval computes graident penalized log likelihood a r f and it in buf get value gradient buf cached gradient stale will fill in expectations cached value stale get value compute gradient cached gradient stale = buf length != num illegal argument incorrect length buffer to get value gradient expected + num + received + buf length arraycopy cached gradient 0 buf 0 cached gradient length computes gradient penalized log likelihood a r f and places it in cached gradient gradient constraint expectation gaussian prior variance compute gradient index into current element cached gradient gidx = 0 first gradient wrt weights tidx = 0 tidx < templates length tidx++ sparse vector these weights = templates tidx get weights sparse vector these constraints = constraints tidx sparse vector these expectations = expectations tidx j = 0 j < these weights num locations j++ weight = these weights value at location j constraint = these constraints value at location j expectation = these expectations value at location j print gradient out gradient + gidx + = d e f a u l t +templates tidx + +j+ = + constraint + ctr + expectation + exp + weight gaussian prior variance + reg scale = size scale ? 1 0 train data size 1 0 cached gradient gidx++ = scale constraint expectation weight gaussian prior variance now other weights tidx = 0 tidx < templates length tidx++ template tmpl = templates tidx sparse vector weights = tmpl get weights i = 0 i < weights length i++ sparse vector weight vec = weights i sparse vector constraint vec = constraints tidx i sparse vector expectation vec = expectations tidx i j = 0 j < weight vec num locations j++ w = weight vec value at location j gradient computed below a parameter may be set to infinity an external user we set gradient to 0 because parameter s value can never change anyway and it will mess up future calculations on matrix infinite w logger warning infinite weight node index +i+ feature + input alphabet lookup j gradient = 0 0 constraint = constraint vec value at location j expectation = expectation vec value at location j scale = size scale ? 1 0 train data size 1 0 gradient = scale constraint expectation w gaussian prior variance print gradient feature name = input alphabet lookup j out gradient + gidx + = w e i g h t +templates tidx + +i+ +feature name+ = + constraint + ctr + expectation + exp + w gaussian prior variance + reg cached gradient gidx++ = gradient report gradient only useful debugging grad call no = 0 report gradient verbose output directory != grad call no++ = verbose output directory acrf grad +grad call no+ txt print writer writer = print writer writer writer utils to cached gradient writer close = verbose output directory acrf value +grad call no+ txt writer = print writer writer writer cached value writer close buf = get num get buf = verbose output directory acrf weight +grad call no+ txt writer = print writer writer writer utils to buf writer close = verbose output directory acrf constraint +grad call no+ txt print vecs constraints constraints = verbose output directory acrf exp +grad call no+ txt print vecs expectations expectations = verbose output directory acrf dumps +grad call no+ txt writer = print writer writer ii = 0 ii < train data size ii++ a r f unrolled graph unrolled = unroll train data get ii writer unrolled writer close i o e runtime e print vecs sparse vector constraints sparse vector constraints i o print writer writer = print writer writer ti = 0 ti < constraints length ti++ writer constraints ti ti = 0 ti < constraints length ti++ i = 0 i < constraints ti length i++ writer constraints ti i writer close every feature f k computes expected value f k aver all possible label sequences given list instances we have these values are stored in collector that collector i j k gets expected value feature clique i label assignment j and input features k collect expectations unrolled graph unrolled inferencer inferencer iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next tidx = clique tmpl index tidx == 1 factor ptl = inferencer lookup marginal clique each assigment to clique note that we get assignment iterator from factor rather than clique because factor knows about any potential sparsity also note that we use assn it index current assn assumes that ordering variables in var set lookup margianl consistent between all calls to a somewhat brittle assumption but i t how to relax it without being terribly inefficient assignment iterator assn it = ptl assignment iterator assn it has next marginal = ptl value assn it idx = assn it index current assn expectations tidx idx plus equals sparse clique fv marginal expectations tidx location idx != 1 expectations tidx increment value idx marginal assn it advance collect constraints instance list ilist inum = 0 inum < ilist size inum++ logger finest collecting constraints instance +inum instance inst = ilist get inum unrolled graph unrolled = unrolled graph inst templates total nodes =+ unrolled num variables iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next tidx = clique tmpl index tidx == 1 assn = clique lookup assignment number constraints tidx assn plus equals sparse clique fv constraints tidx location assn != 1 constraints tidx increment value assn 1 0 dump gradient to name print stream w = print stream output stream name i = 0 i < num i++ w cached gradient i w close i o e err could not open output e print stack trace dump defaults out constraints i = 0 i < constraints length i++ out template +i constraints i print out expectations i = 0 i < expectations length i++ out template +i expectations i print print debug info unrolled graph unrolled print err assignment assn = unrolled get assignment iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next out clique +clique dump assn clique assn clique factor ptl = unrolled factor clique out value = +ptl value assn out ptl dump assn clique assignment assn unrolled var set clique iterator it = clique iterator it has next variable var = variable it next out var+ ==> +assn get var + +assn get var + weight valid w cnum j infinite w logger warning weight infinite clique +cnum+ assignment +j na n w logger warning weight nan clique +cnum+ assignment +j report nmsg = 1 inferencer belief propagation nmsg = belief propagation inferencer get total messages sent inferencer junction tree inferencer nmsg = junction tree inferencer inferencer get total messages sent nmsg != 1 logger info total messages sent = +nmsg force stale cached value stale = cached gradient stale = get total nodes total nodes maximizable a r f printing functions print output stream os print stream out = print stream os out a r f number templates == +templates length out weights tidx = 0 tidx < templates length tidx++ template tmpl = templates tidx out t e m p l a t e +tidx+ == +tmpl out weights sparse vector defaults = tmpl get weights loc = 0 loc < defaults num locations loc++ out +defaults index at location loc + = +defaults value at location loc sparse vector weights = tmpl get weights assn = 0 assn < weights length assn++ out assignment +assn sparse vector w = weights assn x = 0 x < w num locations x++ idx = w index at location x idx == feature index out print d e f a u l t out print input alphabet lookup idx out +w value at location x dump values title sparse vector values cnum = 0 cnum < values length cnum++ out title+ clique +cnum write clique values values cnum i o e err writing to file! e print stack trace write clique values sparse vector values i o out num assignments = +values length assn = 0 assn < values length assn++ out num locations = +values assn num locations j = 0 j < values assn num locations j++ idx = values assn index at location j out print sparse +assn+ +idx+ = out values assn value at location j dump one graph unrolled graph unrolled assignment assn = unrolled get assignment iterator it = unrolled unrolled var set iterator it has next unrolled var set clique = unrolled var set it next out clique +clique dump assn clique assn clique factor ptl = unrolled factor clique ptl != out ptl dump unrolled graphs instance list lst i = 0 i < lst size i++ instance inst = lst get i out i n s t a n e +i+ +inst get name unrolled graph unrolled = unroll inst dump one graph unrolled templates a template that adds edges between adjacent nodes in a label sequence one factor bigram template a r f sequence template factor bigram template factor factor = factor add instantiated cliques a r f unrolled graph graph feature vector sequence fvs labels assignment lblseq i = 0 i < lblseq max time 1 i++ variable v1 = lblseq var index i factor variable v2 = lblseq var index i + 1 factor feature vector fv = fvs get feature vector i variable vars = variable v1 v2 v1 != couldn t get label factor +factor+ time +i v2 != couldn t get label factor +factor+ time + i+1 a r f unrolled var set clique = a r f unrolled var set graph vars fv graph add clique clique to bigram template +factor+ get factor factor serial u = 8944142287103225874 l a template that adds node potentials a given factor unigram template a r f sequence template factor unigram template factor factor = factor add instantiated cliques a r f unrolled graph graph feature vector sequence fvs labels assignment lblseq i = 0 i < lblseq max time i++ variable v = lblseq var index i factor feature vector fv = fvs get feature vector i variable vars = variable v v != couldn t get label factor +factor+ time +i a r f unrolled var set clique = a r f unrolled var set graph vars fv graph add clique clique to unigram template +factor+ serial u = 1 l a template that adds edges between cotemporal nodes a given pair factors pairwise factor template a r f sequence template factor0 factor1 pairwise factor template factor0 factor1 factor0 = factor0 factor1 = factor1 add instantiated cliques a r f unrolled graph graph feature vector sequence fvs labels assignment lblseq i = 0 i < lblseq max time i++ variable v1 = lblseq var index i factor0 variable v2 = lblseq var index i factor1 feature vector fv = fvs get feature vector i variable vars = variable v1 v2 v1 != couldn t get label factor +factor0+ time +i v2 != couldn t get label factor +factor1+ time +i a r f unrolled var set clique = a r f unrolled var set graph vars fv graph add clique clique to pairwise factor template +factor0+ +factor1+ serial u = 1 l read weights from text reader reader i o document d = s a x builder build reader element root = d get root element list tmpls = root get children t e m p l a t e iterator it = tmpls iterator it has next element tmpl elt = element it next tmpl name = tmpl elt get attribute value n a m e ti = parse tmpl elt get attribute value x a r f template tmpl = templates ti ! tmpl get get name equals tmpl name runtime expected template +tmpl+ got +tmpl name element def w elt = tmpl elt get child d e f a u l t w e i g h t s sparse vector def w = read sparse vector def w elt get text element w vec elt = tmpl elt get child w e i g h t s nw = parse w vec elt get attribute value s i z e sparse vector w = sparse vector nw list w lst = w vec elt get children w e i g h t iterator it2 = w lst iterator it2 has next element w elt = element it2 next wi = parse w elt get attribute value x w wi = read sparse vector w elt get text get input alphabet tmpl set weights def w tmpl weights = w j dom e runtime e sparse vector read sparse vector str alphabet dict i o t list idxs = t list t list vals = t list lines = str split = 0 < lines length li++ line = lines pattern matches ^\\s $ line fields = line split idx dict != idx = dict lookup index fields 0 idx = parse fields 0 val = parse fields 1 idxs add idx vals add val sparse vector idxs to vals to write weights text writer writer print writer out = print writer writer out < r f> ti = 0 ti < templates length ti++ template tmpl = templates ti out < t e m p l a t e n a m e=\ +tmpl get get name + \ x=\ +ti+ \ > out < d e f a u l t w e i g h t s> sparse vector def w = tmpl get weights loc = 0 loc < def w num locations loc++ out print def w index at location loc out print out def w value at location loc out < d e f a u l t w e i g h t s> out sparse vector w = tmpl get weights out < w e i g h t s s i z e=\ +w length+ \ > wi = 0 wi < w length wi++ out < w e i g h t x=\ +wi+ \ > write weight vector out w wi out out < w e i g h t> out < w e i g h t s> out < t e m p l a t e> out < r f> write weight vector print writer out sparse vector sv out <! d a t a alphabet dict = get input alphabet loc = 0 loc < sv num locations loc++ idx = sv index at location loc val = sv value at location loc idx < dict size out print dict lookup idx out print x +idx out print out val out > convenient constructing a r fs a r f make factorial pipe p num levels list t = list i = 0 i < num levels i++ t add bigram template i i+1 < num levels t add pairwise factor template i i+1 template tmpls = template t to template t size a r f p tmpls i hate serialization serial u = 2865175696692468236 l 2113750667182393436 l read input stream in i o not found in read graph cache = t hash map verbose output directory = set verbose output directory dir verbose output directory = dir a r f 