2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e author andrew mc callum <a href= mailto >mccallum edu< a> types per label info gain log2 = math log 2 binary = print = info gain ig per label info gain instance list ilist pcig = calc per label info gains ilist alphabet v = ilist get data alphabet num classes = ilist get target alphabet size ig = info gain num classes i = 0 i < num classes i++ ig i = info gain v pcig i info gain get info gain index ig index get num classes ig length entropy pc pnc math abs pc+pnc 1 < 0 0001 pc= +pc+ pnc= +pnc pc == 0 || pnc == 0 0 ret = pc math log pc log2 pnc math log pnc log2 ret >= 0 pc= +pc+ pnc= +pnc ret calc per label info gains instance list ilist binary feature counts feature counts counts num classes = ilist get target alphabet size num features = ilist get data alphabet size num instances = ilist size fill in feature counts feature counts = num classes num features feature counts = num features counts = num classes fi = 0 fi < num features fi++ feature counts fi = 0 ci = 0 ci < num classes ci++ counts ci = 0 fi = 0 fi < num features fi++ feature counts ci fi = 0 i = 0 i < ilist size i++ instance instance = ilist get i feature vector fv = feature vector instance get data xxx note that ignores uncertainly labeled instances! index = instance get labeling get best index counts index ++ fvi = 0 fvi < fv num locations fvi++ feature index = fv index at location fvi feature counts index feature index ++ feature counts feature index ++ out fi= +feature index+ ni= +num instances+ fc= +feature counts feature index + i= +i feature counts feature index <= num instances fi= +feature index+ ni= +num instances+ fc= +feature counts feature index + i= +i alphabet v = ilist get data alphabet print ci = 0 ci < num classes ci++ out ilist get target alphabet lookup ci to + = +ci let i be a random variable on i !c i per entropy feature f j = h i|f j h i|f j = p i|f j log p i|f j p !c i|f j log p !c i|f j first calculate per entropy not conditioned on any feature and store it in counts entropies = num classes ci = 0 ci < num classes ci++ pc pnc pc = counts ci num instances pnc = num instances counts ci num instances entropies ci = entropy pc pnc calculate per infogain each feature and store it in feature counts fi = 0 fi < num features fi++ pf = feature counts fi num instances pnf = num instances feature counts fi num instances pf >= 0 pnf >= 0 print fi < 10000 out print v lookup fi to ci = 0 ci < num classes ci++ out print +class feature counts ci fi out sum == feature counts fi ci = 0 ci < num classes ci++ feature counts fi == 0 feature counts ci fi = 0 pc pnc ef calculate ci !ci entropy given that feature does occur pc = feature counts ci fi feature counts fi pnc = feature counts fi feature counts ci fi feature counts fi ef = entropy pc pnc calculate ci !ci entropy given that feature does not occur pc = counts ci feature counts ci fi num instances feature counts fi pnc = num instances feature counts fi counts ci feature counts ci fi num instances feature counts fi enf = entropy pc pnc feature counts ci fi = entropies ci pf ef + pnf enf print fi < 10000 out pf= +pf+ ef= +ef+ pnf= +pnf+ enf= +enf+ e= +class entropies ci + cig= +class feature counts ci fi print selected features print fi = 0 fi < 100 fi++ feature name = v lookup fi to ci = 0 ci < num classes ci++ name = ilist get target alphabet lookup ci to feature counts ci fi > 1 out feature name+ +class name+ = +class feature counts ci fi feature counts factory ranked feature vector per label factory factory ranked feature vector ranked feature vectors instance list ilist per label info gain x = per label info gain ilist x ig 