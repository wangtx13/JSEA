2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics types randoms arrays list list zip io text number format gnu trove latent dirichlet allocation author david mimno andrew mc callum multinomial h m m num topics number topics to be fit num states number hidden states num docs num sequences dirichlet alpha alpha over topics alpha alpha sum prior on per topic multinomial over words beta beta sum prior on state state transition distributions gamma gamma sum pi sum pi t hash map< t hash map> document topics document sequence ids document states state topic counts state topic totals state state transitions state transition totals initial state counts keep track most times each topic used in any document max tokens per topic size largest document max doc length rather than calculating log gammas every state and every topic we cache log predictive distributions every possible state and document topic log gamma cache doc log gamma cache num iterations = 1000 burnin period = 200 save sample interval = 10 optimize interval = 0 show topics interval = 50 topic keys randoms random number format formatter multinomial h m m number topics topics filename num states i o formatter = number format get instance formatter set maximum fraction digits 5 out l d a h m m + number topics document topics = t hash map< t hash map> num topics = number topics alpha sum = number topics alpha = number topics arrays fill alpha alpha sum num topics topic keys = num topics initializes num docs well load topics from topics filename document states = num docs document sequence ids = num docs max tokens per topic = num topics max doc length = 0 histogram = 380 total tokens = 0 doc=0 doc < num docs doc++ ! document topics contains key doc t hash map topic counts = document topics get doc count = 0 topic topic counts keys topic count = topic counts get topic histogram topic count ++ total tokens += topic count topic count > max tokens per topic topic max tokens per topic topic = topic count count += topic count count > max doc length max doc length = count running total = 0 0 i=337 i >= 0 i running total += i histogram i out format %d %d % 3f i histogram i running total total tokens num states = num states initial state counts = num states topic log gamma cache = num states num topics state=0 state < num states state++ topic=0 topic < num topics topic++ topic log gamma cache state topic = max tokens per topic topic + 1 topic log gamma cache state topic = 21 out max doc length doc log gamma cache = num states max doc length + 1 set gamma g gamma = g set num iterations num iterations num iterations = num iterations set burnin period burnin period burnin period = burnin period set topic display interval interval show topics interval = interval set random seed seed random = randoms seed set optimize interval interval optimize interval = interval initialize random == random = randoms gamma sum = gamma num states state topic counts = num states num topics state topic totals = num states state state transitions = num states num states state transition totals = num states pi = 1000 0 sum pi = num states pi max tokens = 0 total tokens = 0 num sequences = 0 sequence current sequence = 1 to cache topic distributions takes an hashmap a mask to only update distributions topics that have actually changed here we create a dummy count hash that has all topics t hash map all topics dummy = t hash map topic = 0 topic < num topics topic++ all topics dummy put topic 1 state=0 state < num states state++ recache state topic state all topics dummy doc = 0 doc < num docs doc++ sample state doc random recache state topic state t hash map topic counts current state topic counts = state topic counts state current state cache = topic log gamma cache state cache topic topic counts keys cache = current state cache topic cache 0 = 0 0 i=1 i < cache length i++ cache i = cache i 1 + math log alpha topic + i 1 + current state topic counts topic doc log gamma cache state 0 = 0 0 i=1 i < doc log gamma cache state length i++ doc log gamma cache state i = doc log gamma cache state i 1 + math log alpha sum + i 1 + state topic totals state sample i o start time = current time millis iterations = 1 iterations <= num iterations iterations++ iteration start = current time millis out print state transitions doc = 0 doc < num docs doc++ sample state doc random doc % 10000 == 0 out print state transitions out print current time millis iteration start + iterations % 10 == 0 out < + iterations + > print writer out = print writer buffered writer writer state state matrix + iterations out print state transition matrix out close out = print writer buffered writer writer state topics + iterations out print state topics out close iterations % 10 == 0 out = print writer buffered writer writer states + iterations doc = 0 doc < document states length doc++ out document states doc out close out flush seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds load topics from state filename i o buffered reader in state filename ends gz in = buffered reader input stream reader g z i p input stream input stream state filename in = buffered reader reader state filename num docs = 0 line = line = in read line != line starts # fields = line split doc = parse fields 0 token = parse fields 1 type = parse fields 2 topic = parse fields 4 now add topic ! document topics contains key doc document topics put doc t hash map document topics get doc contains key topic document topics get doc increment topic document topics get doc put topic 1 doc >= num docs num docs = doc + 1 in close out loaded topics + num docs + documents load alpha from alpha filename i o now restore saved alpha alpha sum = 0 0 buffered reader in = buffered reader reader alpha filename line = line = in read line != line equals fields = line split \\s+ topic = parse fields 0 alpha topic = 1 0 parse fields 1 alpha sum += alpha topic buffer topic key = buffer i=2 i<fields length i++ topic key append fields i + topic keys topic = topic key to in close out loaded alpha load states from state filename i o doc = 0 state buffered reader in = buffered reader reader state filename line = line = in read line != we assume that sequences are in instance list in order state = parse line document states doc = state additional bookkeeping will be performed when we load sequence ids so states m u s t be loaded before sequences doc++ in close out loaded states load sequence ids from sequence filename i o doc = 0 sequence current sequence = 1 buffered reader in = buffered reader reader sequence filename line = line = in read line != we assume that sequences are in instance list in order fields = line split \ sequence = parse fields 0 document sequence ids doc = sequence sequence != current sequence num sequences ++ current sequence = sequence doc++ in close doc != num docs out warning number documents topics + num docs + not equal to number docs sequence ids + doc + out loaded sequence sample state doc randoms r initializing doc % 10000 == 0 initializing out initializing doc + doc out sampling doc + doc start time = current time millis it s possible document contains no words in which it has no topics and no entry in document topics hash ! document topics contains key doc t hash map topic counts = document topics get doc we are in initializing mode meaningless but it won t hurt old state = document states doc current state topic counts = state topic counts old state look at document features topics we re not in initializing mode reduce topic counts current old state doc length = 0 topic topic counts keys topic count = topic counts get topic ! initializing current state topic counts topic = topic count doc length += topic count ! initializing state topic totals old state = doc length recache state topic old state topic counts previous sequence = 1 doc > 0 previous sequence = document sequence ids doc 1 sequence = document sequence ids doc next sequence = 1 ! initializing doc < num docs 1 next sequence = document sequence ids doc+1 state log likelihoods = num states sampling = num states next state previous state initializing initializing states same sampling them but we only look at previous state and we t decrement any counts previous sequence != sequence sequence start from scratch state = 0 state < num states state++ state log likelihoods state = math log initial state counts state + pi num sequences 1 + sum pi continuation previous state = document states doc 1 state = 0 state < num states state++ state log likelihoods state = math log state state transitions previous state state + gamma infinite state log likelihoods state out infinite end there are four cases previous sequence != sequence sequence != next sequence 1 a singleton document initial state counts old state state = 0 state < num states state++ state log likelihoods state = math log initial state counts state + pi num sequences 1 + sum pi previous sequence != sequence 2 beginning a sequence initial state counts old state next state = document states doc+1 state state transitions old state next state state state transitions old state next state >= 0 state transition totals old state state = 0 state < num states state++ state log likelihoods state = math log state state transitions state next state + gamma initial state counts state + pi num sequences 1 + sum pi infinite state log likelihoods state out infinite beginning sequence != next sequence 3 end a sequence previous state = document states doc 1 state state transitions previous state old state state state transitions previous state old state >= 0 state = 0 state < num states state++ state log likelihoods state = math log state state transitions previous state state + gamma infinite state log likelihoods state out infinite end 4 middle a sequence next state = document states doc+1 state state transitions old state next state state state transitions old state next state < 0 out print state transitions out old state + > + next state out sequence state state transitions old state next state >= 0 state transition totals old state previous state = document states doc 1 state state transitions previous state old state state state transitions previous state old state >= 0 state = 0 state < num states state++ previous state == state state == next state state log likelihoods state = math log state state transitions previous state state + gamma state state transitions state next state + 1 + gamma state transition totals state + 1 + gamma sum previous state == state state log likelihoods state = math log state state transitions previous state state + gamma state state transitions state next state + gamma state transition totals state + 1 + gamma sum state log likelihoods state = math log state state transitions previous state state + gamma state state transitions state next state + gamma state transition totals state + gamma sum infinite state log likelihoods state out infinite middle + doc out previous state + > + state + > + next state out state state transitions previous state state + > + state state transitions state next state + + state transition totals state max = n e g a t i v e i n f i n i t y state = 0 state < num states state++ state log likelihoods state = state transition totals state 10 current state topic counts = state topic counts state current state log gamma cache = topic log gamma cache state total tokens = 0 topic topic counts keys count = topic counts get topic cached sampling state log likelihoods state += current state log gamma cache topic count hybrid count < current state log gamma cache topic length state log likelihoods state += current state log gamma cache topic count i = current state log gamma cache topic length 1 state log likelihoods state += current state log gamma cache topic i i < count i++ state log likelihoods state += math log alpha topic + current state topic counts topic + i j=0 j < count j++ state log likelihoods state += math log alpha topic + current state topic counts topic + j alpha sum + state topic totals state + total tokens na n state log likelihoods state out na n + alpha topic + + + current state topic counts topic + + + j + + + alpha sum + + + state topic totals state + + + total tokens total tokens++ cached sampling state log likelihoods state = doc log gamma cache state doc length hybrid doc length < doc log gamma cache state length state log likelihoods state = doc log gamma cache state doc length i = doc log gamma cache state length 1 state log likelihoods state = doc log gamma cache state i i < doc length i++ state log likelihoods state = math log alpha sum + state topic totals state + i state log likelihoods state > max max = state log likelihoods state sum = 0 0 state = 0 state < num states state++ na n sampling state out state log likelihoods state ! na n sampling state sampling state = math exp state log likelihoods state max sum += sampling state na n sampling state out state log likelihoods state ! na n sampling state doc % 100 == 0 out sampling state state = r next discrete sampling sum document states doc = state topic = 0 topic < num topics topic++ state topic counts state topic += topic counts get topic state topic totals state += doc length recache state topic state topic counts initializing we re initializing states t bother looking at next state previous sequence != sequence initial state counts state ++ previous state = document states doc 1 state state transitions previous state state ++ state transition totals state ++ previous sequence != sequence sequence != next sequence 1 a singleton document initial state counts state ++ previous sequence != sequence 2 beginning a sequence initial state counts state ++ next state = document states doc+1 state state transitions state next state ++ state transition totals state ++ sequence != next sequence 3 end a sequence previous state = document states doc 1 state state transitions previous state state ++ 4 middle a sequence previous state = document states doc 1 state state transitions previous state state ++ next state = document states doc+1 state state transitions state next state ++ state transition totals state ++ print state transitions buffer out = buffer sorter sorted topics = sorter num topics s = 0 s < num states s++ topic=0 topic<num topics topic++ sorted topics topic = sorter topic state topic counts s topic state topic totals s arrays sort sorted topics out append + s + i=0 i<4 i++ topic = sorted topics i get out append state topic counts s topic + + topic keys topic + out append out append + initial state counts s + + num sequences + out append + state transition totals s + t = 0 t < num states t++ out append s == t out append + state state transitions s t + out append state state transitions s t out append out to state transition matrix buffer out = buffer s = 0 s < num states s++ t = 0 t < num states t++ out append state state transitions s t out append out append out to state topics buffer out = buffer s = 0 s < num states s++ topic=0 topic<num topics topic++ out append state topic counts s topic + out append out to i o length != 4 err usage multinomial h m m num topics lda state lda keys sequence metadata exit 0 num topics = parse 0 multinomial h m m hmm = multinomial h m m num topics 1 150 hmm set gamma 1 0 hmm set random seed 1 hmm load alpha from 2 hmm load sequence ids from 3 hmm initialize hmm sample 