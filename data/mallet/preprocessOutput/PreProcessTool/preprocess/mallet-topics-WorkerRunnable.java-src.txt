2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics arrays list zip io text number format types randoms a parallel topic model runnable task author david mimno andrew mc callum worker runnable runnable finished = list< topic assignment> data start doc num docs num topics number topics to be fit these values are used to encode type topic counts count topic pairs in a single topic mask topic bits num types alpha dirichlet alpha alpha over topics alpha sum beta prior on per topic multinomial over words beta sum d e f a u l t b e t a = 0 01 smoothing only mass = 0 0 cached coefficients type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> dirichlet estimation doc length counts histogram document sizes topic doc counts histogram document topic counts indexed <topic index sequence position index> should save state = should build local counts = randoms random worker runnable num topics alpha alpha sum beta randoms random list< topic assignment> data type topic counts tokens per topic start doc num docs data = data num topics = num topics num types = type topic counts length bit count num topics == 1 exact power 2 topic mask = num topics 1 topic bits = bit count topic mask otherwise add an extra bit topic mask = highest one bit num topics 2 1 topic bits = bit count topic mask type topic counts = type topic counts tokens per topic = tokens per topic alpha sum = alpha sum alpha = alpha beta = beta beta sum = beta num types random = random start doc = start doc num docs = num docs cached coefficients = num topics err worker runnable thread + num topics + topics + topic bits + topic bits + to binary topic mask + topic mask there only one thread we t need to go through communication overhead asks worker not to prepare local type topic counts should be called when we are using in a non threaded environment make only thread should build local counts = get tokens per topic tokens per topic get type topic counts type topic counts get doc length counts doc length counts get topic doc counts topic doc counts initialize alpha statistics size doc length counts = size topic doc counts = num topics size collect alpha statistics should save state = reset beta beta beta sum beta = beta beta sum = beta sum once we have sampled local counts trash global type topic counts and reuse space to build a summary type topic counts specific to worker s section corpus build local type topic counts clear topic totals arrays fill tokens per topic 0 clear type topic counts only looking at entries before first 0 entry type = 0 type < type topic counts length type++ topic counts = type topic counts type position = 0 position < topic counts length topic counts position > 0 topic counts position = 0 position++ doc = start doc doc < data size doc < start doc + num docs doc++ topic assignment document = data get doc feature sequence tokens = feature sequence document instance get data feature sequence topic sequence = feature sequence document topic sequence topics = topic sequence get features position = 0 position < tokens size position++ topic = topics position topic == parallel topic model u n a s s i g n e d t o p i tokens per topic topic ++ format these arrays topic in rightmost bits count in remaining left bits since count in high bits sorting desc numeric value guarantees that higher counts will be before lower counts type = tokens get index at position position current type topic counts = type topic counts type start assuming that either empty or in sorted descending order here we are only adding counts so we find an existing location topic we only need to ensure that it not larger than its left neighbor index = 0 current topic = current type topic counts index topic mask current value current type topic counts index > 0 current topic != topic index++ index == current type topic counts length out overflow on type + type current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits current value == 0 value 1 so we t have to worry about sorting except topic suffix which doesn t matter current type topic counts index = 1 << topic bits + topic current type topic counts index = current value + 1 << topic bits + topic now ensure that still sorted bubbling value up index > 0 current type topic counts index > current type topic counts index 1 temp = current type topic counts index current type topic counts index = current type topic counts index 1 current type topic counts index 1 = temp index run ! finished out already running! finished = initialize smoothing only sampling bucket smoothing only mass = 0 initialize cached coefficients using only smoothing these values will be selectively replaced in documents non zero counts in particular topics topic=0 topic < num topics topic++ smoothing only mass += alpha topic beta tokens per topic topic + beta sum cached coefficients topic = alpha topic tokens per topic topic + beta sum doc = start doc doc < data size doc < start doc + num docs doc++ doc % 10000 == 0 out processing doc + doc feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence sample topics one doc token sequence topic sequence should build local counts build local type topic counts should save state = finished = e finished = e print stack trace sample topics one doc feature sequence token sequence feature sequence topic sequence readjust topics and stats currently ignored one doc topics = topic sequence get features current type topic counts type old topic topic topic weights sum doc length = token sequence get length local topic counts = num topics local topic index = num topics populate topic counts position = 0 position < doc length position++ one doc topics position == parallel topic model u n a s s i g n e d t o p i local topic counts one doc topics position ++ build an that densely lists topics that have non zero counts dense index = 0 topic = 0 topic < num topics topic++ local topic counts topic != 0 local topic index dense index = topic dense index++ record total number non zero topics non zero topics = dense index initialize topic count beta sampling bucket topic beta mass = 0 0 initialize cached coefficients and topic beta normalizing constant dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index n = local topic counts topic initialize normalization constant b n t|d term topic beta mass += beta n tokens per topic topic + beta sum update coefficients non zero topics cached coefficients topic = alpha topic + n tokens per topic topic + beta sum topic term mass = 0 0 topic term scores = num topics topic term indices topic term values i score iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = one doc topics position current type topic counts = type topic counts type old topic != parallel topic model u n a s s i g n e d t o p i remove token from all counts remove topic s contribution to normalizing constants smoothing only mass = alpha old topic beta tokens per topic old topic + beta sum topic beta mass = beta local topic counts old topic tokens per topic old topic + beta sum decrement local doc topic counts local topic counts old topic maintain dense index we are deleting old topic local topic counts old topic == 0 first get to dense location associated old topic dense index = 0 we know it s in there somewhere so we t need bounds checking local topic index dense index != old topic dense index++ shift all remaining dense indices to left dense index < non zero topics dense index < local topic index length 1 local topic index dense index = local topic index dense index + 1 dense index++ non zero topics decrement global topic count totals tokens per topic old topic tokens per topic old topic >= 0 old topic + old topic + below 0 add old topic s contribution back into normalizing constants smoothing only mass += alpha old topic beta tokens per topic old topic + beta sum topic beta mass += beta local topic counts old topic tokens per topic old topic + beta sum reset cached coefficient topic cached coefficients old topic = alpha old topic + local topic counts old topic tokens per topic old topic + beta sum now go over type topic counts decrementing where appropriate and calculating score each topic at same time index = 0 current topic current value already decremented = old topic == parallel topic model u n a s s i g n e d t o p i topic term mass = 0 0 index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits ! already decremented current topic == old topic we re decrementing and adding up sampling weights at same time but decrementing may require us to reorder topics so after we re done here look at cell in again current value current value == 0 current type topic counts index = 0 current type topic counts index = current value << topic bits + old topic shift reduced value to right necessary sub index = index sub index < current type topic counts length 1 current type topic counts sub index < current type topic counts sub index + 1 temp = current type topic counts sub index current type topic counts sub index = current type topic counts sub index + 1 current type topic counts sub index + 1 = temp sub index++ already decremented = score = cached coefficients current topic current value topic term mass += score topic term scores index = score index++ sample = random next uniform smoothing only mass + topic beta mass + topic term mass orig sample = sample make sure it actually gets set topic = 1 sample < topic term mass topic term count++ i = 1 sample > 0 i++ sample = topic term scores i topic = current type topic counts i topic mask current value = current type topic counts i >> topic bits current type topic counts i = current value + 1 << topic bits + topic bubble value up necessary i > 0 current type topic counts i > current type topic counts i 1 temp = current type topic counts i current type topic counts i = current type topic counts i 1 current type topic counts i 1 = temp i sample = topic term mass sample < topic beta mass beta topic count++ sample = beta dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index sample = local topic counts topic tokens per topic topic + beta sum sample <= 0 0 topic = topic smoothing only count++ sample = topic beta mass sample = beta topic = 0 sample = alpha topic tokens per topic topic + beta sum sample > 0 0 topic++ sample = alpha topic tokens per topic topic + beta sum move to position topic which may be first empty position a topic word index = 0 current type topic counts index > 0 current type topic counts index topic mask != topic index++ index == current type topic counts length err type + type + topic + topic k=0 k<current type topic counts length k++ err print current type topic counts k topic mask + + current type topic counts k >> topic bits + err index should now be set to position topic which may be an empty cell at end list current type topic counts index == 0 inserting a topic guaranteed to be in order w r t count not topic current type topic counts index = 1 << topic bits + topic current value = current type topic counts index >> topic bits current type topic counts index = current value + 1 << topic bits + topic bubble increased value left necessary index > 0 current type topic counts index > current type topic counts index 1 temp = current type topic counts index current type topic counts index = current type topic counts index 1 current type topic counts index 1 = temp index topic == 1 err worker runnable sampling + orig sample + + sample + + smoothing only mass + + topic beta mass + + topic term mass topic = num topics 1 t o d o appropriate illegal state worker runnable topic not sampled topic != 1 put that topic into counts one doc topics position = topic smoothing only mass = alpha topic beta tokens per topic topic + beta sum topic beta mass = beta local topic counts topic tokens per topic topic + beta sum local topic counts topic ++ a topic document add topic to dense index local topic counts topic == 1 first find point where we should insert topic going to end which only reason we re keeping track number non zero topics and working backwards dense index = non zero topics dense index > 0 local topic index dense index 1 > topic local topic index dense index = local topic index dense index 1 dense index local topic index dense index = topic non zero topics++ tokens per topic topic ++ update coefficients non zero topics cached coefficients topic = alpha topic + local topic counts topic tokens per topic topic + beta sum smoothing only mass += alpha topic beta tokens per topic topic + beta sum topic beta mass += beta local topic counts topic tokens per topic topic + beta sum should save state update document topic count histogram dirichlet estimation doc length counts doc length ++ dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index topic doc counts topic local topic counts topic ++ clean up our mess reset coefficients to values only smoothing next doc will update its own non zero topics dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index cached coefficients topic = alpha topic tokens per topic topic + beta sum 