2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e author andrew mc callum <a href= mailto >mccallum edu< a> tests io io input stream io output stream io i o io input stream io output stream io print writer io serializable io reader io writer random regex pattern junit framework test junit framework test junit framework test suite types alphabet types feature sequence types feature vector types feature vector sequence types instance types instance list types matrix ops types sequence types sparse vector types token types token sequence pipe sequence2 token sequence pipe line group string2 token sequence pipe noop pipe pipe pipe print input and target pipe serial pipes pipe target2 label sequence pipe token sequence2 feature vector sequence pipe token sequence lowercase pipe token sequence match data and target pipe token sequence parse feature pipe iterator iterator pipe iterator line group iterator pipe tsf offset conjunctions pipe tsf token text r f r f trainer label likelihood r f trainer stochastic gradient max lattice max lattice sum lattice sum lattice sum lattice scaling token accuracy evaluator transducer optimize optimizable optimize tests test optimizable utils t o d o gsc 08 25 08 some tests fail because tests are using r f trainer label likelihood instead r f optimizable label likelihood and r f optimizable value gradients tests r f training test r f test test r f name name data = free a matter users freedom to run copy distribute study change and improve more precisely it refers to four kinds freedom users freedom to run program any purpose freedom to study how program works and adapt it to your needs freedom to redistribute copies so you can help your neighbor freedom to improve program and release your improvements to so that whole community benefits a program free users have all these freedoms thus you should be free to redistribute copies either or without modifications either gratis or charging a fee to anyone anywhere being free to these things means among other things that you not have to ask or pay permission you should also have freedom to make modifications and use them privately in your own work or play without even mentioning that they you publish your changes you should not be required to notify anyone in particular or in any particular way in order freedoms to make changes and to publish improved versions to be meaningful you must have access to source program therefore accessibility source a necessary condition free note that criteria such those stated in free definition require careful thought their interpretation to decide whether a specific qualifies a free we judge it based on these criteria to determine whether it fits their spirit well precise words a includes unconscionable restrictions we reject it even we did not anticipate issue in these criteria sometimes a requirement raises an issue that calls extensive thought including discussions a lawyer before we can decide requirement acceptable when we reach a conclusion about a issue we often update these criteria to make it easier to why certain licenses or t qualify in order these freedoms to be real they must be irrevocable you nothing wrong developer has power to revoke without your doing anything to give cause not free however certain kinds rules about manner distributing free are acceptable when they t conflict central freedoms example copyleft very simply stated rule that when redistributing program you cannot add restrictions to deny other people central freedoms rule does not conflict central freedoms rather it protects them thus you may have paid money to get copies free or you may have obtained copies at no charge but regardless how you got your copies you always have freedom to copy and change even to sell copies rules about how to a modified are acceptable they t effectively block your freedom to release modified versions rules that ``if you make program available in way you must make it available in that way also can be acceptable too on same condition note that such a rule still leaves you choice whether to publish program or not it also acceptable to require that you have distributed a modified and a previous developer asks a copy it you must send one sometimes government export control regulations and trade sanctions can constrain your freedom to distribute copies programs internationally developers not have power to eliminate or override these restrictions but what they can and must refuse to impose them conditions use program in way restrictions will not affect activities and people outside jurisdictions these governments note that criteria such those stated in free definition require careful thought their interpretation to decide whether a specific qualifies a free we judge it based on these criteria to determine whether it fits their spirit well precise words a includes unconscionable restrictions we reject it even we did not anticipate issue in these criteria sometimes a requirement raises an issue that calls extensive thought including discussions a lawyer before we can decide requirement acceptable when we reach a conclusion about a issue we often update these criteria to make it easier to why certain licenses or t qualify g n u project was launched in 1984 to develop a complete unix like operating which free g n u test get set input vocab size = 100 num states = 5 alphabet input alphabet = alphabet i = 0 i < input vocab size i++ input alphabet lookup index feature + i alphabet output alphabet = alphabet r f crf = r f input alphabet output alphabet state names = num states i = 0 i < num states i++ state names i = state + i crf add fully connected states state names r f trainer label likelihood crft = r f trainer label likelihood crf optimizable gradient value mcrf = crft get optimizable r f instance list test optimizable test get set mcrf test sum log prob w1 = math log 2 w2 = math log 8 s1 = math log 2 + 8 s2 = transducer sum log prob w1 w2 equals s1 s2 0 00001 w1 = math log 99999 w2 = math log 0001 s1 = math log 99999 0001 s2 = transducer sum log prob w1 w2 equals s1 s2 0 00001 test sum lattice input vocab size = 1 num states = 2 alphabet input alphabet = alphabet i = 0 i < input vocab size i++ input alphabet lookup index feature + i alphabet output alphabet = alphabet r f crf = r f input alphabet output alphabet state names = num states i = 0 i < num states i++ state names i = state + i crf add fully connected states state names crf set weights dimension densely crf get state 0 set initial weight 1 0 crf get state 1 set initial weight transducer i m p o s s i b l e w e i g h t crf get state 0 set weight 0 0 crf get state 1 set weight 0 0 crf set parameter 0 0 0 transducer i m p o s s i b l e w e i g h t state0 self transition crf set parameter 0 1 0 1 0 state0 >state1 crf set parameter 1 1 0 1 0 state1 self transition crf set parameter 1 0 0 transducer i m p o s s i b l e w e i g h t state1 >state0 feature vector sequence fvs = feature vector sequence feature vector feature vector alphabet crf get input alphabet 1 feature vector alphabet crf get input alphabet 1 feature vector alphabet crf get input alphabet 1 sum lattice lattice = sum lattice crf fvs we start in state0 lattice get gamma probability 0 crf get state 0 == 1 0 lattice get gamma probability 0 crf get state 1 == 0 0 we go to state1 lattice get gamma probability 1 crf get state 0 == 0 0 lattice get gamma probability 1 crf get state 1 == 1 0 and on through a self transition lattice get xi probability 1 crf get state 1 crf get state 1 == 1 0 lattice get xi probability 1 crf get state 1 crf get state 0 == 0 0 lattice weight = + lattice get total weight lattice get total weight == 4 0 gammas at all times sum to 1 0 time = 0 time < lattice length 1 time++ gammasum = lattice get gamma probability time crf get state 0 + lattice get gamma probability time crf get state 1 equals gammas at time step + time + sum to + gammasum 1 0 gammasum 0 0001 xis at all times sum to 1 0 time = 0 time < lattice length 1 time++ xissum = lattice get xi probability time crf get state 0 crf get state 0 + lattice get xi probability time crf get state 0 crf get state 1 + lattice get xi probability time crf get state 1 crf get state 0 + lattice get xi probability time crf get state 1 crf get state 1 equals xis at time step + time + sum to + xissum 1 0 xissum 0 0001 test max lattice input vocab size = 1 num states = 2 alphabet input alphabet = alphabet i = 0 i < input vocab size i++ input alphabet lookup index feature + i alphabet output alphabet = alphabet r f crf = r f input alphabet output alphabet state names = num states i = 0 i < num states i++ state names i = state + i crf add fully connected states state names crf set weights dimension densely crf get state 0 set initial weight 1 0 crf get state 1 set initial weight transducer i m p o s s i b l e w e i g h t crf get state 0 set weight 0 0 crf get state 1 set weight 0 0 crf set parameter 0 0 0 transducer i m p o s s i b l e w e i g h t state0 self transition crf set parameter 0 1 0 1 0 state0 >state1 crf set parameter 1 1 0 1 0 state1 self transition crf set parameter 1 0 0 transducer i m p o s s i b l e w e i g h t state1 >state0 feature vector sequence fvs = feature vector sequence feature vector feature vector alphabet crf get input alphabet 1 feature vector alphabet crf get input alphabet 1 feature vector alphabet crf get input alphabet 1 max lattice lattice = max lattice crf fvs sequence< transducer state> viterbi path = lattice best state sequence we start in state0 viterbi path get 0 == crf get state 0 we go to state1 viterbi path get 1 == crf get state 1 and on through a self transition to state1 again viterbi path get 2 == crf get state 1 should print at end 4 4 3 unconstrained weight=2912 0 constrained weight=428 0 max weight=35770 0 min grad=520 0 test cost use save input vocab size = 4 num states = 5 create a to store r f f = test obj f2 = test object2 obj alphabet input alphabet = alphabet i = 0 i < input vocab size i++ input alphabet lookup index feature + i alphabet output alphabet = alphabet state names = num states i = 0 i < num states i++ state names i = state + i output alphabet lookup index state names i r f crf = r f input alphabet output alphabet r f save r f = crf input alphabet = feature alphabet crf get input alphabet feature vector sequence fvs = feature vector sequence feature vector feature vector crf get input alphabet 1 2 3 feature vector crf get input alphabet 1 2 3 feature vector crf get input alphabet 1 2 3 feature vector crf get input alphabet 1 2 3 feature sequence ss = feature sequence crf get output alphabet 0 1 2 3 instance list ilist = instance list noop input alphabet output alphabet ilist add fvs ss crf add fully connected states state names r f trainer label likelihood crft = r f trainer label likelihood crf crft set use sparse weights use save output stream oos = output stream output stream f oos write crf oos close i o e err writing + e err wrote out r f err r f hyperbolic prior slope + crft get use hyperbolic prior slope + hyperbolic prior sharpness + crft get use hyperbolic prior sharpness + gaussian prior variance + crft get gaussian prior variance and read it back in crf = input stream ois = input stream input stream f crf = r f ois read ois close i o e err reading + e not found cnfe err cound not find reading in + cnfe err read in r f err r f hyperbolic prior slope + crft get use hyperbolic prior slope + hyperbolic prior sharpness + crft get use hyperbolic prior sharpness + gaussian prior variance + crft get gaussian prior variance output stream oos = output stream output stream f2 oos write crf oos close i o e err writing + e err wrote out r f crf = save r f optimizable gradient value mcrf = crft get optimizable r f ilist unconstrained weight = sum lattice crf fvs get total weight constrained weight = sum lattice crf fvs ss get total weight optimizable value = 0 gradient norm = 0 gradient = mcrf get num out unconstrained cost= +unconstrained cost+ constrained cost= +constrained cost i = 0 i < num states i++ j = 0 j < num states j++ k = 0 k < input vocab size k++ crf set parameter i j k k + i + j k i + i j unconstrained weight = sum lattice crf fvs get total weight constrained weight = sum lattice crf fvs ss get total weight optimizable value = mcrf get value mcrf get value gradient gradient gradient norm = matrix ops one norm gradient out + i + + j + + k + unconstrained weight = + unconstrained weight + constrained weight = + constrained weight + optimizable value = + optimizable value + gradient norm = + gradient norm value should be 35770 but + optimizable value math abs optimizable value + 35770 < 0 001 math abs gradient norm 520 < 0 001 test cost test cost test cost serialized test cost test increment test r f token sequence remove spaces pipe serializable test r f token sequence remove spaces alphabet instance pipe instance carrier token sequence ts = token sequence carrier get data token sequence ts = token sequence feature sequence label seq = feature sequence get target alphabet last was space = buffer sb = buffer i = 0 i < ts size i++ token t = ts get i t get text equals last was space = sb append t get text ts add t label seq add last was space ? start notstart last was space = target processing carrier set target label seq carrier set data ts carrier set source sb to carrier serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n read input stream in i o not found = in read test r f2 pipe serializable test r f2 instance pipe instance carrier buffer sb = buffer source = carrier get source sequence = sequence carrier get target start label index = get alphabet lookup index start i = 0 i < source length i++ out target + i + = + get i to get i to equals start i != 0 sb append sb append source at i carrier set source sb to out carrier get source = + carrier get source carrier serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n read input stream in i o not found = in read test value gradient test space prediction test train test space prediction test space prediction test value and gradient pipe p = make space prediction pipe pipe p2 = test r f2 instance list instances = instance list p instances add thru pipe iterator data instance list lists = instances split random 1 5 5 r f crf = r f p p2 crf add fully connected states labels r f trainer label likelihood crft = r f trainer label likelihood crf test value and gradient optimizable gradient value optable = crft get optimizable r f lists 0 test optimizable test value and gradient minable gradient = optable get num optable get value gradient gradient test optimizable test value and gradient in direction optable gradient test optimizable test value and gradient current optable test optimizable test value and gradient optable tests at current and at purturbed toward gradient out training accuracy before training = + crf average token accuracy lists 0 out testing accuracy before training = + crf average token accuracy lists 1 out training crft train incremental lists 0 out training accuracy after training = + crf average token accuracy lists 0 out testing accuracy after training = + crf average token accuracy lists 1 out training results i = 0 i < lists 0 size i++ instance inst = lists 0 get i sequence input = sequence inst get data sequence output = crf transduce input out output out testing results i = 0 i < lists 1 size i++ instance inst = lists 1 get i sequence input = sequence inst get data sequence output = crf transduce input out output test space prediction test value and gradient use saved use sparse weights pipe p = make space prediction pipe r f saved r f f = test obj instance list instances = instance list p instances add thru pipe iterator data instance list lists = instances split 5 5 r f crf = r f p get data alphabet p get target alphabet crf add fully connected states labels r f trainer label likelihood crft = r f trainer label likelihood crf crft set use sparse weights use sparse weights test value and gradient optimizable gradient value minable = crft get optimizable r f lists 0 test optimizable test value and gradient minable out training accuracy before training = + crf average token accuracy lists 0 out testing accuracy before training = + crf average token accuracy lists 1 saved r f = crf out training serialized crf crft train incremental lists 0 pre train acc = crf average token accuracy lists 0 pre test acc = crf average token accuracy lists 1 out training accuracy after training = + pre train acc out testing accuracy after training = + pre test acc output stream oos = output stream output stream f oos write crf oos close i o e err writing + e err wrote out r f err r f hyperbolic prior slope + crft get use hyperbolic prior slope + hyperbolic prior sharpness + crft get use hyperbolic prior sharpness + gaussian prior variance + crft get gaussian prior variance and read it back in use saved crf = input stream ois = input stream input stream f crf = r f ois read ois close i o e err reading + e not found cnfe err cound not find reading in + cnfe err read in r f crf = saved r f post train acc = crf average token accuracy lists 0 post test acc = crf average token accuracy lists 1 out training accuracy after saving = + post train acc out testing accuracy after saving = + post test acc equals post train acc pre train acc 0 0001 equals post test acc pre test acc 0 0001 pipe make space prediction pipe pipe p = serial pipes pipe sequence2 token sequence token sequence lowercase test r f token sequence remove spaces token text offset conjunctions 0 1 1 0 original test had conjunction in it too 1 1 0 0 1 0 1 i d like to comment out next line to make it run faster but then we d need to adjust likelihood and accuracy test values akm 12 2007 t o d o uncomment line 2 1 0 0 1 2 3 2 1 1 2 3 these were commented before 2 1 1 0 0 1 1 2 3 2 1 2 1 0 1 0 1 0 1 2 1 2 3 print input and target token sequence2 feature vector sequence p test add order n states pipe p = make space prediction pipe instance list instances = instance list p instances add thru pipe iterator data instance list lists = instances split random 678 5 5 compare 3 r fs trained add order n states and make sure that having more features leads to a higher likelihood r f crf1 = r f p get data alphabet p get target alphabet crf1 add order n states lists 0 1 s t a r t r f trainer label likelihood crf1 train incremental lists 0 r f crf2 = r f p get data alphabet p get target alphabet crf2 add order n states lists 0 1 2 s t a r t r f trainer label likelihood crf2 train incremental lists 0 r f crf3 = r f p get data alphabet p get target alphabet crf3 add order n states lists 0 1 2 s t a r t r f trainer label likelihood crf3 train incremental lists 0 prevent cached values lik1 = get likelihood crf1 lists 0 lik2 = get likelihood crf2 lists 0 lik3 = get likelihood crf3 lists 0 out r f1 likelihood + lik1 zero order likelihood < + lik1 + > greater than first order < + lik2 + > lik1 < lik2 defaults only likelihood < + lik2 + > greater than full first order < + lik3 + > lik2 < lik3 equals 167 2234457483949 lik1 0 0001 equals 165 81326484466342 lik2 0 0001 equals 90 37680146432787 lik3 0 0001 get likelihood r f crf instance list data r f trainer label likelihood crft = r f trainer label likelihood crf optimizable gradient value mcrf = crft get optimizable r f data elaborate thing so that crf cached value stale forced params = mcrf get num mcrf get params mcrf set params mcrf get value test frozen weights pipe p = make space prediction pipe instance list instances = instance list p instances add thru pipe iterator data r f crf1 = r f p get data alphabet p get target alphabet crf1 add fully connected states labels r f trainer label likelihood crft1 = r f trainer label likelihood crf1 crft1 train incremental instances r f crf2 = r f p get data alphabet p get target alphabet crf2 add fully connected states labels freeze some weights before training i = 0 i < crf2 get weights length i += 2 crf2 freeze weights i r f trainer label likelihood crft2 = r f trainer label likelihood crf2 crft2 train incremental instances sparse vector w = crf2 get weights b = crf2 get weights i = 0 i < w length i += 2 equals 0 0 b i 1e 10 loc = 0 loc < w i num locations loc++ equals 0 0 w i value at location loc 1e 10 check that frozen weights has worse likelihood optimizable gradient value optable1 = crft1 get optimizable r f instances optimizable gradient value optable2 = crft2 get optimizable r f instances val1 = optable1 get value val2 = optable2 get value freezing weights does not harm log likelihood! full + val1 + frozen + val2 val1 > val2 test dense train test space prediction test train stochastic gradient pipe p = make space prediction pipe pipe p2 = test r f2 instance list instances = instance list p instances add thru pipe iterator data instance list lists = instances split 5 5 r f crf = r f p p2 crf add fully connected states labels crf set weights dimension in lists 0 r f trainer stochastic gradient crft = r f trainer stochastic gradient crf 0 0001 out training accuracy before training = + crf average token accuracy lists 0 out testing accuracy before training = + crf average token accuracy lists 1 out training either fixed rate or selected on a sample crft set rate likelihood lists 0 crft set rate 0 01 crft train lists 0 100 crf print out training accuracy after training = + crf average token accuracy lists 0 out testing accuracy after training = + crf average token accuracy lists 1 test sum lattice implementations pipe p = make space prediction pipe pipe p2 = test r f2 first normal training getting weights instance list instances = instance list p instances add thru pipe iterator data instance list lists = instances split 5 5 r f crf = r f p p2 crf add fully connected states labels crf set weights dimension in lists 0 r f trainer stochastic gradient crft = r f trainer stochastic gradient crf 0 0001 out training accuracy before training = + crf average token accuracy lists 0 out testing accuracy before training = + crf average token accuracy lists 1 out training either fixed rate or selected on a sample crft set rate likelihood lists 0 crft set rate 0 01 crft train lists 0 100 crf print out training accuracy after training = + crf average token accuracy lists 0 out testing accuracy after training = + crf average token accuracy lists 1 now check speeds sum lattice vs sum lattice scaling total time = 0 total time scaling = 0 iter = 0 iter < 10000 iter++ ii = 0 ii < lists 1 size ii++ feature vector sequence input = feature vector sequence lists 1 get ii get data total time = current time millis sum lattice lattice = sum lattice crf input total time += current time millis total time scaling = current time millis sum lattice scaling lattice = sum lattice scaling crf input total time scaling += current time millis iter == 0 check that total weight same equals lattice get total weight scaling lattice get total weight 0 0001 check that gammas g1 = lattice get gammas g2 = scaling lattice get gammas i = 0 i < g1 length i++ j = 0 j < g1 i length j++ equals g1 i j g2 i j 0 0001 check that xis match x1 = lattice get xis x2 = scaling lattice get xis i = 0 i < x1 length i++ j = 0 j < x1 i length j++ k = 0 k < x1 i j length k++ equals x1 i j k x2 i j k 0 0001 iter + 1 % 100 == 0 out print iter + 1 + out flush iter + 1 % 1000 == 0 out out out time in ms = + total time out time in ms scaling = + total time scaling total time scaling > total time out sum lattice f t w!! time diff= + total time scaling total time + ms out sum lattice scaling f t w!! time diff= + total time total time scaling + ms test serialization test space prediction test dense serialization test space prediction test token accuracy pipe p = make space prediction pipe instance list instances = instance list p instances add thru pipe iterator data instance list lists = instances split random 777 5 5 r f crf = r f p get data alphabet p get target alphabet crf add fully connected states labels r f trainer label likelihood crft = r f trainer label likelihood crf crft set use sparse weights crft train incremental lists 0 token accuracy evaluator eval = token accuracy evaluator lists train test eval evaluate instance list crft lists 1 test equals 0 9409 eval get accuracy test 0 001 test print pipe p = serial pipes pipe sequence2 token sequence token text test r f token sequence remove spaces token sequence2 feature vector sequence print input and target instance list one = instance list p data = a b d e one add thru pipe iterator data r f crf = r f p crf add fully connected states three quarter labels one r f trainer label likelihood crft = r f trainer label likelihood crf crf set weights dimension in one optimizable mcrf = crft get optimizable r f one params = mcrf get num i = 0 i < params length i++ params i = i mcrf set params crf print test copy states and weights pipe p = serial pipes pipe sequence2 token sequence token text test r f token sequence remove spaces token sequence2 feature vector sequence print input and target instance list one = instance list p data = a b d e one add thru pipe iterator data r f crf = r f p crf add fully connected states labels r f trainer label likelihood crft = r f trainer label likelihood crf crf set weights dimension in one optimizable gradient value mcrf = crft get optimizable r f one params = mcrf get num i = 0 i < params length i++ params i = i mcrf set params writer out = writer crf print print writer out out r f1 crf print make a copy r f r f crf2 = r f crf writer out2 = writer crf2 print print writer out2 out r f2 crf2 print equals out to out2 to val1 = mcrf get value r f trainer label likelihood crft2 = r f trainer label likelihood crf2 val2 = crft2 get optimizable r f one get value equals val1 val2 1e 5 toy = a a b b d d b b test start state pipe p = serial pipes pipe line group string2 token sequence token sequence match data and target pattern compile ^ \\ s+ 2 1 token sequence parse feature token text token sequence2 feature vector sequence target2 label sequence print input and target instance list data = instance list p data add thru pipe line group iterator reader toy pattern compile r f crf = r f p crf print crf add states labels connected in data crf add start state r f trainer label likelihood crft = r f trainer label likelihood crf optimizable gradient value maxable = crft get optimizable r f data equals 1 3862 maxable get value 1e 4 crf = r f p crf add order n states data 1 a crf print crft = r f trainer label likelihood crf maxable = crft get optimizable r f data equals 3 09104245335831 maxable get value 1e 4 tests that set weights dimension densely respects feature selections test dense feature selection pipe p = make space prediction pipe instance list instances = instance list p instances add thru pipe iterator data test that dense observations wights aren t added feature edges r f crf1 = r f p crf1 add order n states instances 0 start r f trainer label likelihood crft1 = r f trainer label likelihood crf1 crft1 set use sparse weights crft1 train instances 1 set weights dimension n params1 = crft1 get optimizable r f instances get num r f crf2 = r f p crf2 add order n states instances 0 1 start r f trainer label likelihood crft2 = r f trainer label likelihood crf2 crft2 set use sparse weights crft2 train instances 1 set weights dimension n params2 = crft2 get optimizable r f instances get num equals n params2 n params1 + 4 test xis pipe p = make space prediction pipe instance list instances = instance list p instances add thru pipe iterator data r f crf1 = r f p crf1 add fully connected states labels r f trainer label likelihood crft1 = r f trainer label likelihood crf1 crft1 train instances 10 let s get some instance inst = instances get 0 sequence input = sequence inst get data sum lattice lattice = sum lattice crf1 input sequence inst get target ip = 0 ip < lattice length 1 ip++ i = 0 i < crf1 num states i++ transducer state state = crf1 get state i transducer transition iterator it = state transition iterator input ip gamma = lattice get gamma probability ip state xi sum = 0 it has next transducer state dest = it next state xi = lattice get xi probability ip state dest xi sum += xi equals gamma xi sum 1e 5 test suite test suite test r f test state add weights pipe p = make space prediction pipe used to be m e m m make space prediction pipe but i t know why akm 12 2007 instance list training = instance list p training add thru pipe iterator data used to be m e m m data but i t know why akm 12 2007 r f crf = r f p crf add fully connected states labels r f trainer label likelihood crft = r f trainer label likelihood crf crft train incremental training check that notstart state used at test time sequence input = sequence training get 0 get data sequence output = max lattice crf input best output sequence notstart found = i = 0 i < output size i++ output get i to equals notstart notstart found = err output to notstart found now add infinite weight onto a transition and make sure that it s honored r f state state = crf get state notstart widx = crf get weights index bad bad num features = crf get input alphabet size sparse vector w = sparse vector num features w set all n e g a t i v e i n f i n i t y crf set weights widx w state add weight 0 bad bad state add weight 1 bad bad verify that effectively prevents notstart state from being used output = max lattice crf input best output sequence notstart found = i = 0 i < output size 1 i++ output get i to equals notstart notstart found = !notstart found old crf = test base crf cnl03 ser gz test = john n n p b n p o doe n n p i n p o said v b z b v p o hi n n b n p o skiptest old crf r f crf = r f utils read old crf instance inst = crf get input pipe instance from instance test sequence output = crf transduce sequence inst get data std = output to equals b p e r i p e r o o std test suite suite length > 0 suite = test suite i = 0 i < length i++ suite add test test r f i suite = test suite suite junit textui test runner run suite 