2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e classify users culotta cluster classify base classify io i o io input stream io output stream list arrays iterator list logging logger optimize conjugate gradient optimize limited memory b f g s optimize optimizable optimize optimizer types alphabet types exp gain types feature inducer types feature selection types feature vector types feature vector sequence types gradient gain types info gain types instance types instance list types label types label alphabet types label vector types labels types matrix ops types ranked feature vector command option logger progress message logger maths trainer a link rank max ent classifier expects instance data to be a feature vector sequence and target to be a representation index best feature vector sequence note that instance target may be a labels to indicate a tie best instance author aron culotta <a href= mailto culotta >culotta edu< a> rank max ent trainer max ent trainer logger logger = logger get logger rank max ent trainer get name logger progress logger = progress message logger get logger rank max ent trainer get name + pl rank max ent trainer constructs a trainer a parameter to avoid overtraining 1 0 usually a reasonable value rank max ent trainer gaussian prior variance gaussian prior variance optimizable gradient value get maximizable trainer instance list ilist ilist == maximizable trainer maximizable trainer ilist max ent train instance list training set logger fine training set size = +training set size rank max ent trainer maximizable trainer mt = rank max ent trainer maximizable trainer training set rank max ent initial classifier optimizer maximizer = limited memory b f g s mt maximizer optimize x x x given loop below seems wrong converged i = 0 i < num iterations i++ converged = maximizer optimize 1 illegal argument e e print stack trace logger info catching saying converged converged = converged num iterations == m a x v a l u e run it again because in our and sam roweis experience b f g s can still eke out more likelihood after first convergence re running without being restricted its gradient history optimizer = conjugate gradient mt optimizer optimize illegal argument e e print stack trace logger info catching saying converged progress logger info progess messages are on one line move on mt get classifier xxx won t work here must fix <p> like other <code>train feature induction< code> but allows some options to be changed < p> maxent an initial partially trained classifier <code>null< code> classifier may be modified during training gain name estimate gain log likelihood increase we want our chosen features to maximize should be one <code> max ent trainer e x p g a i n< code> <code> max ent trainer g r a d i e n t g a i n< code> or <code> max ent trainer i n f o r m a t i o n g a i n< code> <code> e x p g a i n< code> trained <code> max ent< code> classifier classifier train feature induction instance list training data instance list validation data instance list testing data classifier evaluating evaluator max ent maxent total iterations num iterations between feature inductions num feature inductions num features per feature induction gain name x x x ought to be a parameter except that setting it to can crash training jump too small save during f i = alphabet input alphabet = training data get data alphabet alphabet output alphabet = training data get target alphabet maxent == maxent = rank max ent training data get pipe 1+input alphabet size output alphabet size training iteration = 0 num labels = output alphabet size initialize feature selection feature selection global f s = training data get feature selection global f s == mask out all features some will be added later feature inducer induce features global f s = feature selection training data get data alphabet training data set feature selection global f s validation data != validation data set feature selection global f s testing data != testing data set feature selection global f s maxent = rank max ent maxent get instance pipe maxent get global f s run feature induction feature induction iteration = 0 feature induction iteration < num feature inductions feature induction iteration++ print out some feature logger info feature induction iteration +feature induction iteration train model a little bit we t care whether it converges we execute all feature induction iterations no matter what feature induction iteration != 0 t train until we have added some features set num iterations num iterations between feature inductions maxent = rank max ent train training data validation data testing data evaluator maxent training iteration += num iterations between feature inductions logger info starting feature induction + 1+input alphabet size + features over +num labels+ labels create list tokens instance list instances = instance list training data get data alphabet training data get target alphabet instance list instances = instance list input alphabet output alphabet instances feature selection will get examined feature inducer so it can know how to add singleton features instances set feature selection global f s list label vectors = list these are length 1 vectors i = 0 i < training data size i++ instance inst = training data get i having trained using just current features how we classify training data now classification classification = maxent classify inst !classification best label correct instance list il = instance list inst get data instance sub instance = il get inst get labeling get best label get entry value instances add sub instance label vectors add classification get label vector label vectors add create label vector sub instance classification logger info instance list size = +error instances size s = label vectors size label vector lvs = label vector s i = 0 i < s i++ lvs i = label vector label vectors get i ranked feature vector factory gain factory = gain name equals e x p g a i n gain factory = exp gain factory lvs gaussian prior variance gain name equals g r a d i e n t g a i n gain factory = gradient gain factory lvs gain name equals i n f o r m a t i o n g a i n gain factory = info gain factory illegal argument unsupported gain name +gain name feature inducer klfi = feature inducer gain factory instances num features per feature induction 2 num features per feature induction 2 num features per feature induction note that adds features globally but not on a per transition basis klfi induce features training data testing data != klfi induce features testing data logger info max ent feature selection now includes +global f s cardinality + features klfi = = 1+input alphabet size output alphabet size x x x executing block often causes an during training i t know why save during f i keep current parameter values x x x relies on detail that most recent features added to an alphabet get highest indices count per output label old count = maxent length output alphabet size count = 1+input alphabet size copy params into proper locations i=0 i<output alphabet size i++ arraycopy maxent i old count i count old count i=0 i<old count i++ maxent i != i out maxent i + +new i exit 0 maxent = maxent feature index = input alphabet size finished feature induction logger info ended +global f s cardinality + features set num iterations total iterations training iteration train training data validation data testing data evaluator maxent to rank max ent trainer + +maximizer get name + + num iterations= + num iterations + gaussian prior variance= +gaussian prior variance a inner that wraps up a rank max ent classifier and its training data result a maximize maximizable function maximizable trainer optimizable gradient value constraints cached gradient rank max ent classifier instance list training list expectations are temporarily stored in cached gradient cached value cached value stale cached gradient stale num labels num features feature index just clarity feature selection feature selection feature selection per label feature selection maximizable trainer maximizable trainer instance list ilist rank max ent initial classifier training list = ilist alphabet fd = ilist get data alphabet label alphabet ld = label alphabet ilist get target alphabet t fd stop growth because someone might want to feature induction ld stop growth add one feature feature assume underlying instances are binary num labels = underlying label alphabet size xxx num labels = 2 num features = fd size + 1 feature index = num features 1 = num labels num features constraints = num labels num features cached gradient = num labels num features arrays fill 0 0 arrays fill constraints 0 0 arrays fill cached gradient 0 0 feature selection = ilist get feature selection per label feature selection = ilist get per label feature selection add feature index to selection feature selection != feature selection add feature index per label feature selection != i = 0 i < per label feature selection length i++ per label feature selection i add feature index xxx later change to allow both to be set but select which one to use a flag? feature selection == || per label feature selection == initial classifier != classifier = initial classifier = classifier feature selection = classifier feature selection per label feature selection = classifier per feature selection feature index = classifier feature index initial classifier get instance pipe == ilist get pipe classifier == classifier = rank max ent ilist get pipe feature selection per label feature selection cached value stale = cached gradient stale = initialize constraints using only constraints from positive instance iterator< instance> iter = training list iterator logger fine number instances in training list = + training list size iter has next instance instance = iter next instance weight = training list get instance weight instance feature vector sequence fvs = feature vector sequence instance get data label best instance in sub list target = instance get target label label = target labels label = labels target get 0 label = label target positive index = value label get best label get entry to value positive index == 1 invalid instance logger warning label 1 skipping feature vector fv = feature vector fvs get positive index alphabet fdict = fv get alphabet fv get alphabet == fd xxx ensure dimensionality constraints correct matrix ops row plus equals constraints num features 0 fv instance weight feature whose weight 1 0 ! na n instance weight instance weight na n ! na n best index na n has na n = i = 0 i < fv num locations i++ na n fv value at location i logger info na n feature + fdict lookup fv index at location i to has na n = has na n logger info na n in instance + instance get name constraints positive instances xxx constraints 0 num features + feature index += 1 0 instance weight test maximizable test value and gradient current rank max ent get classifier classifier get parameter index index set parameter index v cached value stale = cached gradient stale = index = v get num length get buff buff == || buff length != length buff = length arraycopy 0 buff 0 length set buff buff != cached value stale = cached gradient stale = buff length != length = buff length arraycopy buff 0 0 buff length log probability training labels which here means probability positive example being labeled such get value cached value stale cached value = 0 we ll store expectation values in cached gradient now cached gradient stale = matrix ops set all cached gradient 0 0 incorporate likelihood data value = 0 0 iterator< instance> iter = training list iterator ii=0 iter has next ii++ instance instance = iter next feature vector sequence fvs = feature vector sequence instance get data scores stores pr sub list i being positive instance scores = fvs size instance weight = training list get instance weight instance labeling a representation an indicating which feature vector from sub list positive example proceed usual not penalize scores duplicate entries improved accuracy in some expts target = instance get target = 1 target label = value label target to value == 1 hack to avoid invalid instances >=0 < fvs size classifier get classification scores instance scores target labels labels labels = labels target best positions = labels size pi = 0 pi < labels size pi++ best positions pi = value labels get pi to = best positions 0 classifier get classification scores ties instance scores best positions value = instance weight math log scores na n value logger fine max ent trainer instance + instance get name + has na n value log scores = + math log scores + scores = + scores + has instance weight = + instance weight infinite value logger warning instance +instance get source + has infinite value skipping value and gradient cached value = value cached value stale = value cached value += value positive score = scores si=0 si < fvs size si++ scores si ==0 ! infinite scores si feature vector cfv = feature vector fvs get si matrix ops row plus equals cached gradient num features 0 cfv instance weight scores si cached gradient num features 0 + feature index += instance weight scores si incorporate prior on = 0 < num labels li++ fi = 0 fi < num features fi++ = num features + fi cached value += 2 gaussian prior variance cached value = 1 0 m a x i m i z e n o t m i n i m i z e cached value stale = progress logger info value loglikelihood = +cached value cached value get value gradient buffer gradient constraint expectation gaussian prior variance cached gradient stale cached value stale will fill in cached gradient expectation get value matrix ops plus equals cached gradient constraints incorporate prior on matrix ops plus equals cached gradient 1 0 gaussian prior variance a parameter may be set to infinity an external user we set gradient to 0 because parameter s value can never change anyway and it will mess up future calculations on matrix such norm matrix ops substitute cached gradient n e g a t i v e i n f i n i t y 0 0 set to zero all gradient dimensions that are not among selected features per label feature selection == label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 feature selection label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 per label feature selection label index cached gradient stale = buffer != buffer length == length arraycopy cached gradient 0 buffer 0 cached gradient length s e r i a l i z a t i o n serial u = 1 u r r e n t s e r i a l v e r s i o n = 1 write output stream out i o out write out write u r r e n t s e r i a l v e r s i o n read input stream in i o not found in read = in read 