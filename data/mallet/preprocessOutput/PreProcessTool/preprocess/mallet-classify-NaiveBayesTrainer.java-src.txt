2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e classify io serializable io output stream io i o io input stream iterator classify classifier pipe noop pipe pipe types alphabet types alphabet carrying types feature selection types feature vector types instance types instance list types label vector types labeling types multinomial used to generate a naive bayes classifier from a set training data in an bayes classifier p classification| data = p data| classification p classification p data <p> to compute likelihood <br> p data| classification = p d1 d2 dn | classification <br> naive bayes makes assumption that all data are conditionally independent given classification <br> p d1 d2 dn | classification = p d1| classification p d2| classification <p> other classifiers in naive bayes two classes a trainer and a classifier naive bayes trainer produces estimates various p dn| classifier and contructs those estimates <p> a call to train or incremental train produces a link classify naive bayes classifier that can can be used to classify instances a call to incremental train does not away internal state trainer subsequent calls to incremental train train extending previous training set <p> a naive bayes trainer can be persisted using serialization naive bayes author andrew mc callum <a href= mailto >mccallum edu< a> naive bayes trainer classifier trainer< naive bayes> classifier trainer instance increments< naive bayes> boostable alphabet carrying serializable these function selections kind estimator used multinomial estimator feature estimator = multinomial laplace estimator multinomial estimator prior estimator = multinomial laplace estimator added to support incremental training these are counts formed after naive bayes training note that these are not estimates passed to naive bayes classifier rather estimates are formed from these counts we could these five fields out into a inner multinomial estimator me multinomial estimator pe doc length normalization = 1 a value 1 means t any document length normalization naive bayes classifier style incremental training successful following members should probably be moved up into incremental classifier trainer pipe instance pipe needed to construct a classifier alphabet data alphabet extracted from instance list must be same all calls to incremental train alphabet target alphabet extracted from instance list must be same all calls to incremental train naive bayes trainer naive bayes initial classifier initial classifier != instance pipe = initial classifier get instance pipe data alphabet = initial classifier get alphabet target alphabet = initial classifier get label alphabet classifier = initial classifier naive bayes trainer pipe instance pipe instance pipe = instance pipe data alphabet = instance pipe get data alphabet target alphabet = instance pipe get target alphabet naive bayes trainer naive bayes get classifier classifier naive bayes trainer set doc length normalization d doc length normalization = d get doc length normalization doc length normalization get multinomial estimator instance used to specify type estimator features estimator to be cloned on next call to train or first call to incremental train multinomial estimator get feature multinomial estimator feature estimator set multinomial estimator used features mulitnomial estimator internally cloned and clone used to maintain counts that will be used to generate probability estimates next time train or an initial incremental train run defaults to a multinomial laplace estimator me to be cloned on next call to train or first call to incremental train naive bayes trainer set feature multinomial estimator multinomial estimator me instance pipe != illegal state can t set after incremental train called feature estimator = me get multinomial estimator instance used to specify type estimator priors estimator to be cloned on next call to train or first call to incremental train multinomial estimator get prior multinomial estimator prior estimator set multinomial estimator used priors mulitnomial estimator internally cloned and clone used to maintain counts that will be used to generate probability estimates next time train or an initial incremental train run defaults to a multinomial laplace estimator me to be cloned on next call to train or first call to incremental train naive bayes trainer set prior multinomial estimator multinomial estimator me instance pipe != illegal state can t set after incremental train called prior estimator = me create a naive bayes classifier from a set training data trainer uses counts each feature in an instance s feature vector to provide an estimate p labeling| feature internal state trainer thrown away a call to reset when train each call to train completely independent any other training list instance list to be used to train classifier within each instance data slot an instance feature vector and target slot an instance labeling validation list currently unused test set currently unused evaluator currently unused initial classifier currently unused naive bayes classifier trained on training list naive bayes train instance list training list forget all previous sufficient statistics counts me = pe = train a classifier based on data classifier = train incremental training list classifier naive bayes train incremental instance list training instances to add initialize and check instance variables necessary setup training instances to add incrementally add counts training data instance instance training instances to add incorporate one instance instance training instances to add get instance weight instance estimate multinomials and a naive bayes classifier note that unlike max ent naive bayes immutable so we create a one each time classifier = naive bayes instance pipe pe estimate estimate feature multinomials classifier naive bayes train incremental instance instance setup instance incrementally add counts training instance incorporate one instance instance 1 0 instance pipe == instance pipe = noop data alphabet target alphabet classifier = naive bayes instance pipe pe estimate estimate feature multinomials classifier setup instance list instances instance instance instances != || instance != instance == instances != instance = instances get 0 initialize alphabets data alphabet == data alphabet = instance get data alphabet target alphabet = instance get target alphabet ! alphabet alphabets match instance make sure alphabets match illegal argument training set alphabets not match those naive bayes trainer initialize or check instance pipe instances != instance pipe == instance pipe = instances get pipe instance pipe != instances get pipe make sure that pipes match really necessary?? i t think so but it could be confusing to have each classifier have a different pipe? akm 1 08 illegal argument training set pipe does not match that naive bayes trainer me == num labels = target alphabet size me = multinomial estimator num labels i = 0 i < num labels i++ me i = multinomial estimator feature estimator clone me i set alphabet data alphabet pe = multinomial estimator prior estimator clone target alphabet size > me length target alphabet grew increase size our multinomial target alphabet size = target alphabet size copy over old values multinomial estimator me = multinomial estimator target alphabet size arraycopy me 0 me 0 me length initialize expanded space i= me length i<target alphabet size i++ multinomial estimator mest = multinomial estimator feature estimator clone mest set alphabet data alphabet me i = mest me = me incorporate one instance instance instance instance weight labeling labeling = instance get labeling labeling == handle unlabeled instances skipping them feature vector fv = feature vector instance get data one norm = fv one norm one norm <= 0 skip instances that have no features present doc length normalization > 0 make document have counts that sum to doc length normalization i e 20 it would be document had 20 words instance weight = doc length normalization one norm instance weight > 0 ! infinite instance weight lpos = 0 lpos < labeling num locations lpos++ = labeling index at location lpos label weight = labeling value at location lpos label weight == 0 out naive bayes trainer me increment + label weight instance weight me increment fv label weight instance weight relies on label weight summing to 1 over all labels pe increment label weight instance weight multinomial estimate feature multinomials num labels = target alphabet size multinomial m = multinomial num labels = 0 < num labels li++ me print debugging m = me estimate m create a naive bayes classifier from a set training data and previous state trainer subsequent calls to incremental train add to state trainer an incremental training session should consist only calls to incremental train and have no calls to train training list instance list to be used to train classifier within each instance data slot an instance feature vector and target slot an instance labeling validation list currently unused test set currently unused evaluator currently unused initial classifier currently unused naive bayes classifier trained on training list and previous training lists passed to incremental train to naive bayes trainer alphabet carrying alphabets match alphabet carrying alphabet alphabets match alphabet get alphabet data alphabet alphabet get alphabets alphabet data alphabet target alphabet serialization serial u overriden to prevent innocuous changes in from making serialization mechanism think external format has changed serial u = 1 u r r e n t s e r i a l v e r s i o n = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n selections kind estimator used out write feature estimator out write prior estimator these are counts formed after naive bayes training out write me out write pe pipe and alphabets out write instance pipe out write data alphabet out write target alphabet read input stream in i o not found = in read != u r r e n t s e r i a l v e r s i o n not found mismatched naive bayes trainer versions wanted + u r r e n t s e r i a l v e r s i o n + got + selections kind estimator used feature estimator = multinomial estimator in read prior estimator = multinomial estimator in read these are counts formed after naive bayes training me = multinomial estimator in read pe = multinomial estimator in read pipe and alphabets instance pipe = pipe in read data alphabet = alphabet in read target alphabet = alphabet in read factory classifier trainer factory< naive bayes trainer> multinomial estimator feature estimator = multinomial laplace estimator multinomial estimator prior estimator = multinomial laplace estimator doc length normalization = 1 naive bayes trainer classifier trainer classifier initial classifier naive bayes trainer naive bayes initial classifier naive bayes trainer factory set doc length normalization doc length normalization doc length normalization = doc length normalization naive bayes trainer factory set feature multinomial estimator multinomial estimator feature estimator feature estimator = feature estimator naive bayes trainer factory set prior multinomial estimator multinomial estimator prior estimator prior estimator = prior estimator 