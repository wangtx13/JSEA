2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics arrays list list tree set iterator formatter locale concurrent logging zip io text number format types topics topic assignment randoms logger simple parallel threaded l d a following newman asuncion smyth and welling distributed algorithms topic models j m l r 2009 sparse l d a sampling scheme and data structure from yao mimno and mc callum efficient topic model inference on streaming document collections k d d 2009 author david mimno andrew mc callum parallel topic model serializable u n a s s i g n e d t o p i = 1 logger logger = logger get logger parallel topic model get name list< topic assignment> data training instances and their topic assignments alphabet alphabet alphabet input data label alphabet topic alphabet alphabet topics num topics number topics to be fit these values are used to encode type topic counts count topic pairs in a single topic mask topic bits num types total tokens alpha dirichlet alpha alpha over topics alpha sum beta prior on per topic multinomial over words beta sum using symmetric alpha = d e f a u l t b e t a = 0 01 type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> dirichlet estimation doc length counts histogram document sizes topic doc counts histogram document topic counts indexed <topic index sequence position index> num iterations = 1000 burnin period = 200 save sample interval = 10 optimize interval = 50 tempering interval = 0 show topics interval = 50 words per topic = 7 save state interval = 0 state filename = save model interval = 0 model filename = random seed = 1 number format formatter print log likelihood = number times each type appears in corpus type totals max over type totals used beta optimization max type count num threads = 1 parallel topic model number topics number topics number topics d e f a u l t b e t a parallel topic model number topics alpha sum beta label alphabet number topics alpha sum beta label alphabet label alphabet num topics label alphabet ret = label alphabet i = 0 i < num topics i++ ret lookup index topic +i ret parallel topic model label alphabet topic alphabet alpha sum beta data = list< topic assignment> topic alphabet = topic alphabet alpha sum = alpha sum beta = beta set num topics topic alphabet size formatter = number format get instance formatter set maximum fraction digits 5 logger info l d a + num topics + topics + topic bits + topic bits + to binary topic mask + topic mask alphabet get alphabet alphabet label alphabet get topic alphabet topic alphabet get num topics num topics set or reset number topics will not change any token topic assignments so it should only be used before initializing or restoring a previously saved state set num topics num topics num topics = num topics bit count num topics == 1 exact power 2 topic mask = num topics 1 topic bits = bit count topic mask otherwise add an extra bit topic mask = highest one bit num topics 2 1 topic bits = bit count topic mask alpha = num topics arrays fill alpha alpha sum num topics tokens per topic = num topics list< topic assignment> get data data get type topic counts type topic counts get tokens per topic tokens per topic set num iterations num iterations num iterations = num iterations set burnin period burnin period burnin period = burnin period set topic display interval n show topics interval = interval words per topic = n set random seed seed random seed = seed interval optimizing dirichlet hyperparameters set optimize interval interval optimize interval = interval make sure we always have at least one sample before optimizing hyperparameters save sample interval > optimize interval save sample interval = optimize interval set symmetric alpha b using symmetric alpha = b set tempering interval interval tempering interval = interval set num threads threads num threads = threads define how often and where to save a text representation current state files are g zipped interval save a copy state every <code>interval< code> iterations filename save state to iteration number a suffix set save state interval filename save state interval = interval state filename = filename define how often and where to save a serialized model interval save a serialized model every <code>interval< code> iterations filename save to iteration number a suffix set save serialized model interval filename save model interval = interval model filename = filename add instances instance list training alphabet = training get data alphabet num types = alphabet size beta sum = beta num types randoms random = random seed == 1 random = randoms random = randoms random seed instance instance training feature sequence tokens = feature sequence instance get data label sequence topic sequence = label sequence topic alphabet tokens size topics = topic sequence get features position = 0 position < topics length position++ topic = random next num topics topics position = topic topic assignment t = topic assignment instance topic sequence data add t build initial type topic counts initialize histograms initialize from state state i o line fields buffered reader reader = buffered reader input stream reader g z i p input stream input stream state line = reader read line skip some lines starting # that describe format and specify hyperparameters line starts # line starts #alpha line = line replace #alpha fields = line split set num topics fields length alpha sum = 0 0 topic = 0 topic < fields length topic++ alpha topic = parse fields topic alpha sum += alpha topic line starts #beta line = line replace #beta beta = parse line beta sum = beta num types line = reader read line fields = line split topic assignment document data feature sequence tokens = feature sequence document instance get data feature sequence topic sequence = feature sequence document topic sequence topics = topic sequence get features position = 0 position < tokens size position++ type = tokens get index at position position type == parse fields 3 topics position = parse fields 5 err instance list and state not match + line illegal state line = reader read line line != fields = line split build initial type topic counts initialize histograms build initial type topic counts type topic counts = num types tokens per topic = num topics get total number occurrences each word type type totals = num types type totals = num types create type topic counts data structure topic assignment document data feature sequence tokens = feature sequence document instance get data position = 0 position < tokens get length position++ type = tokens get index at position position type totals type ++ max type count = 0 allocate enough space so that we never have to worry about overflows either number topics or number times type occurs type = 0 type < num types type++ type totals type > max type count max type count = type totals type type topic counts type = math min num topics type totals type topic assignment document data feature sequence tokens = feature sequence document instance get data feature sequence topic sequence = feature sequence document topic sequence topics = topic sequence get features position = 0 position < tokens size position++ topic = topics position topic == u n a s s i g n e d t o p i tokens per topic topic ++ format these arrays topic in rightmost bits count in remaining left bits since count in high bits sorting desc numeric value guarantees that higher counts will be before lower counts type = tokens get index at position position current type topic counts = type topic counts type start assuming that either empty or in sorted descending order here we are only adding counts so we find an existing location topic we only need to ensure that it not larger than its left neighbor index = 0 current topic = current type topic counts index topic mask current value current type topic counts index > 0 current topic != topic index++ index == current type topic counts length logger info overflow on type + type current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits current value == 0 value 1 so we t have to worry about sorting except topic suffix which doesn t matter current type topic counts index = 1 << topic bits + topic current type topic counts index = current value + 1 << topic bits + topic now ensure that still sorted bubbling value up index > 0 current type topic counts index > current type topic counts index 1 temp = current type topic counts index current type topic counts index = current type topic counts index 1 current type topic counts index 1 = temp index sum type topic counts worker runnable runnables clear topic totals arrays fill tokens per topic 0 clear type topic counts only looking at entries before first 0 entry type = 0 type < num types type++ target counts = type topic counts type position = 0 position < target counts length target counts position > 0 target counts position = 0 position++ thread = 0 thread < num threads thread++ handle total tokens per topic source totals = runnables thread get tokens per topic topic = 0 topic < num topics topic++ tokens per topic topic += source totals topic now handle individual type topic counts source type topic counts = runnables thread get type topic counts type = 0 type < num types type++ here source individual thread counts and target global counts source counts = source type topic counts type target counts = type topic counts type source index = 0 source index < source counts length source counts source index > 0 topic = source counts source index topic mask count = source counts source index >> topic bits target index = 0 current topic = target counts target index topic mask current count target counts target index > 0 current topic != topic target index++ target index == target counts length logger info overflow in merging on type + type current topic = target counts target index topic mask current count = target counts target index >> topic bits target counts target index = current count + count << topic bits + topic now ensure that still sorted bubbling value up target index > 0 target counts target index > target counts target index 1 temp = target counts target index target counts target index = target counts target index 1 target counts target index 1 = temp target index source index++ debuggging to ensure counts are being reconstructed correctly type = 0 type < num types type++ target counts = type topic counts type index = 0 count = 0 index < target counts length target counts index > 0 count += target counts index >> topic bits index++ count != type totals type err expected + type totals type + found + count gather statistics on size documents and create histograms use in dirichlet hyperparameter optimization initialize histograms max tokens = 0 total tokens = 0 seq len doc = 0 doc < data size doc++ feature sequence fs = feature sequence data get doc instance get data seq len = fs get length seq len > max tokens max tokens = seq len total tokens += seq len logger info max tokens + max tokens logger info total tokens + total tokens doc length counts = max tokens + 1 topic doc counts = num topics max tokens + 1 optimize alpha worker runnable runnables first clear sufficient statistic histograms arrays fill doc length counts 0 topic = 0 topic < topic doc counts length topic++ arrays fill topic doc counts topic 0 thread = 0 thread < num threads thread++ source length counts = runnables thread get doc length counts source topic counts = runnables thread get topic doc counts count=0 count < source length counts length count++ source length counts count > 0 doc length counts count += source length counts count source length counts count = 0 topic=0 topic < num topics topic++ ! using symmetric alpha count=0 count < source topic counts topic length count++ source topic counts topic count > 0 topic doc counts topic count += source topic counts topic count source topic counts topic count = 0 symmetric we only need one count which i m putting in same data structure but topic 0 all other topic histograms will be empty i m duplicating loop which isn t best thing but it means only checking whether we are symmetric or not num topics times instead num topics longest document length count=0 count < source topic counts topic length count++ source topic counts topic count > 0 topic doc counts 0 count += source topic counts topic count ^ only change source topic counts topic count = 0 using symmetric alpha alpha sum = dirichlet learn symmetric concentration topic doc counts 0 doc length counts num topics alpha sum topic = 0 topic < num topics topic++ alpha topic = alpha sum num topics alpha sum = dirichlet learn alpha topic doc counts doc length counts 1 001 1 0 1 runtime e dirichlet optimization has become unstable known to happen very small corpora ~5 docs logger warning dirichlet optimization has become unstable resetting to alpha t = 1 0 alpha sum = num topics topic = 0 topic < num topics topic++ alpha topic = 1 0 temper alpha worker runnable runnables first clear sufficient statistic histograms arrays fill doc length counts 0 topic = 0 topic < topic doc counts length topic++ arrays fill topic doc counts topic 0 thread = 0 thread < num threads thread++ source length counts = runnables thread get doc length counts source topic counts = runnables thread get topic doc counts count=0 count < source length counts length count++ source length counts count > 0 source length counts count = 0 topic=0 topic < num topics topic++ count=0 count < source topic counts topic length count++ source topic counts topic count > 0 source topic counts topic count = 0 topic = 0 topic < num topics topic++ alpha topic = 1 0 alpha sum = num topics optimize beta worker runnable runnables histogram starts at count 0 so all tokens most frequent type were assigned to one topic we would need to store a max type count + 1 count count histogram = max type count + 1 now count number type topic pairs that have each number tokens index type = 0 type < num types type++ counts = type topic counts type index = 0 index < counts length counts index > 0 count = counts index >> topic bits count histogram count ++ index++ figure out how large we need to make observation lengths histogram max topic size = 0 topic = 0 topic < num topics topic++ tokens per topic topic > max topic size max topic size = tokens per topic topic now allocate it and populate it topic size histogram = max topic size + 1 topic = 0 topic < num topics topic++ topic size histogram tokens per topic topic ++ beta sum = dirichlet learn symmetric concentration count histogram topic size histogram num types beta sum beta = beta sum num types logger info beta + formatter format beta + now publish value thread = 0 thread < num threads thread++ runnables thread reset beta beta beta sum estimate i o start time = current time millis worker runnable runnables = worker runnable num threads docs per thread = data size num threads offset = 0 num threads > 1 thread = 0 thread < num threads thread++ runnable totals = num topics arraycopy tokens per topic 0 runnable totals 0 num topics runnable counts = num types type = 0 type < num types type++ counts = type topic counts type length arraycopy type topic counts type 0 counts 0 counts length runnable counts type = counts some docs may be missing at end due to division thread == num threads 1 docs per thread = data size offset randoms random = random seed == 1 random = randoms random = randoms random seed runnables thread = worker runnable num topics alpha alpha sum beta random data runnable counts runnable totals offset docs per thread runnables thread initialize alpha statistics doc length counts length offset += docs per thread there only one thread copy type topic counts arrays directly rather than allocating memory randoms random = random seed == 1 random = randoms random = randoms random seed runnables 0 = worker runnable num topics alpha alpha sum beta random data type topic counts tokens per topic offset docs per thread runnables 0 initialize alpha statistics doc length counts length there only one thread we can avoid communications overhead informs thread not to gather statistics its portion data runnables 0 make only thread executor service executor = executors fixed thread pool num threads iteration = 1 iteration <= num iterations iteration++ iteration start = current time millis show topics interval != 0 iteration != 0 iteration % show topics interval == 0 logger info + display top words words per topic save state interval != 0 iteration % save state interval == 0 print state state filename + + iteration save model interval != 0 iteration % save model interval == 0 write model filename + + iteration num threads > 1 submit runnables to thread pool thread = 0 thread < num threads thread++ iteration > burnin period optimize interval != 0 iteration % save sample interval == 0 runnables thread collect alpha statistics logger fine submitting thread + thread executor submit runnables thread runnables thread run i m getting some problems that look like a thread hasn t started yet when it first polled so it appears to be finished only occurs in very corpora thread sleep 20 interrupted e finished = ! finished thread sleep 10 interrupted e finished = are all threads done? thread = 0 thread < num threads thread++ logger info thread + thread + done? + runnables thread finished finished = finished runnables thread finished out print + current time millis iteration start + sum type topic counts runnables out print + current time millis iteration start + thread = 0 thread < num threads thread++ runnable totals = runnables thread get tokens per topic arraycopy tokens per topic 0 runnable totals 0 num topics runnable counts = runnables thread get type topic counts type = 0 type < num types type++ target counts = runnable counts type source counts = type topic counts type index = 0 index < source counts length source counts index != 0 target counts index = source counts index target counts index != 0 target counts index = 0 index++ arraycopy type topic counts type 0 counts 0 counts length iteration > burnin period optimize interval != 0 iteration % save sample interval == 0 runnables 0 collect alpha statistics runnables 0 run elapsed millis = current time millis iteration start elapsed millis < 1000 logger fine elapsed millis + ms logger fine elapsed millis 1000 + s iteration > burnin period optimize interval != 0 iteration % optimize interval == 0 optimize alpha runnables optimize beta runnables logger fine o + current time millis iteration start + iteration % 10 == 0 print log likelihood logger info < + iteration + > l l token + formatter format model log likelihood total tokens logger info < + iteration + > executor shutdown now seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 builder time report = builder time report append total time days != 0 time report append days time report append days hours != 0 time report append hours time report append hours minutes != 0 time report append minutes time report append minutes time report append seconds time report append seconds logger info time report to iterated conditional modes which equivalent to gibbs sampling but replacing sampling from conditional taking maximum topic it tends to converge within a small number iterations models that have reached a good state through gibbs sampling maximize iterations iteration = 0 total change = m a x v a l u e topic coefficients = num topics current topic current value iteration < iterations total change > 0 iteration start = current time millis total change = 0 loop over every document in corpus doc = 0 doc < data size doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence one doc topics = topic sequence get features current type topic counts type old topic topic doc length = token sequence get length local topic counts = num topics populate topic counts position = 0 position < doc length position++ local topic counts one doc topics position ++ global max topic = 0 global max score = 0 0 topic = 0 topic < num topics topic++ topic coefficients topic = alpha topic + local topic counts topic beta sum + tokens per topic topic beta topic coefficients topic > global max score global max topic = topic global max score = beta topic coefficients topic score max score topic term scores = num topics iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = one doc topics position grab relevant row from our two dimensional current type topic counts = type topic counts type remove token from all counts local topic counts old topic tokens per topic old topic recalculate word invariant topic coefficients old topic = alpha old topic + local topic counts old topic beta sum + tokens per topic old topic topic we just decremented was previous max topic search a max topic old topic == global max topic global max score = beta topic coefficients old topic topic = 0 topic < num topics topic++ beta topic coefficients topic > global max score global max topic = topic global max score = beta topic coefficients topic topic = global max topic max score = global max score tokens per topic old topic >= 0 old topic + old topic + below 0 index = 0 already decremented = index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits ! already decremented current topic == old topic we re decrementing and adding up sampling weights at same time but decrementing may require us to reorder topics so after we re done here look at cell in again current value current value == 0 current type topic counts index = 0 current type topic counts index = current value << topic bits + old topic shift reduced value to right necessary sub index = index sub index < current type topic counts length 1 current type topic counts sub index < current type topic counts sub index + 1 temp = current type topic counts sub index current type topic counts sub index = current type topic counts sub index + 1 current type topic counts sub index + 1 = temp sub index++ already decremented = score = topic coefficients current topic beta + current value score > max score topic = current topic max score = score index++ put that topic into counts one doc topics position = topic local topic counts topic ++ tokens per topic topic ++ index = 0 found topic = ! found topic index < current type topic counts length current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits current topic == topic current type topic counts index = current value + 1 << topic bits + topic index > 0 current type topic counts index > current type topic counts index 1 temp = current type topic counts index current type topic counts index = current type topic counts index 1 current type topic counts index 1 = temp found topic = current value == 0 current type topic counts index = 1 << topic bits + topic found topic = index++ topic coefficients topic = alpha topic + local topic counts topic beta sum + tokens per topic topic beta topic coefficients topic > global max score global max score = beta topic coefficients topic global max topic = topic topic != old topic total change++ elapsed millis = current time millis iteration start logger info iteration + + elapsed millis + ms + total change + + model log likelihood total tokens iteration++ an sorted sets one set per topic each set contains sorter keys into alphabet to get direct access to strings use get top words list< tree set< sorter>> get sorted words list< tree set< sorter>> topic sorted words = list< tree set< sorter>> num topics initialize tree sets topic = 0 topic < num topics topic++ topic sorted words add tree set< sorter> collect counts type = 0 type < num types type++ topic counts = type topic counts type index = 0 index < topic counts length topic counts index > 0 topic = topic counts index topic mask count = topic counts index >> topic bits topic sorted words get topic add sorter type count index++ topic sorted words an one element each topic arrays words which are most probable words that topic in descending order these are but will probably be strings num words maximum length each topic s words may be less get top words num words list< tree set< sorter>> topic sorted words = get sorted words result = num topics topic = 0 topic < num topics topic++ tree set< sorter> sorted words = topic sorted words get topic how many words should we report? some topics may have fewer than number words non zero weight limit = num words sorted words size < num words limit = sorted words size result topic = limit iterator< sorter> iterator = sorted words iterator i=0 i < limit i++ sorter info = iterator next result topic i = alphabet lookup info get result print top words num words use lines i o print stream out = print stream print top words out num words use lines out close print top words print stream out num words using lines out print display top words num words using lines display top words num words using lines builder out = builder list< tree set< sorter>> topic sorted words = get sorted words print results each topic topic = 0 topic < num topics topic++ tree set< sorter> sorted words = topic sorted words get topic word = 0 iterator< sorter> iterator = sorted words iterator using lines out append topic + + formatter format alpha topic + iterator has next word < num words sorter info = iterator next out append alphabet lookup info get + + formatter format info get weight + word++ out append topic + + formatter format alpha topic + iterator has next word < num words sorter info = iterator next out append alphabet lookup info get + word++ out append out to topic xml report print writer out num words list< tree set< sorter>> topic sorted words = get sorted words out <?xml version= 1 0 ?> out <topic model> topic = 0 topic < num topics topic++ out <topic id= + topic + alpha= + alpha topic + total tokens= + tokens per topic topic + > rank = 1 iterator< sorter> iterator = topic sorted words get topic iterator iterator has next rank <= num words sorter info = iterator next out <word rank= + rank + count= + info get weight + > + alphabet lookup info get + < word> rank++ out < topic> out < topic model> topic phrase xml report print writer out num words num topics = get num topics gnu trove t hash map< string> phrases = gnu trove t hash map num topics alphabet alphabet = get alphabet get counts phrases ti = 0 ti < num topics ti++ phrases ti = gnu trove t hash map< string> di = 0 di < get data size di++ topic assignment t = get data get di instance instance = t instance feature sequence fvs = feature sequence instance get data bigrams = fvs feature sequence bigrams bigrams = prevtopic = 1 prevfeature = 1 topic = 1 buffer sb = feature = 1 doclen = fvs size pi = 0 pi < doclen pi++ feature = fvs get index at position pi topic = get data get di topic sequence get index at position pi topic == prevtopic !with bigrams || feature sequence bigrams fvs get bi index at position pi != 1 sb == sb = buffer alphabet lookup prevfeature to + + alphabet lookup feature sb append sb append alphabet lookup feature sb != sbs = sb to logger info phrase +sbs phrases prevtopic get sbs == 0 phrases prevtopic put sbs 0 phrases prevtopic increment sbs prevtopic = prevfeature = 1 sb = prevtopic = topic prevfeature = feature phrases now filled counts now start printing xml out <?xml version= 1 0 ?> out <topics> list< tree set< sorter>> topic sorted words = get sorted words probs = alphabet size ti = 0 ti < num topics ti++ out print <topic id=\ + ti + \ alpha=\ + alpha ti + \ total tokens=\ + tokens per topic ti + \ gathering <term> and <phrase> output temporarily so that we can get topic title before printing it to out output stream bout = output stream print stream pout = print stream bout holding candidate topic titles augmentable feature vector titles = augmentable feature vector alphabet print words word = 0 iterator< sorter> iterator = topic sorted words get ti iterator iterator has next word < num words sorter info = iterator next pout <word weight=\ + info get weight tokens per topic ti + \ count=\ + math round info get weight + \ > + alphabet lookup info get + < word> word++ word < 20 consider top 20 individual words candidate titles titles add alphabet lookup info get info get weight type = 0 type < alphabet size type++ probs type = get count feature topic type ti get count tokens per topic ti ranked feature vector rfv = ranked feature vector alphabet probs ri = 0 ri < num words ri++ fi = rfv get index at rank ri pout <term weight=\ +probs fi + \ count=\ +this get count feature topic fi ti + \ > +alphabet lookup fi + < term> ri < 20 consider top 20 individual words candidate titles titles add alphabet lookup fi get count feature topic fi ti print phrases keys = phrases ti keys values = phrases ti get values counts = keys length i = 0 i < counts length i++ counts i = values i countssum = matrix ops sum counts alphabet alph = alphabet keys ranked feature vector rfv = ranked feature vector alph counts max = rfv num locations < num words ? rfv num locations num words ri = 0 ri < max ri++ fi = rfv get index at rank ri pout <phrase weight=\ +counts fi countssum+ \ count=\ +values fi + \ > +alph lookup fi + < phrase> any phrase count less than 20 simply unreliable ri < 20 values fi > 20 titles add alph lookup fi 100 values fi prefer phrases a factor 100 select candidate titles buffer titles buffer = buffer rfv = ranked feature vector titles get alphabet titles num titles = 10 ri = 0 ri < num titles ri < rfv num locations ri++ t add redundant titles titles buffer index rfv get at rank ri to == 1 titles buffer append rfv get at rank ri ri < num titles 1 titles buffer append num titles++ out titles=\ + titles buffer to + \ > out print bout to out < topic> out < topics> write internal representation type topic counts count topic pairs in descending order count to a print type topic counts i o print writer out = print writer writer type = 0 type < num types type++ builder buffer = builder buffer append type + + alphabet lookup type topic counts = type topic counts type index = 0 index < topic counts length topic counts index > 0 topic = topic counts index topic mask count = topic counts index >> topic bits buffer append + topic + + count index++ out buffer out close print topic word weights i o print writer out = print writer writer print topic word weights out out close print an unnormalized weight every word in every topic most these will be equal to smoothing parameter beta print topic word weights print writer out i o probably not most efficient way to topic = 0 topic < num topics topic++ type = 0 type < num types type++ topic counts = type topic counts type weight = beta index = 0 index < topic counts length topic counts index > 0 current topic = topic counts index topic mask current topic == topic weight += topic counts index >> topic bits index++ out topic + + alphabet lookup type + + weight get smoothed over topics a training instance get topic probabilities instance label sequence topics = data get instance topic sequence get topic probabilities topics get smoothed over topics a topic sequence which may be from training set or from a instance topics assigned an inferencer get topic probabilities label sequence topics topic = num topics loop over tokens in document counting current topic assignments position = 0 position < topics get length position++ topic topics get index at position position ++ add smoothing and normalize sum = 0 0 topic = 0 topic < num topics topic++ topic topic += alpha topic sum += topic topic and normalize topic = 0 topic < num topics topic++ topic topic = sum topic print document topics i o print writer out = print writer writer print document topics out out close print dense document topics print writer out doc len topic counts = num topics doc = 0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence current doc topics = topic sequence get features builder builder = builder builder append doc builder append data get doc instance get name != builder append data get doc instance get name builder append no name doc len = current doc topics length count up tokens token=0 token < doc len token++ topic counts current doc topics token ++ and normalize topic = 0 topic < num topics topic++ builder append + alpha topic + topic counts topic doc len + alpha sum out builder arrays fill topic counts 0 print document topics print writer out print document topics out 0 0 1 out a print writer threshold only print topics proportion greater than number max print no more than many topics print document topics print writer out threshold max out print #doc name topic proportion doc len topic counts = num topics sorter sorted topics = sorter num topics topic = 0 topic < num topics topic++ initialize sorters dummy values sorted topics topic = sorter topic topic max < 0 || max > num topics max = num topics doc = 0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence current doc topics = topic sequence get features builder builder = builder builder append doc builder append data get doc instance get name != builder append data get doc instance get name builder append no name builder append doc len = current doc topics length count up tokens token=0 token < doc len token++ topic counts current doc topics token ++ and normalize topic = 0 topic < num topics topic++ sorted topics topic set topic alpha topic + topic counts topic doc len + alpha sum arrays sort sorted topics i = 0 i < max i++ sorted topics i get weight < threshold builder append sorted topics i get + + sorted topics i get weight + out builder arrays fill topic counts 0 get sub corpus topic words document mask normalized smoothed result = num topics num types sub corpus tokens per topic = num topics doc = 0 doc < data size doc++ document mask doc words = feature sequence data get doc instance get data get features topics = data get doc topic sequence get features position = 0 position < topics length position++ result topics position words position ++ sub corpus tokens per topic topics position ++ smoothed topic = 0 topic < num topics topic++ type = 0 type < num types type++ result topic type += beta normalized topic normalizers = num topics smoothed topic = 0 topic < num topics topic++ topic normalizers topic = 1 0 sub corpus tokens per topic topic + num types beta topic = 0 topic < num topics topic++ topic normalizers topic = 1 0 sub corpus tokens per topic topic topic = 0 topic < num topics topic++ type = 0 type < num types type++ result topic type = topic normalizers topic result get topic words normalized smoothed result = num topics num types type = 0 type < num types type++ topic counts = type topic counts type index = 0 index < topic counts length topic counts index > 0 topic = topic counts index topic mask count = topic counts index >> topic bits result topic type += count index++ smoothed topic = 0 topic < num topics topic++ type = 0 type < num types type++ result topic type += beta normalized topic normalizers = num topics smoothed topic = 0 topic < num topics topic++ topic normalizers topic = 1 0 tokens per topic topic + num types beta topic = 0 topic < num topics topic++ topic normalizers topic = 1 0 tokens per topic topic topic = 0 topic < num topics topic++ type = 0 type < num types type++ result topic type = topic normalizers topic result get document topics normalized smoothed result = data size num topics doc = 0 doc < data size doc++ topics = data get doc topic sequence get features position = 0 position < topics length position++ result doc topics position ++ smoothed topic = 0 topic < num topics topic++ result doc topic += alpha topic normalized sum = 0 0 topic = 0 topic < num topics topic++ sum += result doc topic normalizer = 1 0 sum topic = 0 topic < num topics topic++ result doc topic = normalizer result list< tree set< sorter>> get topic documents smoothing list< tree set< sorter>> topic sorted documents = list< tree set< sorter>> num topics initialize tree sets topic = 0 topic < num topics topic++ topic sorted documents add tree set< sorter> topic counts = num topics doc = 0 doc < data size doc++ topics = data get doc topic sequence get features position = 0 position < topics length position++ topic counts topics position ++ topic = 0 topic < num topics topic++ topic sorted documents get topic add sorter doc topic counts topic + smoothing topics length + num topics smoothing topic counts topic = 0 topic sorted documents print topic documents print writer out print topic documents out 100 out a print writer count print number top documents print topic documents print writer out max out #topic doc name proportion list< tree set< sorter>> topic sorted documents = get topic documents 10 0 topic = 0 topic < num topics topic++ tree set< sorter> sorted documents = topic sorted documents get topic i = 0 sorter sorter sorted documents i == max doc = sorter get proportion = sorter get weight name = data get doc instance get name name = data get doc instance get name to name == name = no name out format %d %d %s %f topic doc name proportion i++ print state f i o print stream out = print stream g z i p output stream buffered output stream output stream f print state out out close print state print stream out out #doc source pos typeindex type topic out print #alpha topic = 0 topic < num topics topic++ out print alpha topic + out out #beta + beta doc = 0 doc < data size doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence source = n a data get doc instance get source != source = data get doc instance get source to formatter output = formatter builder locale u s pi = 0 pi < topic sequence get length pi++ type = token sequence get index at position pi topic = topic sequence get index at position pi output format %d %s %d %d %s %d doc source pi type alphabet lookup type topic out print doc out print out print source out print out print pi out print out print type out print out print alphabet lookup type out print out print topic out out print output model log likelihood log likelihood = 0 0 non zero topics likelihood model a combination a dirichlet multinomial words in each topic and a dirichlet multinomial topics in each document likelihood function a dirichlet multinomial gamma sum i alpha i prod i gamma alpha i + n i prod i gamma alpha i gamma sum i alpha i + n i so log likelihood log gamma sum i alpha i log gamma sum i alpha i + n i + sum i log gamma alpha i + n i log gamma alpha i documents first topic counts = num topics topic log gammas = num topics doc topics topic=0 topic < num topics topic++ topic log gammas topic = dirichlet log gamma stirling alpha topic doc=0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence doc topics = topic sequence get features token=0 token < doc topics length token++ topic counts doc topics token ++ topic=0 topic < num topics topic++ topic counts topic > 0 log likelihood += dirichlet log gamma stirling alpha topic + topic counts topic topic log gammas topic subtract count + parameter sum term log likelihood = dirichlet log gamma stirling alpha sum + doc topics length arrays fill topic counts 0 add parameter sum term log likelihood += data size dirichlet log gamma stirling alpha sum and topics count number type topic pairs that are not just log gamma beta log gamma beta non zero type topics = 0 type=0 type < num types type++ reuse a pointer topic counts = type topic counts type index = 0 index < topic counts length topic counts index > 0 topic = topic counts index topic mask count = topic counts index >> topic bits non zero type topics++ log likelihood += dirichlet log gamma stirling beta + count na n log likelihood logger warning na n in log likelihood calculation 0 infinite log likelihood logger warning infinite log likelihood 0 index++ topic=0 topic < num topics topic++ log likelihood = dirichlet log gamma stirling beta num types + tokens per topic topic na n log likelihood logger info na n after topic + topic + + tokens per topic topic 0 infinite log likelihood logger info infinite value after topic + topic + + tokens per topic topic 0 log gamma | v| beta every topic log likelihood += dirichlet log gamma stirling beta num types num topics log gamma beta all type topic pairs non zero count log likelihood = dirichlet log gamma stirling beta non zero type topics na n log likelihood logger info at end infinite log likelihood logger info infinite value beta + beta + + num types 0 log likelihood a tool estimating topic distributions documents topic inferencer get inferencer topic inferencer type topic counts tokens per topic data get 0 instance get data alphabet alpha beta beta sum a tool evaluating marginal probability documents model marginal prob estimator get prob estimator marginal prob estimator num topics alpha alpha sum beta type topic counts tokens per topic serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write data out write alphabet out write topic alphabet out write num topics out write topic mask out write topic bits out write num types out write alpha out write alpha sum out write beta out write beta sum out write type topic counts out write tokens per topic out write doc length counts out write topic doc counts out write num iterations out write burnin period out write save sample interval out write optimize interval out write show topics interval out write words per topic out write save state interval out write state filename out write save model interval out write model filename out write random seed out write formatter out write print log likelihood out write num threads read input stream in i o not found = in read data = list< topic assignment> in read alphabet = alphabet in read topic alphabet = label alphabet in read num topics = in read topic mask = in read topic bits = in read num types = in read alpha = in read alpha sum = in read beta = in read beta sum = in read type topic counts = in read tokens per topic = in read doc length counts = in read topic doc counts = in read num iterations = in read burnin period = in read save sample interval = in read optimize interval = in read show topics interval = in read words per topic = in read save state interval = in read state filename = in read save model interval = in read model filename = in read random seed = in read formatter = number format in read print log likelihood = in read num threads = in read write serialized model output stream oos = output stream output stream serialized model oos write oos close i o e err problem serializing parallel topic model to + serialized model + + e parallel topic model read f parallel topic model topic model = input stream ois = input stream input stream f topic model = parallel topic model ois read ois close topic model initialize histograms topic model instance list training = instance list load 0 num topics = length > 1 ? parse 1 200 parallel topic model lda = parallel topic model num topics 50 0 0 01 lda print log likelihood = lda set topic display 50 7 lda add instances training lda set num threads parse 2 lda estimate logger info printing state lda print state state gz logger info finished printing e e print stack trace 