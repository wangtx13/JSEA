2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics logging zip io text number format topics types a simple latent dirichlet allocation using gibbs sampling slower than regular l d a but provides a better starting place understanding how sampling works and building topic models author david mimno andrew mc callum simple l d a serializable logger logger = logger get logger simple l d a get name training instances and their topic assignments list< topic assignment> data alphabet input data alphabet alphabet alphabet topics label alphabet topic alphabet number topics requested num topics size vocabulary num types prior alpha dirichlet alpha alpha over topics alpha sum beta prior on per topic multinomial over words beta sum d e f a u l t b e t a = 0 01 an to put topic counts current document initialized locally below defined here to avoid garbage collection overhead one doc topic counts indexed <document index topic index> statistics needed sampling type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> show topics interval = 50 words per topic = 10 randoms random number format formatter print log likelihood = simple l d a number topics number topics number topics d e f a u l t b e t a simple l d a number topics alpha sum beta number topics alpha sum beta randoms label alphabet label alphabet num topics label alphabet ret = label alphabet i = 0 i < num topics i++ ret lookup index topic +i ret simple l d a number topics alpha sum beta randoms random label alphabet number topics alpha sum beta random simple l d a label alphabet topic alphabet alpha sum beta randoms random data = list< topic assignment> topic alphabet = topic alphabet num topics = topic alphabet size alpha sum = alpha sum alpha = alpha sum num topics beta = beta random = random one doc topic counts = num topics tokens per topic = num topics formatter = number format get instance formatter set maximum fraction digits 5 logger info simple l d a + num topics + topics alphabet get alphabet alphabet label alphabet get topic alphabet topic alphabet get num topics num topics list< topic assignment> get data data set topic display interval n show topics interval = interval words per topic = n set random seed seed random = randoms seed get type topic counts type topic counts get topic totals tokens per topic add instances instance list training alphabet = training get data alphabet num types = alphabet size beta sum = beta num types type topic counts = num types num topics doc = 0 instance instance training doc++ feature sequence tokens = feature sequence instance get data label sequence topic sequence = label sequence topic alphabet tokens size topics = topic sequence get features position = 0 position < tokens size position++ topic = random next num topics topics position = topic tokens per topic topic ++ type = tokens get index at position position type topic counts type topic ++ topic assignment t = topic assignment instance topic sequence data add t sample iterations i o iteration = 1 iteration <= iterations iteration++ iteration start = current time millis loop over every document in corpus doc = 0 doc < data size doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence sample topics one doc token sequence topic sequence elapsed millis = current time millis iteration start logger fine iteration + + elapsed millis + ms occasionally print more show topics interval != 0 iteration % show topics interval == 0 logger info < + iteration + > log likelihood + model log likelihood + + top words words per topic sample topics one doc feature sequence token sequence feature sequence topic sequence one doc topics = topic sequence get features current type topic counts type old topic topic topic weights sum doc length = token sequence get length local topic counts = num topics populate topic counts position = 0 position < doc length position++ local topic counts one doc topics position ++ score sum topic term scores = num topics iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = one doc topics position grab relevant row from our two dimensional current type topic counts = type topic counts type remove token from all counts local topic counts old topic tokens per topic old topic tokens per topic old topic >= 0 old topic + old topic + below 0 current type topic counts old topic now calculate and add up scores each topic word sum = 0 0 here s where math happens! note that overall performance dominated what you in loop topic = 0 topic < num topics topic++ score = alpha + local topic counts topic beta + current type topic counts topic beta sum + tokens per topic topic sum += score topic term scores topic = score choose a random point between 0 and sum all topic scores sample = random next uniform sum figure out which topic contains that point topic = 1 sample > 0 0 topic++ sample = topic term scores topic make sure we actually sampled a topic topic == 1 illegal state simple l d a topic not sampled put that topic into counts one doc topics position = topic local topic counts topic ++ tokens per topic topic ++ current type topic counts topic ++ model log likelihood log likelihood = 0 0 likelihood model a combination a dirichlet multinomial words in each topic and a dirichlet multinomial topics in each document likelihood function a dirichlet multinomial gamma sum i alpha i prod i gamma alpha i + n i prod i gamma alpha i gamma sum i alpha i + n i so log likelihood log gamma sum i alpha i log gamma sum i alpha i + n i + sum i log gamma alpha i + n i log gamma alpha i documents first topic counts = num topics topic log gammas = num topics doc topics topic=0 topic < num topics topic++ topic log gammas topic = dirichlet log gamma alpha doc=0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence doc topics = topic sequence get features token=0 token < doc topics length token++ topic counts doc topics token ++ topic=0 topic < num topics topic++ topic counts topic > 0 log likelihood += dirichlet log gamma alpha + topic counts topic topic log gammas topic subtract count + parameter sum term log likelihood = dirichlet log gamma alpha sum + doc topics length arrays fill topic counts 0 add parameter sum term log likelihood += data size dirichlet log gamma alpha sum and topics log gamma beta = dirichlet log gamma beta type=0 type < num types type++ reuse a pointer topic counts = type topic counts type topic = 0 topic < num topics topic++ topic counts topic == 0 log likelihood += dirichlet log gamma beta + topic counts topic log gamma beta na n log likelihood out topic counts topic exit 1 topic=0 topic < num topics topic++ log likelihood = dirichlet log gamma beta num types + tokens per topic topic na n log likelihood out after topic + topic + + tokens per topic topic exit 1 log likelihood += num topics dirichlet log gamma beta num types na n log likelihood out at end exit 1 log likelihood displaying and saving results top words num words builder output = builder sorter sorted words = sorter num types topic = 0 topic < num topics topic++ type = 0 type < num types type++ sorted words type = sorter type type topic counts type topic arrays sort sorted words output append topic + + tokens per topic topic + i=0 i < num words i++ output append alphabet lookup sorted words i get + output append output to filename to print to threshold only print topics proportion greater than number max print no more than many topics print document topics threshold max i o print writer out = print writer out print #doc source topic proportion doc len topic counts = num topics sorter sorted topics = sorter num topics topic = 0 topic < num topics topic++ initialize sorters dummy values sorted topics topic = sorter topic topic max < 0 || max > num topics max = num topics doc = 0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence current doc topics = topic sequence get features out print doc out print data get doc instance get source != out print data get doc instance get source out print source out print doc len = current doc topics length count up tokens token=0 token < doc len token++ topic counts current doc topics token ++ and normalize topic = 0 topic < num topics topic++ sorted topics topic set topic topic counts topic doc len arrays sort sorted topics i = 0 i < max i++ sorted topics i get weight < threshold out print sorted topics i get + + sorted topics i get weight + out print arrays fill topic counts 0 print state f i o print stream out = print stream g z i p output stream buffered output stream output stream f print state out out close print state print stream out out #doc source pos typeindex type topic doc = 0 doc < data size doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence source = n a data get doc instance get source != source = data get doc instance get source to position = 0 position < topic sequence get length position++ type = token sequence get index at position position topic = topic sequence get index at position position out print doc out print out print source out print out print position out print out print type out print out print alphabet lookup type out print out print topic out serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write f output stream oos = output stream output stream f oos write oos close i o e err writing + f + + e write output stream out i o out write u r r e n t s e r i a l v e r s i o n instance lists out write data out write alphabet out write topic alphabet out write num topics out write alpha out write beta out write beta sum out write show topics interval out write words per topic out write random out write formatter out write print log likelihood out write type topic counts ti = 0 ti < num topics ti++ out write tokens per topic ti read input stream in i o not found features length = in read data = list< topic assignment> in read alphabet = alphabet in read topic alphabet = label alphabet in read num topics = in read alpha = in read alpha sum = alpha num topics beta = in read beta sum = in read show topics interval = in read words per topic = in read random = randoms in read formatter = number format in read print log likelihood = in read num docs = data size num types = alphabet size type topic counts = in read tokens per topic = num topics ti = 0 ti < num topics ti++ tokens per topic ti = in read i o instance list training = instance list load 0 num topics = length > 1 ? parse 1 200 simple l d a lda = simple l d a num topics 50 0 0 01 lda add instances training lda sample 1000 