2006 m a l l e t m achine languag e 1 0 further ` l i e n s e inference inferencer types optimize optimizable types utils logger maths timing caching optimizable io io output stream io i o io print stream list bit set iterator list logging logger piecewise p l sutton and mc callum 2007 n b wrong wrong options are an extension that we tried that never quite worked created mar 15 2005 author < a h r e f= mailto casutton edu>casutton edu< a> $ biconditional piecewise a r f trainer v 1 1 2007 10 22 21 37 40 exp $ pwpl a r f trainer acrf trainer logger logger = logger get logger pwpl a r f trainer get name print gradient = n o w r o n g w r o n g = 0 o n d i t i o n w w = 1 wrong wrong type = n o w r o n g w r o n g wrong wrong iter = 10 wrong wrong threshold = 0 1 output prefix = optimizable gradient value create optimizable a r f acrf instance list training pwpl a r f trainer maxable acrf training get wrong wrong threshold wrong wrong threshold set wrong wrong threshold wrong wrong threshold wrong wrong threshold = wrong wrong threshold set wrong wrong type wrong wrong type wrong wrong type = wrong wrong type set wrong wrong iter wrong wrong iter wrong wrong iter = wrong wrong iter train a r f acrf instance list training list instance list validation list instance list test set a r f evaluator eval num iter optimizable gradient value macrf wrong wrong type == n o w r o n g w r o n g train acrf training list validation list test set eval num iter macrf maxable bipw maxable = maxable macrf add wrong wrongs after 5 iterations logger info biconditional piecewise a r f trainer initial training train acrf training list validation list test set eval wrong wrong iter macrf utils write gzipped output prefix initial acrf ser gz acrf logger info biconditional piecewise a r f trainer adding wrong wrongs bipw maxable add wrong wrong training list logger info biconditional piecewise a r f trainer adding wrong wrongs converged = train acrf training list validation list test set eval num iter macrf report training likelihood acrf training list converged reports joint likelihood estimated on training set report training likelihood a r f acrf instance list training list total = 0 inferencer inf = acrf get inferencer i = 0 i < training list size i++ instance inst = training list get i a r f unrolled graph unrolled = acrf unroll inst inf compute marginals unrolled lik = inf lookup log joint unrolled get assignment total += lik logger info instance +i+ likelihood = +lik logger info unregularized joint likelihood = +total maxable caching optimizable gradient a r f acrf instance list train data a r f template templates bit set infinite values = num d e f a u l t g a u s s i a n p r i o r v a r i a n e = 10 0 get gaussian prior variance gaussian prior variance set gaussian prior variance gaussian prior variance gaussian prior variance = gaussian prior variance gaussian prior variance = pwpl a r f trainer maxable d e f a u l t g a u s s i a n p r i o r v a r i a n e vectors that contain counts features observed in training data maps clique template x feature number => count sparse vector constraints vectors that contain expected value over labels all features have seen training data but not training labels sparse vector expectations sparse vector constraints sparse vector expectations init weights instance list training tidx = 0 tidx < templates length tidx++ num += templates tidx init weights training initialize constraints and expectations to have same dimensions weights but to be all zero init constraints expectations defaults first constraints = sparse vector templates length expectations = sparse vector templates length tidx = 0 tidx < templates length tidx++ sparse vector defaults = templates tidx get weights constraints tidx = sparse vector defaults clone matrix zeroed expectations tidx = sparse vector defaults clone matrix zeroed and now others constraints = sparse vector templates length expectations = sparse vector templates length tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights constraints tidx = sparse vector weights length expectations tidx = sparse vector weights length i = 0 i < weights length i++ constraints tidx i = sparse vector weights i clone matrix zeroed expectations tidx i = sparse vector weights i clone matrix zeroed num cvga calls = 0 time per cvga call = 0 reset profiling call num cvga calls = 0 time per cvga call = 0 set all expectations to 0 after they ve been initialized reset expectations tidx = 0 tidx < expectations length tidx++ expectations tidx set all 0 0 i = 0 i < expectations tidx length i++ expectations tidx i set all 0 0 reset constraints tidx = 0 tidx < constraints length tidx++ constraints tidx set all 0 0 i = 0 i < constraints tidx length i++ constraints tidx i set all 0 0 maxable a r f acrf instance list ilist pwpl a r f trainer logger finest initializing optimizable a r f acrf = acrf templates = acrf get templates allocate weights constraints and expectations train data = ilist init weights train data init constraints expectations num instances = train data size cached value stale = cached gradient stale = cache unrolled graphs unrolled graphs = unrolled graph num instances pwpl a r f trainer logger info number training instances = + num instances pwpl a r f trainer logger info number = + num describe prior pwpl a r f trainer logger fine computing constraints collect constraints train data describe prior pwpl a r f trainer logger info using gaussian prior variance + gaussian prior variance get num num negate initial value and value because are in weights not values get buf buf length != num illegal argument argument not + correct dimensions idx = 0 tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector defaults = tmpl get weights values = defaults get values arraycopy values 0 buf idx values length idx += values length tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights assn = 0 assn < weights length assn++ values = weights assn get values arraycopy values 0 buf idx values length idx += values length set internal params cached value stale = cached gradient stale = idx = 0 tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector defaults = tmpl get weights values = defaults get values arraycopy params idx values 0 values length idx += values length tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights assn = 0 assn < weights length assn++ values = weights assn get values arraycopy params idx values 0 values length idx += values length functions unit tests to get constraints and expectations i m too lazy to make a deep copy callers should not modify these sparse vector get expectations cnum expectations cnum sparse vector get constraints cnum constraints cnum print weights print buf = num get buf len = buf length w = 0 w < len w++ out print buf w + out compute value retval = 0 0 num instances = train data size start = current time millis unroll time = 0 reset profiling call instance values must either always or never be in total values we can t just sometimes skip a value because it infinite that off total values we only allow an instance to have infinite value it happens from start we t compute value instance after first round any other instance has infinite value after that it an initializing infinite values = infinite values == we could initialize bitset one slot every instance but it probably cheaper not to taking time hit to allocate space a bit becomes necessary infinite values = bit set initializing infinite values = clear sufficient statistics that we are about to fill reset expectations fill in expectations each instance i = 0 i < num instances i++ instance instance = train data get i compute marginals each clique unroll start = current time millis a r f unrolled graph unrolled = acrf unroll structure only instance a r f unrolled graph unrolled = a r f unrolled graph instance templates arrays list fixed tmpls unroll end = current time millis unroll time += unroll end unroll start unrolled num variables == 0 happens all nodes are pruned save expected value each feature when we compute gradient assignment observations = unrolled get assignment value = collect expectations and value unrolled observations i infinite value initializing infinite values pwpl a r f trainer logger warning instance + instance get name + has infinite value skipping infinite values set i !infinite values get i pwpl a r f trainer logger warning infinite value on instance + instance get name + returning infinity n e g a t i v e i n f i n i t y print debug info unrolled illegal state instance + instance get name + used to have non infinite + value but now it has infinite value na n value out na n on instance + i + + instance get name print debug info unrolled illegal state value na n in a r f get value instance +i pwpl a r f trainer logger warning value na n in a r f get value instance + i + + returning infinity n e g a t i v e i n f i n i t y retval += value incorporate gaussian prior on means that each weight we will add w^2 2 variance to log probability prior denom = 2 gaussian prior variance tidx = 0 tidx < templates length tidx++ sparse vector weights = templates tidx get weights j = 0 j < weights length j++ fnum = 0 fnum < weights j num locations fnum++ w = weights j value at location fnum weight valid w tidx j retval += w w prior denom end = current time millis pwpl a r f trainer logger info a r f inference time ms = + end start pwpl a r f trainer logger info a r f unroll time ms = + unroll time pwpl a r f trainer logger info get value loglikelihood = + retval logger info number v g a calls = + num cvga calls logger info total v g a time ms = + time per cvga call retval computes gradient penalized log likelihood a r f and places it in cached gradient <p > gradient constraint expectation gaussian prior variance compute value gradient grad index into current element cached gradient gidx = 0 first gradient wrt weights tidx = 0 tidx < templates length tidx++ sparse vector these weights = templates tidx get weights sparse vector these constraints = constraints tidx sparse vector these expectations = expectations tidx j = 0 j < these weights num locations j++ weight = these weights value at location j constraint = these constraints value at location j expectation = these expectations value at location j pwpl a r f trainer print gradient out gradient + gidx + = + constraint + ctr + expectation + exp + weight gaussian prior variance + reg feature= d e f a u l t grad gidx++ = constraint expectation weight gaussian prior variance now other weights tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights i = 0 i < weights length i++ sparse vector weight vec = weights i sparse vector constraint vec = constraints tidx i sparse vector expectation vec = expectations tidx i j = 0 j < weight vec num locations j++ w = weight vec value at location j gradient computed below constraint = constraint vec value at location j expectation = expectation vec value at location j a parameter may be set to infinity an external user we set gradient to 0 because parameter s value can never change anyway and it will mess up future calculations on matrix infinite w pwpl a r f trainer logger warning infinite weight node index + i + feature + acrf get input alphabet lookup j gradient = 0 0 gradient = constraint w gaussian prior variance expectation pwpl a r f trainer print gradient idx = weight vec index at location j fname = acrf get input alphabet lookup idx out gradient + gidx + = + constraint + ctr + expectation + exp + w gaussian prior variance + reg feature= + fname + grad gidx++ = gradient every feature f k computes expected value f k aver all possible label sequences given list instances we have <p > these values are stored in collector that collector i j k gets expected value feature clique i label assignment j and input features k collect expectations and value a r f unrolled graph unrolled assignment observations inum value = 0 0 iterator it = unrolled unrolled var set iterator it has next a r f unrolled var set clique = a r f unrolled var set it next a r f template tmpl = clique get template tidx = tmpl index tidx == 1 vi = 0 vi < clique size vi++ variable target = clique get vi value += compute value gradient assn observations clique target wrong wrong type n o w r o n g w r o n g o n d i t i o n w w value += add conditional w w unrolled inum illegal state value add conditional w w a r f unrolled graph unrolled inum value = 0 all wrong wrongs != list wrongs = all wrong wrongs inum iterator it = wrongs iterator it has next wrong wrong ww = wrong wrong it next variable target = ww find variable unrolled a r f unrolled var set clique = ww find var set unrolled assignment wrong = assignment make from single index clique ww assn idx out computing w w +clique+ idx +ww assn idx+ target +target value += compute value gradient assn wrong clique target value compute value gradient assn assignment observations a r f unrolled var set clique variable target num cvga calls++ timing timing = timing a r f template tmpl = clique get template tidx = tmpl index assignment clique assn = assignment restriction observations clique m = target get num outcomes vals = m singles = m assn idx = 0 assn idx < m assn idx++ clique assn set value target assn idx vals assn idx = compute log factor value clique assn tmpl clique get fv singles assn idx = clique assn single index log z = maths sum log prob vals assn idx = 0 assn idx < m assn idx++ marginal = math exp vals assn idx log z exp idx = singles assn idx expectations tidx exp idx plus equals sparse clique get fv marginal expectations tidx location exp idx != 1 expectations tidx increment value exp idx marginal observed val = observations get target time per cvga call += timing elapsed time vals observed val log z compute log factor value assignment clique assn a r f template tmpl feature vector fv sparse vector weights = tmpl get weights idx = clique assn single index sparse vector w = weights idx dp = w dot product fv dp += tmpl get weight idx dp collect constraints instance list ilist inum = 0 inum < ilist size inum++ pwpl a r f trainer logger finest collecting constraints instance + inum instance inst = ilist get inum a r f unrolled graph unrolled = a r f unrolled graph inst templates iterator it = unrolled unrolled var set iterator it has next a r f unrolled var set clique = a r f unrolled var set it next tidx = clique get template index tidx == 1 assn = clique lookup assignment number constraints tidx assn plus equals sparse clique get fv clique size constraints tidx location assn != 1 constraints tidx increment value assn clique size constraints wrong wrongs instance all wrong wrongs != list wrongs = all wrong wrongs inum iterator ww it = wrongs iterator ww it has next wrong wrong ww = wrong wrong ww it next a r f unrolled var set clique = ww find var set unrolled tidx = clique get template index wrong2right = ww assn idx constraints tidx wrong2right plus equals sparse clique get fv 1 0 constraints tidx location wrong2right != 1 constraints tidx increment value wrong2right 1 0 dump gradient to name grad = get num get value gradient grad print stream w = print stream output stream name i = 0 i < num i++ w grad i w close i o e err could not open output e print stack trace dump defaults out constraints i = 0 i < constraints length i++ out template + i constraints i print out expectations i = 0 i < expectations length i++ out template + i expectations i print print debug info a r f unrolled graph unrolled acrf print err assignment assn = unrolled get assignment iterator it = unrolled unrolled var set iterator it has next a r f unrolled var set clique = a r f unrolled var set it next out clique + clique dump assn clique assn clique factor ptl = unrolled factor clique out value = + ptl value assn out ptl dump assn clique assignment assn a r f unrolled var set clique iterator it = clique iterator it has next variable var = variable it next out var + ==> + assn get var + + assn get var + weight valid w cnum j infinite w pwpl a r f trainer logger warning weight infinite clique + cnum + assignment + j na n w pwpl a r f trainer logger warning weight nan clique + cnum + assignment + j w r o n g w r o n g h a n d l i n g wrong wrong var idx vs idx assn idx wrong wrong a r f unrolled graph graph var set vs variable var assn idx var idx = graph get index var vs idx = graph get index vs assn idx = assn idx a r f unrolled var set find var set a r f unrolled graph unrolled unrolled get unrolled var set vs idx variable find variable a r f unrolled graph unrolled unrolled get var idx list all wrong wrongs add wrong wrong instance list training all wrong wrongs = list training size total added = 0 !acrf cache unrolled graphs illegal state wrong wrong won t work without caching unrolled graphs i = 0 i < training size i++ all wrong wrongs i = list num added = 0 instance instance = training get i a r f unrolled graph unrolled = acrf unroll instance unrolled factors size == 0 err w a r n i n g factor graph instance + instance get name + no factors inferencer inf = acrf get inferencer inf compute marginals unrolled assignment target = unrolled get assignment iterator it = unrolled unrolled var set iterator it has next a r f unrolled var set vs = a r f unrolled var set it next factor marg = inf lookup marginal vs assignment iterator assn it = vs assignment iterator assn it has next assn it advance marg value assn it > wrong wrong threshold assignment assn = assn it assignment vi = 0 vi < vs size vi++ variable var = vs get vi wrong2 right assn target assn var assn idx = assn single index out computing w w +vs+ idx +assn idx+ target +var all wrong wrongs i add wrong wrong unrolled vs var assn idx num added++ logger info wrong wrongs instance + i + + instance get name + num added = + num added total added += num added reset constraints collect constraints training force stale logger info total timesteps = + total timesteps training logger info total wrong wrongs = + total added total timesteps instance list ilist total = 0 i = 0 i < ilist size i++ instance inst = ilist get i sequence seq = sequence inst get data total += seq size total wrong2 right assn assignment target assignment assn variable to exclude variable vars = assn get vars i = 0 i < vars length i++ variable variable = vars i variable != to exclude assn get variable != target get variable assn get to exclude == target get to exclude optimizable a r f 