2003 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e types assignment types assignment iterator types factor types variable optimize optimizable types instance types instance list types sparse vector logger caching optimizable io output stream io i o io print stream io serializable bit set iterator logging logger created mar 15 2005 author < a h r e f= mailto casutton edu>casutton edu< a> $ cliquewise a r f trainer v 1 1 2007 10 22 21 37 40 exp $ piecewise a r f trainer acrf trainer logger logger = logger get logger piecewise a r f trainer get name print gradient = optimizable gradient value create optimizable a r f acrf instance list training maxable acrf training maxable caching optimizable gradient serializable a r f acrf instance list train data a r f template templates a r f template fixed tmpls bit set infinite values = num d e f a u l t g a u s s i a n p r i o r v a r i a n e = 10 0 get gaussian prior variance gaussian prior variance set gaussian prior variance gaussian prior variance gaussian prior variance = gaussian prior variance gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e vectors that contain counts features observed in training data maps clique template x feature number => count sparse vector constraints vectors that contain expected value over labels all features have seen training data but not training labels sparse vector expectations sparse vector constraints sparse vector expectations init weights instance list training tidx = 0 tidx < templates length tidx++ num += templates tidx init weights training initialize constraints and expectations to have same dimensions weights but to be all zero init constraints expectations defaults first constraints = sparse vector templates length expectations = sparse vector templates length tidx = 0 tidx < templates length tidx++ sparse vector defaults = templates tidx get weights constraints tidx = sparse vector defaults clone matrix zeroed expectations tidx = sparse vector defaults clone matrix zeroed and now others constraints = sparse vector templates length expectations = sparse vector templates length tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights constraints tidx = sparse vector weights length expectations tidx = sparse vector weights length i = 0 i < weights length i++ constraints tidx i = sparse vector weights i clone matrix zeroed expectations tidx i = sparse vector weights i clone matrix zeroed set all expectations to 0 after they ve been initialized reset expectations tidx = 0 tidx < expectations length tidx++ expectations tidx set all 0 0 i = 0 i < expectations tidx length i++ expectations tidx i set all 0 0 reset constraints tidx = 0 tidx < constraints length tidx++ constraints tidx set all 0 0 i = 0 i < constraints tidx length i++ constraints tidx i set all 0 0 maxable a r f acrf instance list ilist logger finest initializing optimizable a r f acrf = acrf templates = acrf get templates fixed tmpls = acrf get fixed templates allocate weights constraints and expectations train data = ilist init weights train data init constraints expectations num instances = train data size cached value stale = cached gradient stale = cache unrolled graphs unrolled graphs = unrolled graph num instances logger info number training instances = + num instances logger info number = + num describe prior logger fine computing constraints collect constraints train data describe prior logger info using gaussian prior variance +gaussian prior variance get num num negate initial value and value because are in weights not values get buf buf length != num illegal argument argument not + correct dimensions idx = 0 tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector defaults = tmpl get weights values = defaults get values arraycopy values 0 buf idx values length idx += values length tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights assn = 0 assn < weights length assn++ values = weights assn get values arraycopy values 0 buf idx values length idx += values length set internal params cached value stale = cached gradient stale = idx = 0 tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector defaults = tmpl get weights values = defaults get values arraycopy params idx values 0 values length idx += values length tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights assn = 0 assn < weights length assn++ values = weights assn get values arraycopy params idx values 0 values length idx += values length functions unit tests to get constraints and expectations i m too lazy to make a deep copy callers should not modify these sparse vector get expectations cnum expectations cnum sparse vector get constraints cnum constraints cnum print weights print buf = num get buf len = buf length w = 0 w < len w++ out print buf w + out compute value retval = 0 0 num instances = train data size start = current time millis unroll time = 0 instance values must either always or never be in total values we can t just sometimes skip a value because it infinite that off total values we only allow an instance to have infinite value it happens from start we t compute value instance after first round any other instance has infinite value after that it an initializing infinite values = infinite values == we could initialize bitset one slot every instance but it probably cheaper not to taking time hit to allocate space a bit becomes necessary infinite values = bit set initializing infinite values = clear sufficient statistics that we are about to fill reset expectations fill in expectations each instance i = 0 i < num instances i++ retval += compute value instance i retval += compute prior end = current time millis logger info a r f inference time ms = + end start logger info a r f unroll time ms = +unroll time logger info get value loglikelihood = +retval retval incorporate gaussian prior on means that each weight we will add w^2 2 variance to log probability compute prior retval = 0 0 prior denom = 2 gaussian prior variance tidx = 0 tidx < templates length tidx++ sparse vector weights = templates tidx get weights j = 0 j < weights length j++ fnum = 0 fnum < weights j num locations fnum++ w = weights j value at location fnum weight valid w tidx j retval += w w prior denom retval compute value instance i retval = 0 0 instance instance = train data get i compute marginals each clique a r f unrolled graph unrolled = a r f unrolled graph instance templates fixed tmpls unrolled num variables == 0 0 happens all nodes are pruned save expected value each feature when we compute gradient assignment observations = unrolled get assignment value = collect expectations and value unrolled observations na n value out na n on instance +i+ +instance get name print debug info unrolled illegal state value na n in a r f get value instance +i logger warning value na n in a r f get value instance +i+ + returning infinity n e g a t i v e i n f i n i t y retval += value retval computes gradient penalized log likelihood a r f and places it in cached gradient gradient constraint expectation gaussian prior variance compute value gradient grad compute value gradient grad 1 0 compute value gradient grad prior scale index into current element cached gradient gidx = 0 first gradient wrt weights tidx = 0 tidx < templates length tidx++ sparse vector these weights = templates tidx get weights sparse vector these constraints = constraints tidx sparse vector these expectations = expectations tidx j = 0 j < these weights num locations j++ weight = these weights value at location j constraint = these constraints value at location j expectation = these expectations value at location j print gradient out gradient +gidx+ = +constraint+ ctr +expectation+ exp + weight gaussian prior variance + reg feature= d e f a u l t grad gidx++ = constraint expectation prior scale weight gaussian prior variance now other weights tidx = 0 tidx < templates length tidx++ a r f template tmpl = templates tidx sparse vector weights = tmpl get weights i = 0 i < weights length i++ sparse vector weight vec = weights i sparse vector constraint vec = constraints tidx i sparse vector expectation vec = expectations tidx i j = 0 j < weight vec num locations j++ w = weight vec value at location j gradient computed below constraint = constraint vec value at location j expectation = expectation vec value at location j a parameter may be set to infinity an external user we set gradient to 0 because parameter s value can never change anyway and it will mess up future calculations on matrix infinite w logger warning infinite weight node index +i+ feature + acrf get input alphabet lookup j gradient = 0 0 gradient = constraint prior scale w gaussian prior variance expectation print gradient idx = weight vec index at location j fname = acrf get input alphabet lookup idx out gradient +gidx+ = +constraint+ ctr +expectation+ exp + w gaussian prior variance + reg feature= +fname+ grad gidx++ = gradient every feature f k computes expected value f k aver all possible label sequences given list instances we have these values are stored in collector that collector i j k gets expected value feature clique i label assignment j and input features k collect expectations and value a r f unrolled graph unrolled assignment observations value = 0 0 iterator it = unrolled unrolled var set iterator it has next a r f unrolled var set clique = a r f unrolled var set it next tidx = clique get template index tidx == 1 factor ptl = unrolled factor clique log z = math log ptl sum each assigment to clique xxx s l o w will need to be sparsified assignment iterator assn it = clique assignment iterator i = 0 assn it has next marginal = math exp ptl log value assn it log z expectations tidx i plus equals sparse clique get fv marginal expectations tidx location i != 1 expectations tidx increment value i marginal assn it advance i++ value += ptl log value observations log z value collect constraints instance list ilist inum = 0 inum < ilist size inum++ logger finest collecting constraints instance +inum collect constraints instance ilist inum collect constraints instance instance list ilist inum instance inst = ilist get inum a r f unrolled graph unrolled = a r f unrolled graph inst templates iterator it = unrolled unrolled var set iterator it has next a r f unrolled var set clique = a r f unrolled var set it next tidx = clique get template index tidx == 1 assn = clique lookup assignment number constraints tidx assn plus equals sparse clique get fv constraints tidx location assn != 1 constraints tidx increment value assn 1 0 dump gradient to name grad = get num get value gradient grad print stream w = print stream output stream name i = 0 i < num i++ w grad i w close i o e err could not open output e print stack trace dump defaults out constraints i = 0 i < constraints length i++ out template +i constraints i print out expectations i = 0 i < expectations length i++ out template +i expectations i print print debug info a r f unrolled graph unrolled acrf print err assignment assn = unrolled get assignment iterator it = unrolled unrolled var set iterator it has next a r f unrolled var set clique = a r f unrolled var set it next out clique +clique dump assn clique assn clique factor ptl = unrolled factor clique out value = +ptl value assn out ptl dump assn clique assignment assn a r f unrolled var set clique iterator it = clique iterator it has next variable var = variable it next out var+ ==> +assn get var + +assn get var + weight valid w cnum j infinite w logger warning weight infinite clique +cnum+ assignment +j na n w logger warning weight nan clique +cnum+ assignment +j num in batch = 0 compute value and gradient instance num in batch++ collect constraints instance train data instance value = compute value instance instance value += compute prior train data size value get num instances train data size get cached gradient grad compute value gradient grad num in batch train data size reset value gradient reset expectations reset constraints optimizable a r f 