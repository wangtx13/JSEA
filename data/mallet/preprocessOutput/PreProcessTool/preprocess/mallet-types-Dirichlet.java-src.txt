2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e types gnu trove t hash set gnu trove t hash map gnu trove t iterator io buffered writer io writer io i o io print writer text number format list arrays collection types multinomial maths randoms various useful functions related to dirichlet distributions author andrew mc callum and david mimno dirichlet alphabet dict magnitude = 1 partition randoms random = actually negative euler mascheroni constant e u l e r m a s h e r o n i = 0 5772156649015328606065121 p i s q u a r e d o v e r s i x = math p i math p i 6 h a l f l o g t w o p i = math log 2 math p i 2 d i g a m m a o e f 1 = 1 12 d i g a m m a o e f 2 = 1 120 d i g a m m a o e f 3 = 1 252 d i g a m m a o e f 4 = 1 240 d i g a m m a o e f 5 = 1 132 d i g a m m a o e f 6 = 691 32760 d i g a m m a o e f 7 = 1 12 d i g a m m a o e f 8 = 3617 8160 d i g a m m a o e f 9 = 43867 14364 d i g a m m a o e f 10 = 174611 6600 d i g a m m a l a r g e = 9 5 d i g a m m a s m a l l = 000001 a dirichlet parameterized a and a magnitude m magnitude dirichlet sum i alpha i p a probability p i = alpha i m dirichlet m p magnitude = m partition = p a symmetric dirichlet e x i = e x j all i j m magnitude dirichlet sum i alpha i n number dimensions dirichlet m n magnitude = m partition = n partition 0 = 1 0 n i=1 i<n i++ partition i = partition 0 a dirichlet parameterized a single vector positive reals dirichlet p magnitude = 0 partition = p length add up total i=0 i<p length i++ magnitude += p i i=0 i<p length i++ partition i = p i magnitude constructor that takes an alphabet representing meaning each dimension dirichlet alphas alphabet dict alphas dict != alphas length != dict size illegal argument alphas and dict sizes not match dict = dict dict != dict stop growth a symmetric dirichlet alpha i = 1 0 and number dimensions given alphabet dirichlet alphabet dict dict 1 0 a symmetric dirichlet alpha i = <code>alpha< code> and number dimensions given alphabet dirichlet alphabet dict alpha dict size alpha dict = dict dict stop growth a symmetric dirichlet alpha i = 1 0 and <code>size< code> dimensions dirichlet size size 1 0 a symmetric dirichlet e x i = e x j all i j n number dimensions alpha parameter each dimension dirichlet size alpha magnitude = size alpha partition = size partition 0 = 1 0 size i=1 i<size i++ partition i = partition 0 init random random == random = randoms next = partition length init random each dimension draw a sample from gamma mp i 1 sum = 0 i=0 i<distribution length i++ i = random next gamma partition i magnitude 1 i <= 0 i = 0 0001 sum += i normalize i=0 i<distribution length i++ i = sum create a printable list alpha i to magnitude buffer output = buffer number format formatter = number format get instance formatter set maximum fraction digits 5 output append formatter format magnitude + i=0 i<distribution length i++ output append formatter format i + output to write alpha i to specified one per line to filename i o print writer out = print writer buffered writer writer filename i=0 i<partition length i++ out magnitude partition i out flush out close dirichlet multinomial draw a from dirichlet then draw n samples from that multinomial draw observation n init random = next draw observation n draw a count vector from probability n <i>expected< i> total number counts in vector actual number ~ poisson <code>n< code> draw observation n init random histogram = partition length arrays fill histogram 0 count i was using a poisson but poisson variate generator goes berzerk lambda above ~500 n < 100 count = random next poisson p n 100 10 <= 0 = 7 619853e 24 count = math round random next gaussian n n i=0 i<count i++ histogram random next discrete ++ histogram create a set d draws from a dirichlet multinomial each an average n observations draw observations d n observations = d i=0 i<d i++ observations i = draw observation n observations calculates a log gamma function exactly it s extremely inefficient use comparison only log gamma definition z result = e u l e r m a s h e r o n i z math log z k=1 k < 10000000 k++ result += z k math log 1 + z k result directly calculates difference between two log gamma functions using a recursive formula even stirling approximation about n=2 so it s not necessarily worth using log gamma difference z n result = 0 0 i=0 i < n i++ result += math log z + i result currently aliased to <code>log gamma stirling< code> log gamma z log gamma stirling z use a fifth order stirling s approximation z note that stirling s approximation increasingly unstable <code>z< code> approaches 0 <code>z< code> less than 2 we shift it up calculate approximation and then shift answer back down log gamma stirling z shift = 0 z < 2 z++ shift++ result = h a l f l o g t w o p i + z 0 5 math log z z + 1 12 z 1 360 z z z + 1 1260 z z z z z shift > 0 shift z result = math log z result gergo nemes approximation log gamma nemes z result = h a l f l o g t w o p i math log z 2 + z math log z + 1 12 z 1 10 z 1 result calculate digamma using an asymptotic expansion involving bernoulli numbers digamma z based on matlab tom minka z < 0 out less than zero psi = 0 z < d i g a m m a s m a l l psi = e u l e r m a s h e r o n i 1 z + p i s q u a r e d o v e r s i x z n=1 n<100000 n++ psi += z n n + z psi z < d i g a m m a l a r g e psi = 1 z z++ inv z = 1 z inv z squared = inv z inv z psi += math log z 5 inv z inv z squared d i g a m m a o e f 1 inv z squared d i g a m m a o e f 2 inv z squared d i g a m m a o e f 3 inv z squared d i g a m m a o e f 4 inv z squared d i g a m m a o e f 5 inv z squared d i g a m m a o e f 6 inv z squared d i g a m m a o e f 7 psi digamma difference x n sum = 0 i=0 i<n i++ sum += 1 x + i sum trigamma z shift = 0 z < 2 z++ shift++ one over z = 1 0 z one over z squared = one over z one over z result = one over z + 0 5 one over z squared + 0 1666667 one over z squared one over z 0 03333333 one over z squared one over z squared one over z + 0 02380952 one over z squared one over z squared one over z squared one over z 0 03333333 one over z squared one over z squared one over z squared one over z squared one over z shift > 0 shift z result += 1 0 z z result learn concentration parameter a symmetric dirichlet using frequency histograms since all are same we only need to keep track number observation dimension pairs count n count histogram an frequencies matrix x represents observations such that x<sub>dt< sub> how many times word t occurs in document d <code>count histogram 3 < code> total number cells <i>in any column< i> that equal 3 observation lengths a histogram sample lengths example <code>observation lengths 20 < code> could be number documents that are exactly 20 tokens num dimensions total number dimensions current value an initial starting value learn symmetric concentration count histogram observation lengths num dimensions current value current digamma histogram arrays are presumably allocated before we knew what went in them it therefore likely that largest non zero value may be much closer to beginning than end we t want to iterate over a whole bunch zeros so keep track last value largest non zero count = 0 non zero length index = observation lengths length index = 0 index < count histogram length index++ count histogram index > 0 largest non zero count = index dense index = 0 index = 0 index < observation lengths length index++ observation lengths index > 0 non zero length index dense index = index dense index++ dense index size = dense index iteration = 1 iteration <= 200 iteration++ current parameter = current value num dimensions calculate numerator current digamma = 0 numerator = 0 counts 0 t matter so start 1 index = 1 index <= largest non zero count index++ current digamma += 1 0 current parameter + index 1 numerator += count histogram index current digamma now calculate denominator a sum over all observation lengths current digamma = 0 denominator = 0 previous length = 0 cached digamma = digamma current value dense index = 0 dense index < dense index size dense index++ length = non zero length index dense index length previous length > 20 next length sufficiently far from previous it s faster to recalculate from scratch current digamma = digamma current value + length cached digamma otherwise iterate up looks slightly different from previous no 1 because we re indexing differently index = previous length index < length index++ current digamma += 1 0 current value + index denominator += current digamma observation lengths length current value = current parameter numerator denominator out current value + = + current parameter + + numerator + + denominator current value test symmetric concentration num dimensions num observations observation mean length log d = math log num dimensions exponent = 5 exponent < 4 exponent++ alpha = num dimensions 1 0 dirichlet prior = dirichlet num dimensions alpha num dimensions count histogram = 1000000 observation lengths = 1000000 observations = prior draw observations num observations observation mean length dirichlet optimized dirichlet = dirichlet num dimensions 1 0 optimized dirichlet learn histogram observations out optimized dirichlet magnitude i=0 i < num observations i++ observation = observations i total = 0 k=0 k < num dimensions k++ observation k > 0 total += observation k count histogram observation k ++ observation lengths total ++ estimated alpha = learn symmetric concentration count histogram observation lengths num dimensions 1 0 out alpha + + estimated alpha + + math abs alpha estimated alpha learn dirichlet using frequency histograms a reference to current values which will be updated in place observations an count histograms <code>observations 10 3 < code> could be number documents that contain exactly 3 tokens word type 10 observation lengths a histogram sample lengths example <code>observation lengths 20 < code> could be number documents that are exactly 20 tokens sum learned learn observations observation lengths learn observations observation lengths 1 00001 1 0 200 learn dirichlet using frequency histograms a reference to current values which will be updated in place observations an count histograms <code>observations 10 3 < code> could be number documents that contain exactly 3 tokens word type 10 observation lengths a histogram sample lengths example <code>observation lengths 20 < code> could be number documents that are exactly 20 tokens shape gamma prior e x = shape scale var x = shape scale<sup>2< sup> scale num iterations 200 to 1000 generally insures convergence but 1 5 often enough to step in right direction sum learned learn observations observation lengths shape scale num iterations i k sum = 0 initialize parameter sum k=0 k < length k++ sum += k old k current digamma denominator non zero limit non zero limits = observations length arrays fill non zero limits 1 histogram arrays go up to size largest document but non zero values will almost always cluster in low end we avoid looping over empty arrays saving index largest non zero value histogram i=0 i<observations length i++ histogram = observations i buffer out = buffer k = 0 k < histogram length k++ histogram k > 0 non zero limits i = k out append k + + histogram k + out out iteration=0 iteration<num iterations iteration++ calculate denominator denominator = 0 current digamma = 0 iterate over histogram i=1 i<observation lengths length i++ current digamma += 1 sum + i 1 denominator += observation lengths i current digamma bayesian estimation i denominator = 1 scale calculate individual sum = 0 k=0 k<parameters length k++ what s largest non zero element in histogram? non zero limit = non zero limits k old k = k k = 0 current digamma = 0 histogram = observations k i=1 i <= non zero limit i++ current digamma += 1 old k + i 1 k += histogram i current digamma bayesian estimation i i k = old k k + shape denominator sum += k sum < 0 0 runtime sum + sum sum use fixed point iteration described tom minka learn histogram observations max length = 0 max bin counts = partition length arrays fill max bin counts 0 i=0 i < observations length i++ length = 0 observation = observations i bin=0 bin < observation length bin++ observation bin > max bin counts bin max bin counts bin = observation bin length += observation bin length > max length max length = length arrays start at zero so i m sacrificing one greater clarity later on bin count histograms = partition length bin=0 bin < partition length bin++ bin count histograms bin = max bin counts bin + 1 arrays fill bin count histograms bin 0 out got mem + current time millis start length histogram = max length + 1 arrays fill length histogram 0 out got lengths + current time millis start i=0 i < observations length i++ length = 0 observation = observations i bin=0 bin < observation length bin++ bin count histograms bin observation bin ++ length += observation bin length histogram length ++ learn histogram bin count histograms length histogram learn histogram bin count histograms length histogram start = current time millis = partition length alpha k current digamma denominator sum = 0 0 i k k = 0 k < partition length k++ k = magnitude partition k sum += k iteration=0 iteration<1000 iteration++ calculate denominator denominator = 0 current digamma = 0 i=1 i < length histogram length i++ current digamma += 1 sum + i 1 denominator += length histogram i current digamma denominator > 0 0 ! na n denominator sum = 0 0 calculate individual k=0 k<partition length k++ alpha k = k k = 0 0 current digamma = 0 histogram = bin count histograms k histogram length <= 1 since histogram 0 0 k = 0 000001 i=1 i<histogram length i++ current digamma += 1 alpha k + i 1 k += histogram i current digamma ! k > 0 0 out length empty + 0 length i=0 i<histogram length i++ out print histogram i + out k > 0 0 ! na n k k = alpha k denominator sum += k iteration % 25 == 0 out to sum to newsgroups direct iteration + iteration out iteration + + current time millis start e out e k = 0 k < partition length k++ partition k = k sum magnitude = sum out to magnitude partition current time millis start use fixed point iteration described tom minka learn digamma observations bin counts = partition length observations length out got mem + current time millis start observation lengths = observations length out got lengths + current time millis start i=0 i < observations length i++ observation = observations i bin=0 bin < partition length bin++ bin counts bin i = observation bin observation lengths i += observation bin out init + current time millis start learn digamma bin counts observation lengths learn digamma bin counts observation lengths start = current time millis = partition length alpha k denominator magnitude i k iteration=0 iteration<1000 iteration++ magnitude = 0 calculate denominator denominator = 0 i=0 i<observation lengths length i++ denominator += digamma magnitude + observation lengths i denominator = observation lengths length digamma magnitude calculate individual k=0 k<partition length k++ k = 0 counts = bin counts k alpha k = magnitude partition k digamma alpha k = digamma alpha k i=0 i<counts length i++ counts i == 0 k += digamma alpha k k += digamma alpha k + counts i k = counts length digamma alpha k k <= 0 k = 0 000001 k = alpha k denominator k <= 0 out k + + alpha k + + denominator k > 0 ! na n k magnitude += k out finished dimension + k magnitude = magnitude k=0 k<partition length k++ partition k = k magnitude k < 20 out partition k + = +new k + +magnitude iteration % 25 == 0 to newsgroups digamma iteration + iteration out iteration + + current time millis start e out e out to magnitude partition current time millis start estimate a dirichlet moment matching described ronning learn moments observations start = current time millis i bin observation lengths = observations length variances = partition length arrays fill partition 0 0 arrays fill observation lengths 0 arrays fill variances 0 0 find e p k s i=0 i < observations length i++ observation = observations i find sum counts in each bin bin=0 bin < partition length bin++ observation lengths i += observation bin bin=0 bin < partition length bin++ partition bin += observation bin observation lengths i bin=0 bin < partition length bin++ partition bin = observations length find var p k s difference i=0 i < observations length i++ observation = observations i bin=0 bin < partition length bin++ difference = observation bin observation lengths i partition bin variances bin += difference difference avoiding math pow bin=0 bin < partition length bin++ variances bin = observations length 1 now calculate magnitude log \sum k \alpha k = 1 k 1 \sum k log e p k 1 e p k var p k 1 sum = 0 0 bin=0 bin < partition length bin++ partition bin == 0 sum += math log partition bin 1 partition bin variances bin 1 magnitude = math exp sum partition length 1 out to magnitude partition current time millis start learn leave one out observations bin counts = partition length observations length out got mem + current time millis start observation lengths = observations length out got lengths + current time millis start i=0 i < observations length i++ observation = observations i bin=0 bin < partition length bin++ bin counts bin i = observation bin observation lengths i += observation bin out init + current time millis start learn leave one out bin counts observation lengths learn using minka s leave one out l o o likelihood learn leave one out bin counts observation lengths start = current time millis i bin = partition length bin sums = partition length observation sum = 0 0 parameter sum = 0 0 counts uniform initialization arrays fill partition 1 0 partition length iteration = 0 iteration < 1000 iteration++ observation sum = 0 0 arrays fill bin sums 0 0 i=0 i < observation lengths length i++ observation sum += observation lengths i observation lengths i 1 + magnitude bin=0 bin < partition length bin++ counts = bin counts bin i=0 i<counts length i++ counts i >= 2 bin sums bin += counts i counts i 1 + magnitude partition bin parameter sum = 0 0 bin=0 bin < partition length bin++ bin sums bin == 0 0 bin = 0 000001 bin = partition bin magnitude bin sums bin observation sum parameter sum += bin bin=0 bin < partition length bin++ partition bin = bin parameter sum magnitude = parameter sum iteration % 50 == 0 out iteration + + magnitude out to magnitude partition current time millis start compute l1 residual between two dirichlets absolute difference dirichlet other partition length != other partition length illegal argument dirichlets must have same dimension to be compared residual = 0 0 k=0 k<partition length k++ residual += math abs partition k magnitude other partition k other magnitude residual compute l2 residual between two dirichlets squared difference dirichlet other partition length != other partition length illegal argument dirichlets must have same dimension to be compared residual = 0 0 k=0 k<partition length k++ residual += math pow partition k magnitude other partition k other magnitude 2 residual check breakeven x start clock1 clock2 digamma x = digamma x n=1 n < 100 n++ start = current time millis i=0 i<1000000 i++ digamma x + n clock1 = current time millis start start = current time millis i=0 i<1000000 i++ digamma difference x n clock2 = current time millis start out n + direct + clock1 + indirect + clock2 + + clock1 clock2 + out + digamma x + n digamma x + + digamma difference x n compare sum k n w dirichlet uniform dirichlet dirichlet buffer output = buffer output append sum + + k + + n + + w + uniform dirichlet = dirichlet k sum k dirichlet = dirichlet sum uniform dirichlet next out real + to dirichlet magnitude dirichlet partition observations = dirichlet draw observations n w out done drawing time dirichlet estimated dirichlet = dirichlet k sum k time = estimated dirichlet learn digamma observations output append time + + dirichlet absolute difference estimated dirichlet + estimated dirichlet = dirichlet k sum k time = estimated dirichlet learn histogram observations output append time + + dirichlet absolute difference estimated dirichlet + estimated dirichlet = dirichlet k sum k time = estimated dirichlet learn moments observations output append time + + dirichlet absolute difference estimated dirichlet + out moments + time + + dirichlet absolute difference estimated dirichlet estimated dirichlet = dirichlet k sum k time = estimated dirichlet learn leave one out observations output append time + + dirichlet absolute difference estimated dirichlet + out leave one out + time + + dirichlet absolute difference estimated dirichlet output to what probability that these two observations were drawn from same multinomial symmetric dirichlet prior alpha relative to probability that they were drawn from different multinomials both drawn from dirichlet? dirichlet multinomial likelihood ratio t hash map counts x t hash map counts y alpha alpha sum likelihood one d m gamma alpha sum prod gamma alpha + n i prod gamma alpha gamma alpha sum + n when we divide product two other d ms same alpha parameter first term in numerator cancels first term in denominator then moving remaining alpha only term to numerator we get prod gamma alpha prod gamma alpha + x i + y i gamma alpha sum gamma alpha sum + x sum + y sum prod gamma alpha + x i prod gamma alpha + y i gamma alpha sum + x sum gamma alpha sum + y sum log likelihood = 0 0 log gamma alpha = log gamma alpha total x = 0 total y = 0 key x y t hash set distinct keys = t hash set distinct keys add all counts x keys distinct keys add all counts y keys t iterator iterator = distinct keys iterator iterator has next key = iterator next x = 0 counts x contains key key x = counts x get key y = 0 counts y contains key key y = counts y get key total x += x total y += y log likelihood += log gamma alpha + log gamma alpha + x + y log gamma alpha + x log gamma alpha + y log likelihood += log gamma alpha sum + total x + log gamma alpha sum + total y log gamma alpha sum log gamma alpha sum + total x + total y log likelihood what probability that these two observations were drawn from same multinomial symmetric dirichlet prior alpha relative to probability that they were drawn from different multinomials both drawn from dirichlet? dirichlet multinomial likelihood ratio counts x counts y alpha alpha sum exactly same that takes trove hashmaps but fixed size arrays counts x length != counts y length illegal argument both arrays must contain same number dimensions log likelihood = 0 0 log gamma alpha = log gamma alpha total x = 0 total y = 0 x y key=0 key < counts x length key++ x = counts x key y = counts y key total x += x total y += y log likelihood += log gamma alpha + log gamma alpha + x + y log gamma alpha + x log gamma alpha + y log likelihood += log gamma alpha sum + total x + log gamma alpha sum + total y log gamma alpha sum log gamma alpha sum + total x + total y log likelihood uses a non symmetric dirichlet prior dirichlet multinomial likelihood ratio counts x counts y counts x length != counts y length || counts x length != partition length illegal argument both arrays and dirichlet prior must contain same number dimensions log likelihood = 0 0 alpha total x = 0 total y = 0 x y key=0 key < counts x length key++ x = counts x key y = counts y key total x += x total y += y alpha = partition key magnitude log likelihood += log gamma alpha + log gamma alpha + x + y log gamma alpha + x log gamma alpha + y log likelihood += log gamma magnitude + total x + log gamma magnitude + total y log gamma magnitude log gamma magnitude + total x + total y log likelihood similar to dirichlet multinomial test s a likelihood ratio based on ewens sampling formula which can be considered partitions integers generated chinese restaurant process ewens likelihood ratio counts x counts y lambda counts x length != counts y length illegal argument both arrays must contain same number dimensions log likelihood = 0 0 alpha total x = 0 total y = 0 total = 0 x y first count up totals key=0 key < counts x length key++ x = counts x key y = counts y key total x += x total y += y total += x + y now allocate some arrays sufficient statisitics number classes that contain x elements count histogram x = total + 1 count histogram y = total + 1 count histogram both = total + 1 key=0 key < counts x length key++ x = counts x key y = counts y key count histogram x x ++ count histogram x y ++ count histogram both x + y ++ j=1 j <= total j++ count histogram x j == 0 count histogram y j == 0 count histogram both j == 0 log likelihood += count histogram both j count histogram x j count histogram y j math log lambda j log likelihood += log gamma count histogram x j + 1 + log gamma count histogram y j + 1 log gamma count histogram both j + 1 log likelihood += log gamma total + 1 log gamma total x + 1 log gamma total y + 1 log likelihood += log gamma lambda + total x + log gamma lambda + total y log gamma lambda log gamma lambda + total x + total y log likelihood run comparison precision dimensions documents mean size print writer out = print writer buffered writer writer comparison dimensions = 10 j=0 j<5 j++ documents = 100 k=0 k<5 k++ mean size = 100 l=0 l<5 l++ out dimensions + + dimensions + + documents + + mean size run ten times m=0 m<10 m++ always use dir 1 1 1 1 now out compare dimensions dimensions documents mean size out flush mean size = 2 documents = 2 dimensions = 2 out flush out close e e print stack trace out test symmetric concentration 1000 100 1000 dirichlet prior = dirichlet 100 1 0 x y i=0 i<50 i++ dirichlet non symmetric = dirichlet 100 prior next two observations from same multinomial = non symmetric next x = non symmetric draw observation 100 y = non symmetric draw observation 100 out print non symmetric dirichlet multinomial likelihood ratio x y + out print ewens likelihood ratio x y 1 + two observations from different multinomials x = non symmetric draw observation 100 y = non symmetric draw observation 100 out print ewens likelihood ratio x y 0 1 + out non symmetric dirichlet multinomial likelihood ratio x y alphabet get alphabet dict size partition length alpha feature index magnitude partition feature index print out dirichlet j = 0 j < partition length j++ out dict!= ? dict lookup j to j + = + magnitude partition j random raw multinomial randoms r sum = 0 pr = partition length i = 0 i < partition length i++ alphas i < 0 j = 0 j < alphas length j++ out dict lookup symbol j to + = + alphas j pr i = r next gamma magnitude partition i sum += pr i i = 0 i < partition length i++ pr i = sum pr multinomial random multinomial randoms r multinomial random raw multinomial r dict partition length dirichlet random dirichlet randoms r average alpha pr = random raw multinomial r alpha sum = pr length average alpha out random dirichlet alpha sum = +alpha sum i = 0 i < pr length i++ pr i = alpha sum dirichlet pr dict feature sequence random feature sequence randoms r length multinomial m = random multinomial r m random feature sequence r length feature vector random feature vector randoms r size feature vector random feature sequence r size token sequence random token sequence randoms r length feature sequence fs = random feature sequence r length token sequence ts = token sequence length i = 0 i < length i++ ts add fs get at position i to ts random vector randoms r random raw multinomial r estimator list< multinomial> multinomials estimator multinomials = list< multinomial> estimator collection< multinomial> multinomials training multinomials = list< multinomial> multinomials training i = 1 i < multinomials size i++ multinomial multinomials get i 1 size != multinomial multinomials get i size || multinomial multinomials get i 1 get alphabet != multinomial multinomials get i get alphabet illegal argument all multinomials must have same size and alphabet add multinomial multinomial m xxx that it right and size multinomials add m dirichlet estimate moments estimator estimator dirichlet estimate dims = multinomials get 0 size alphas = dims i = 1 i < multinomials size i++ multinomials get i add probabilities to alphas alpha sum = 0 i = 0 i < alphas length i++ alpha sum += alphas i i = 0 i < alphas length i++ alphas i = alpha sum xxx fix to set sum variance matching unsupported operation not yet dirichlet alphas 