optimize arrays logging logger text decimal format optimize optimizer types matrix ops logger author greg druck author kedar bellare stochastic meta ascent optimizer batches logger logger = logger get logger stochastic meta ascent get name m a x i t e r = 200 l a m b d a = 1 0 t o l e r a n e = 0 01 e p s = 1e 10 mu = 0 1 total iterations = 0 eta init = 0 03 use hessian = gain gradient trace optimizable batch gradient maxable = stochastic meta ascent optimizable batch gradient maxable maxable = maxable set initial step step eta init = step set mu m mu = m set use hessian flag use hessian = flag optimize num batches batch assignments optimize m a x i t e r num batches batch assignments optimize num iterations num batches batch assignments num = maxable get num = num gradient = num hessian product = num only initialize these they are in someone wants to optimize a few iterations at a time gain == err stochastic meta ascent initial step= +eta init+ meta step= +mu gain = num arrays fill gain eta init gradient trace = num maxable get iteration = 0 iteration < num iterations iteration++ old approx value = 0 approx value = 0 batch = 0 batch < num batches batch++ logger info iteration + total iterations + iteration + batch + batch + + num batches get current maxable get update value and gradient current batch initial value = maxable get batch value batch batch assignments old approx value += initial value na n initial value illegal argument na n in value computation probably you need to reduce initial step or meta step maxable get batch value gradient gradient batch batch assignments below was originally written stochastic meta descent we are maximizing so we want ascent flip signs on gradient to make it point downhill matrix ops times equals gradient 1 use hessian compute hessian product maxable batch batch assignments gradient gradient trace hessian product report on vec x report on vec step gain report on vec grad gradient report on vec trace gradient trace update rates individual index = 0 index < num index++ first iteration will just be initial step since gradient trace will be all zeros gain index = math max 0 5 1 mu gradient index gradient trace index adjust based on direction index = gain index gradient index use hessian adjust gradient trace gradient trace index = l a m b d a gradient trace index gain index gradient index + l a m b d a hessian product index adjust gradient trace gradient trace index = l a m b d a gradient trace index gain index gradient index + l a m b d a gradient trace index set maxable set value = maxable get batch value batch batch assignments approx value += value logger info stochastic meta ascent initial value +initial value+ value +final value logger info stochastic meta descent value at iteration + total iterations + iteration + = + approx value converge criteria from gradient ascent and limited memory b f g s 2 0 math abs approx value old approx value <= t o l e r a n e math abs approx value + math abs old approx value + e p s logger info stochastic meta ascent value difference + math abs approx value old approx value + below + tolerance saying converged total iterations += iteration old approx value = approx value total iterations += num iterations report on vec s v decimal format f = decimal format 0 #### out stochastic meta ascent +s+ + min + f format matrix ops min v + max + f format matrix ops max v + mean + f format matrix ops mean v + 2norm + f format matrix ops two norm v + abs norm + f format matrix ops abs norm v compute finite difference approximation hessian product compute hessian product optimizable batch gradient maxable batch index batch assignments current gradient vector result num = maxable get num eps = 1 0e 6 eps gradient = num old = num adjust eps vector and recompute gradient arraycopy 0 old 0 num matrix ops plus equals vector eps maxable set maxable get batch value gradient eps gradient batch index batch assignments restore old maxable set old calculate hessian product index = 0 index < result length index++ result index = eps gradient index current gradient index eps 