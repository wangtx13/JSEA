2011 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e semi supervised pr io serializable list list concurrent callable concurrent execution concurrent executors concurrent future concurrent thread pool executor logging logger r f transducer optimize optimizable gradient value types instance types instance list types matrix ops types sequence logger m step m projection p r author kedar bellare author gregory druck r f optimizable k l serializable gradient value logger logger = logger get logger r f optimizable k l get name serial u = 1 l cached value weights stamp cached gradient weights stamp num num threads weight gaussian prior variance = 1 0 cached value = 123456789 cached gradient list<double > initial prob list prob list list<double > transition prob list instance list training set r f crf r f factors constraints expectations thread pool executor executor p r auxiliary model aux model r f optimizable k l r f crf instance list training set p r auxiliary model aux model cached dots num threads weight crf = crf training set = training set num = crf get get num factors cached gradient = num cached value weights stamp = 1 cached gradient weights stamp = 1 weight > 0 weight = weight gather constraints aux model cached dots num threads = num threads executor = thread pool executor executors fixed thread pool num threads to probabilities weights probs = weights length i = 0 i < weights length i++ probs i = math exp weights i t o d o shouldn t be necessary matrix ops normalize probs probs to probabilities weights i = 0 i < weights length i++ j = 0 j < weights i length j++ k = 0 k < weights i j length k++ weights i j k = math exp weights i j k suppress warnings unchecked gather constraints p r auxiliary model aux model cached dots initial prob list = list<double > prob list = list<double > transition prob list = list<double > constraints = r f factors crf get expectations = r f factors crf get constraints zero ii = 0 ii < training set size ii++ instance inst = training set get ii sequence input = sequence inst get data sum lattice p r ge latt = sum lattice p r crf ii input aux model cached dots ii gammas = ge latt get gammas initial probs = to probabilities gammas 0 initial prob list add initial probs probs = to probabilities gammas gammas length 1 prob list add probs transition probs = ge latt get xis to probabilities transition probs transition prob list add transition probs sum lattice k l crf input initial probs probs transition probs constraints incrementor suppress warnings unchecked get expectation value expectations zero updating tasks list< callable< double>> tasks = list< callable< double>> increment = training set size num threads start = 0 end = increment task index = 0 task index < num threads task index++ same structure but zero values r f factors ex copy = r f factors expectations tasks add expectation task start end ex copy start = end task index == num threads 2 end = training set size end = start + increment value = 0 list< future< double>> results = executor all tasks compute value future< double> f results value += f get execution ee ee print stack trace interrupted ie ie print stack trace combine results callable< double> task tasks expectations plus equals expectation task task get expectations copy 1 value get value crf get weights value change stamp != cached value weights stamp cached value weights stamp = crf get weights value change stamp starting time = current time millis cached value = get expectation value incorporate prior on prior value = crf get gaussian prior gaussian prior variance cached value += prior value logger info gaussian prior = + prior value cached value = weight ! na n cached value || infinite cached value label likelihood na n infinite logger info get value loglikelihood optimizable kl div = + cached value ending time = current time millis logger fine inference milliseconds = + ending time starting time cached value get value gradient buffer cached gradient weights stamp != crf get weights value change stamp cached gradient weights stamp = crf get weights value change stamp get value expectations plus equals constraints 1 0 expectations plus equals gaussian prior gradient crf get gaussian prior variance expectations not na n or infinite expectations get cached gradient matrix ops times equals cached gradient weight arraycopy cached gradient 0 buffer 0 cached gradient length get num num get buffer crf get get buffer get parameter index crf get get parameter index set buff crf get set buff crf weights value changed set parameter index value crf get set parameter index value crf weights value changed set gaussian prior variance value gaussian prior variance = value shutdown executor shutdown expectation task callable< double> start end r f factors expectations copy expectation task start end r f factors ex copy start = start end = end expectations copy = ex copy r f factors get expectations copy expectations copy call value = 0 ii = start ii < end ii++ instance inst = training set get ii sequence input = sequence inst get data init probs = initial prob list get ii probs = prob list get ii trans probs = transition prob list get ii cached dots = input size crf num states crf num states j = 0 j < input size j++ k = 0 k < crf num states k++ l = 0 l < crf num states l++ cached dots j k l = transducer i m p o s s i b l e w e i g h t labeled weight = sum lattice k l crf input init probs probs trans probs cached dots get total weight value += labeled weight unlabeled weight = sum lattice crf input expectations copy incrementor get total weight unlabeled weight = sum lattice cached dot crf input cached dots expectations copy incrementor get total weight value = unlabeled weight value 