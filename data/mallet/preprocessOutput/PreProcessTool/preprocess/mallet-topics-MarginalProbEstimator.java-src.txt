2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics arrays list zip io text number format types randoms an topic model marginal probability estimators presented in wallach et al evaluation topic models i m l 2009 author david mimno marginal prob estimator serializable num topics number topics to be fit these values are used to encode type topic counts count topic pairs in a single topic mask topic bits alpha dirichlet alpha alpha over topics alpha sum beta prior on per topic multinomial over words beta sum smoothing only mass = 0 0 cached coefficients type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> randoms random print word probabilities = marginal prob estimator num topics alpha alpha sum beta type topic counts tokens per topic num topics = num topics bit count num topics == 1 exact power 2 topic mask = num topics 1 topic bits = bit count topic mask otherwise add an extra bit topic mask = highest one bit num topics 2 1 topic bits = bit count topic mask type topic counts = type topic counts tokens per topic = tokens per topic alpha sum = alpha sum alpha = alpha beta = beta beta sum = beta type topic counts length random = randoms cached coefficients = num topics initialize smoothing only sampling bucket smoothing only mass = 0 initialize cached coefficients using only smoothing these values will be selectively replaced in documents non zero counts in particular topics topic=0 topic < num topics topic++ smoothing only mass += alpha topic beta tokens per topic topic + beta sum cached coefficients topic = alpha topic tokens per topic topic + beta sum err topic evaluator + num topics + topics + topic bits + topic bits + to binary topic mask + topic mask set random seed seed seed == 1 random = randoms random = randoms seed set random randoms r random = r get tokens per topic tokens per topic get type topic counts type topic counts set print words should print print word probabilities = should print evaluate left to right instance list testing num particles using resampling print stream doc probability stream log num particles = math log num particles total log likelihood = 0 instance instance testing feature sequence token sequence = feature sequence instance get data doc log likelihood = 0 particle probabilities = num particles particle = 0 particle < num particles particle++ particle probabilities particle = left to right token sequence using resampling position = 0 position < particle probabilities 0 length position++ sum = 0 particle = 0 particle < num particles particle++ sum += particle probabilities particle position sum > 0 0 log prob = math log sum log num particles doc log likelihood += log prob print word probabilities word = instance get data alphabet lookup token sequence get index at position position out printf %s %f word log prob doc probability stream != doc probability stream doc log likelihood total log likelihood += doc log likelihood total log likelihood left to right feature sequence token sequence using resampling one doc topics = token sequence get length word probabilities = token sequence get length current type topic counts type old topic topic topic weights sum doc length = token sequence get length keep track number tokens we ve examined not including out vocabulary words tokens so far = 0 local topic counts = num topics local topic index = num topics build an that densely lists topics that have non zero counts dense index = 0 record total number non zero topics non zero topics = dense index initialize topic count beta sampling bucket topic beta mass = 0 0 topic term mass = 0 0 topic term scores = num topics topic term indices topic term values i score log likelihood = 0 all counts are now zero we are starting completely fresh iterate over positions words in document limit = 0 limit < doc length limit++ record marginal probability token at current limit summed over all topics using resampling iterate up to current limit position = 0 position < limit position++ type = token sequence get index at position position old topic = one doc topics position check out vocabulary words type >= type topic counts length || type topic counts type == current type topic counts = type topic counts type remove token from all counts remove topic s contribution to normalizing constants note that we are using clamped estimates p w|t so we are n o t changing smoothing only mass topic beta mass = beta local topic counts old topic tokens per topic old topic + beta sum decrement local doc topic counts local topic counts old topic maintain dense index we are deleting old topic local topic counts old topic == 0 first get to dense location associated old topic dense index = 0 we know it s in there somewhere so we t need bounds checking local topic index dense index != old topic dense index++ shift all remaining dense indices to left dense index < non zero topics dense index < local topic index length 1 local topic index dense index = local topic index dense index + 1 dense index++ non zero topics add old topic s contribution back into normalizing constants topic beta mass += beta local topic counts old topic tokens per topic old topic + beta sum reset cached coefficient topic cached coefficients old topic = alpha old topic + local topic counts old topic tokens per topic old topic + beta sum now go over type topic counts calculating score each topic index = 0 current topic current value already decremented = topic term mass = 0 0 index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits score = cached coefficients current topic current value topic term mass += score topic term scores index = score index++ sample = random next uniform smoothing only mass + topic beta mass + topic term mass orig sample = sample make sure it actually gets set topic = 1 sample < topic term mass i = 1 sample > 0 i++ sample = topic term scores i topic = current type topic counts i topic mask sample = topic term mass sample < topic beta mass beta topic count++ sample = beta dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index sample = local topic counts topic tokens per topic topic + beta sum sample <= 0 0 topic = topic smoothing only count++ sample = topic beta mass sample = beta topic = 0 sample = alpha topic tokens per topic topic + beta sum sample > 0 0 topic++ sample = alpha topic tokens per topic topic + beta sum topic == 1 err sampling + orig sample + + sample + + smoothing only mass + + topic beta mass + + topic term mass topic = num topics 1 t o d o appropriate illegal state worker runnable topic not sampled topic != 1 put that topic into counts one doc topics position = topic topic beta mass = beta local topic counts topic tokens per topic topic + beta sum local topic counts topic ++ a topic document add topic to dense index local topic counts topic == 1 first find point where we should insert topic going to end which only reason we re keeping track number non zero topics and working backwards dense index = non zero topics dense index > 0 local topic index dense index 1 > topic local topic index dense index = local topic index dense index 1 dense index local topic index dense index = topic non zero topics++ update coefficients non zero topics cached coefficients topic = alpha topic + local topic counts topic tokens per topic topic + beta sum topic beta mass += beta local topic counts topic tokens per topic topic + beta sum we ve just resampled all tokens u p t o current limit now sample token a t current limit type = token sequence get index at position limit check out vocabulary words type >= type topic counts length || type topic counts type == current type topic counts = type topic counts type index = 0 current topic current value topic term mass = 0 0 index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits score = cached coefficients current topic current value topic term mass += score topic term scores index = score out + current topic + = + current value index++ debugging to make sure we re getting right probabilities topic = 0 topic < num topics topic++ index = 0 display count = 0 index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits current topic == topic display count = current value index++ out print topic + out print + local topic counts topic + + + alpha topic + + + alpha sum + + + tokens so far + out + display count + + + beta + + + tokens per topic topic + + + beta sum + = + display count + beta tokens per topic topic + beta sum sample = random next uniform smoothing only mass + topic beta mass + topic term mass orig sample = sample note that we ve been absorbing alpha sum + doc length into normalizing constant marginal probability needs term so we stick it back in word probabilities limit += smoothing only mass + topic beta mass + topic term mass alpha sum + tokens so far out normalizer + alpha sum + + + tokens so far tokens so far++ make sure it actually gets set topic = 1 sample < topic term mass i = 1 sample > 0 i++ sample = topic term scores i topic = current type topic counts i topic mask sample = topic term mass sample < topic beta mass beta topic count++ sample = beta dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index sample = local topic counts topic tokens per topic topic + beta sum sample <= 0 0 topic = topic smoothing only count++ sample = topic beta mass sample = beta topic = 0 sample = alpha topic tokens per topic topic + beta sum sample > 0 0 topic++ sample = alpha topic tokens per topic topic + beta sum topic == 1 err sampling + orig sample + + sample + + smoothing only mass + + topic beta mass + + topic term mass topic = num topics 1 t o d o appropriate put that topic into counts one doc topics limit = topic topic beta mass = beta local topic counts topic tokens per topic topic + beta sum local topic counts topic ++ a topic document add topic to dense index local topic counts topic == 1 first find point where we should insert topic going to end which only reason we re keeping track number non zero topics and working backwards dense index = non zero topics dense index > 0 local topic index dense index 1 > topic local topic index dense index = local topic index dense index 1 dense index local topic index dense index = topic non zero topics++ update coefficients non zero topics cached coefficients topic = alpha topic + local topic counts topic tokens per topic topic + beta sum topic beta mass += beta local topic counts topic tokens per topic topic + beta sum out type + + topic + + log likelihood clean up our mess reset coefficients to values only smoothing next doc will update its own non zero topics dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index cached coefficients topic = alpha topic tokens per topic topic + beta sum word probabilities serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write num topics out write topic mask out write topic bits out write alpha out write alpha sum out write beta out write beta sum out write type topic counts out write tokens per topic out write random out write smoothing only mass out write cached coefficients read input stream in i o not found = in read num topics = in read topic mask = in read topic bits = in read alpha = in read alpha sum = in read beta = in read beta sum = in read type topic counts = in read tokens per topic = in read random = randoms in read smoothing only mass = in read cached coefficients = in read marginal prob estimator read f marginal prob estimator estimator = input stream ois = input stream input stream f estimator = marginal prob estimator ois read ois close estimator 