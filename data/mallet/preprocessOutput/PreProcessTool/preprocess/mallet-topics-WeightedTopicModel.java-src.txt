topics logging zip regex io text number format types gnu trove weighted topic model serializable logger logger = logger get logger weighted topic model get name command option input = command option weighted topic model input f i l e n a m e filename from which to read list training instances use stdin + instances must be feature sequence or feature sequence bigrams not feature vector command option weights = command option weighted topic model weights filename f i l e n a m e filename word word weights command option evaluator filename = command option weighted topic model evaluator filename f i l e n a m e a held out likelihood evaluator documents + indicating that no will be written command option state = command option weighted topic model state filename f i l e n a m e filename in which to write gibbs sampling state after at end iterations + indicating that no will be written command option num topics option = command option weighted topic model num topics i n t e g e r 10 number topics to fit command option num epochs option = command option weighted topic model num epochs i n t e g e r 1 number cycles training evaluators and state files will be saved after each epoch command option num iterations option = command option weighted topic model num iterations i n t e g e r 1000 number iterations gibbs sampling p e r e p o h command option random seed option = command option weighted topic model random seed i n t e g e r 0 random seed gibbs sampler 0 which will use clock command option alpha option = command option weighted topic model alpha d e i m a l 50 0 alpha parameter smoothing over topic command option beta option = command option weighted topic model beta d e i m a l 0 01 beta parameter smoothing over topic pattern source word pattern = pattern compile \\ \\d+ \\ pattern target word pattern = pattern compile \\d+ \\d+ \\d\\ + training instances and their topic assignments list< topic assignment> data alphabet input data alphabet alphabet alphabet topics label alphabet topic alphabet number topics requested num topics size vocabulary num types prior alpha dirichlet alpha alpha over topics alpha sum beta beta sum an to put topic counts current document initialized locally below defined here to avoid garbage collection overhead one doc topic counts indexed <document index topic index> statistics needed sampling type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> weights on type type interactions t hash map type type weights log type topic weights type topic weights total topic weights show topics interval = 50 words per topic = 10 randoms random number format formatter print log likelihood = log count ratio cache weighted topic model number topics alpha sum beta randoms random data = list< topic assignment> topic alphabet = alphabet factory label alphabet size number topics num topics = topic alphabet size alpha sum = alpha sum alpha = alpha sum num topics beta = beta random = random one doc topic counts = num topics tokens per topic = num topics formatter = number format get instance formatter set maximum fraction digits 5 logger info weighted l d a + num topics + topics alphabet get alphabet alphabet label alphabet get topic alphabet topic alphabet get num topics num topics list< topic assignment> get data data set topic display interval n show topics interval = interval words per topic = n set random seed seed random = randoms seed get type topic counts type topic counts get topic totals tokens per topic add instances instance list training alphabet = training get data alphabet num types = alphabet size beta sum = beta num types type topic counts = num types num topics type topic weights = num types num topics total topic weights = num topics type = 0 type < num types type++ arrays fill type topic weights type beta arrays fill total topic weights beta sum doc = 0 instance instance training doc++ feature sequence token sequence = feature sequence instance get data label sequence topic sequence = label sequence topic alphabet token sequence size topic assignment t = topic assignment instance topic sequence data add t read type type weights weights type type weights = t hash map num types logger info num types + num types type = 0 type < num types type++ type type weights type = t hash map type type weights type put type 1 0 source type = 0 complains we t initialize source word valid = buffered reader reader = buffered reader reader weights line line = reader read line != fields = line split sum = 0 0 i=1 i < fields length i += 2 sum += parse fields i source type = alphabet lookup index fields 0 type type weights source type put source type parse fields 1 sum i = 2 i < fields length target type = alphabet lookup index fields i type type weights source type put target type parse fields i+1 sum i += 2 sample iterations should initialize doc cycle count i o iteration = 1 iteration <= iterations iteration++ iteration start = current time millis loop over every document in corpus doc = 0 doc < data size doc++ doc = 0 doc < 5000 doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence run sampler in initialization mode first iteration and show debugging info first document sample topics one doc token sequence topic sequence should initialize iteration == 1 i = 1 i < doc cycle count i++ sample topics one doc token sequence topic sequence doc+1 % 1000 == 0 out doc + 1 elapsed millis = current time millis iteration start logger info iteration + + elapsed millis + ms occasionally print more show topics interval != 0 iteration % show topics interval == 0 logger info < + iteration + > + top words words per topic sample topics one doc feature sequence token sequence feature sequence topic sequence initializing debugging one doc topics = topic sequence get features current type topic counts current type topic weights type old topic topic topic weights sum doc length = token sequence get length local topic counts = num topics ! initializing populate topic counts position = 0 position < doc length position++ local topic counts one doc topics position ++ score sum topic term scores = num topics iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = one doc topics position t hash map type factors = type type weights type connected types = type factors keys grab relevant row from our two dimensional current type topic counts = type topic counts type current type topic weights = type topic weights type ! initializing remove token from all counts local topic counts old topic tokens per topic old topic tokens per topic old topic >= 0 current type topic counts old topic type count other type count type count = current type topic counts old topic already incremented other type connected types factor = type factors get other type type topic weights other type old topic = factor total topic weights old topic = factor now calculate and add up scores each topic word sum = 0 0 here s where math happens! note that overall performance dominated what you in loop topic = 0 topic < num topics topic++ score = alpha + local topic counts topic current type topic weights topic total topic weights topic sum += score topic term scores topic = score debugging type == 68 out type + + topic + + local topic counts topic + + current type topic counts topic + + current type topic weights topic + + tokens per topic topic + + sum choose a random point between 0 and sum all topic scores sample = random next uniform sum debugging out sample + sample + + sum figure out which topic contains that point topic = 1 sample > 0 0 topic++ sample = topic term scores topic make sure we actually sampled a topic debugging || topic == 1 out alphabet lookup type topic = 0 topic < num topics topic++ out + alpha + + + local topic counts topic + + + current type topic weights topic + + total topic weights topic + = + topic term scores topic illegal state weighted topic model topic not sampled put that topic into counts one doc topics position = topic local topic counts topic ++ tokens per topic topic ++ current type topic counts topic ++ out topic + + alphabet lookup type type count other type count type count = current type topic counts topic already incremented other type connected types factor = type factors get other type type topic weights other type topic += factor total topic weights topic += factor model log likelihood log likelihood = 0 0 non zero topics likelihood model a combination a dirichlet multinomial words in each topic and a dirichlet multinomial topics in each document likelihood function a dirichlet multinomial gamma sum i alpha i prod i gamma alpha i + n i prod i gamma alpha i gamma sum i alpha i + n i so log likelihood log gamma sum i alpha i log gamma sum i alpha i + n i + sum i log gamma alpha i + n i log gamma alpha i documents first topic counts = num topics topic log gammas = num topics doc topics topic=0 topic < num topics topic++ topic log gammas topic = dirichlet log gamma alpha doc=0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence doc topics = topic sequence get features token=0 token < doc topics length token++ topic counts doc topics token ++ topic=0 topic < num topics topic++ topic counts topic > 0 log likelihood += dirichlet log gamma alpha + topic counts topic topic log gammas topic subtract count + parameter sum term log likelihood = dirichlet log gamma alpha sum + doc topics length arrays fill topic counts 0 add parameter sum term log likelihood += data size dirichlet log gamma alpha sum and topics count number type topic pairs non zero type topics = 0 type=0 type < num types type++ reuse a pointer topic counts = type topic counts type topic = 0 topic < num topics topic++ topic counts topic == 0 non zero type topics++ log likelihood += dirichlet log gamma beta + topic counts topic na n log likelihood out topic counts topic exit 1 topic=0 topic < num topics topic++ log likelihood = dirichlet log gamma beta num topics + tokens per topic topic na n log likelihood out after topic + topic + + tokens per topic topic exit 1 log likelihood += dirichlet log gamma beta num topics dirichlet log gamma beta non zero type topics na n log likelihood out at end exit 1 log likelihood displaying and saving results top words num words builder output = builder sorter sorted words = sorter num types topic = 0 topic < num topics topic++ type = 0 type < num types type++ sorted words type = sorter type type topic counts type topic arrays sort sorted words output append topic + + tokens per topic topic + + formatter format total topic weights topic i=0 i < num words i++ output append alphabet lookup sorted words i get + output append output to marginal prob estimator get estimator type topic counts are dense meaning that index element determines its topic marginal estimator uses sparse bit encoded arrays used in parallel topic model so we need to convert to that format topic mask topic bits bit count num topics == 1 exact power 2 topic mask = num topics 1 topic bits = bit count topic mask otherwise add an extra bit topic mask = highest one bit num topics 2 1 topic bits = bit count topic mask sparse type topic counts = num types type = 0 type < num types type++ current type topic counts = type topic counts type first figure out how many entries we have num non zeros = 0 topic = 0 topic < num topics topic++ current type topic counts topic > 0 num non zeros ++ allocate sparse sparse counts = num non zeros and fill it keeping in descending order topic = 0 topic < num topics topic++ current type topic counts topic > 0 value = current type topic counts topic << topic bits + topic i = 0 move values along note that arrays are all zeros at initialization sparse counts i > value i++ we ve now found where to insert push along any other values i < sparse counts length value > sparse counts i temp = sparse counts i sparse counts i = value value = temp i++ now add it to arrays sparse type topic counts type = sparse counts alphas = num topics arrays fill alphas alpha marginal prob estimator num topics alphas alpha sum beta sparse type topic counts tokens per topic print state f i o print stream out = print stream g z i p output stream buffered output stream output stream f print state out out close print state print stream stream stream #doc source pos typeindex type topic doc = 0 doc < data size doc++ doc = 0 doc < 5000 doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence source = n a builder out = builder position = 0 position < topic sequence get length position++ type = token sequence get index at position position topic = topic sequence get index at position position out append doc out append out append source out append out append position out append out append type out append out append alphabet lookup type out append out append topic out append stream print out to command option set summary weighted topic model train topics weights between word types encoded in prior command option process weighted topic model instance list training = instance list load input value randoms random = random seed option value != 0 random = randoms random seed option value random = randoms weighted topic model lda = weighted topic model num topics option value alpha option value beta option value random lda add instances training lda read type type weights weights value doc cycle count = 1 epoch = 1 epoch <= num epochs option value epoch++ lda sample num iterations option value epoch == 1 doc cycle count state was invoked lda print state state value + + epoch evaluator filename was invoked output stream oos = output stream output stream evaluator filename value + + epoch oos write lda get estimator oos close e e print stack trace 