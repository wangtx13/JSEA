2009 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e semi supervised transducer types feature vector sequence maths runs subsequence constrained forward backward to compute entropy label sequences <p> reference gideon mann andrew mc callum efficient computation entropy gradient semi supervised conditional random fields h l t n a a l 2007 author gideon mann author gaurav chandalia author gregory druck entropy lattice input sequence size + 1 lattice length input sequence size input length model transducer transducer number states in lattice or model s finite state num states ip input position each node has a forward and backward factor used in forward backward algorithm indexed ip state ip state index si lattice node nodes subsequence constrained forward entropy entropy runs constrained forward backward <p> <tt>incrementor< tt> then not update expectations due to these computations <p> contribution entropy to expectations multiplies scaling factor entropy lattice feature vector sequence fvs gammas xis transducer transducer transducer incrementor incrementor scaling factor input length = fvs size lattice length = input length + 1 transducer = transducer num states = transducer num states nodes = lattice node lattice length num states run forward backward and compute entropy entropy = forward lattice gammas xis backward entropy = backward lattice gammas xis maths almost equals entropy backward entropy entropy + + backward entropy incrementor != add entropy to expectations update counts fvs gammas xis scaling factor incrementor get entropy entropy computes forward entropies h^alpha forward lattice gammas xis initialize entropy start states to 0 a = 0 a < num states ++a get lattice node 0 a alpha = 0 ip = 1 ip < lattice length ++ip a = 0 a < num states ++a position ip 1 in input sequence state a lattice node node = get lattice node ip a gamma = gammas ip a gamma > transducer i m p o s s i b l e w e i g h t b = 0 b < num states ++b position ip in input sequence state a coming from state b xi = xis ip 1 b a xi > transducer i m p o s s i b l e w e i g h t p y ip 1 =b|y ip =a cond prob = math exp xi math exp gamma node alpha += cond prob xi gamma + get lattice node ip 1 b alpha entropy = 0 0 a = 0 a < num states ++a gamma = gammas input length a gamma prob = math exp gamma gamma > transducer i m p o s s i b l e w e i g h t entropy += gamma prob gamma entropy += gamma prob get lattice node input length a alpha entropy computes backward entropies h^beta backward lattice gammas xis initialize entropy end states to 0 a = 0 a < num states ++a get lattice node input length a beta = 0 ip = input length ip >= 0 ip a = 0 a < num states ++a position ip 1 in input sequence state a lattice node node = get lattice node ip a gamma = gammas ip a gamma > transducer i m p o s s i b l e w e i g h t b = 0 b < num states ++b position ip in input sequence state a xi = xis ip a b xi > transducer i m p o s s i b l e w e i g h t p y ip =b|y ip 1 =a cond prob = math exp xi math exp gamma node beta += cond prob xi gamma + get lattice node ip+1 b beta entropy = 0 0 a = 0 a < num states ++a gamma = gammas 0 a gamma prob = math exp gamma gamma > transducer i m p o s s i b l e w e i g h t entropy += gamma prob gamma entropy += gamma prob get lattice node 0 a beta entropy updates expectations due to entropy <p> update counts feature vector sequence fvs gammas xis scaling factor transducer incrementor incrementor ip = 0 ip < input length ++ip a = 0 a < num states ++a nodes ip a == transducer state source state = transducer get state a transducer transition iterator iter = source state transition iterator fvs ip ip iter has next b = iter next get index xi = xis ip a b xi == transducer i m p o s s i b l e w e i g h t xi prob = math exp xi obtained after substituting and re arranging equation at end third page paper into equation d d theta h y|x at end second page \sum y i y i+1 f k y i y i+1 x p y i y i+1 log p y i y i+1 + h^a y 1 i 1 y i + h^b y i+2 t |y i+1 constr entropy = xi prob xi + nodes ip a alpha + nodes ip+1 b beta constr entropy <= 0 negative entropy should be negative! + constr entropy full covariance note it could be positive or negative cov contribution = constr entropy xi prob entropy ! na n cov contribution xi + xi + nodes + ip + + a + alpha + nodes ip a alpha + nodes + ip+1 + + b + beta + nodes ip+1 b beta incrementor increment transition iter cov contribution scaling factor lattice node get lattice node ip si nodes ip si == nodes ip si = lattice node ip transducer get state si nodes ip si contains alpha beta values at a particular input position and state pair lattice node ip transducer state state alpha beta lattice node ip transducer state state ip = ip state = state alpha = 0 0 beta = 0 0 