2004 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e author andrew mc callum <a href= mailto >mccallum edu< a> m e m m might have been simply a max ent classifier at each state but i chose not to that so that tied features could be used in different parts f s m just in r f so expectation gathering done in m e m m style without forward backward just local normalized distributions over destination states from source states but there a global maximizeble m e m m and all m e m ms are set together a single optimization io serializable text decimal format types alphabet types feature vector types feature vector sequence types instance list types sequence pipe pipe a maximum entropy markov model suppress warnings serial m e m m r f serializable logger logger = logger get logger m e m m get name m e m m pipe input pipe pipe output pipe input pipe output pipe m e m m alphabet input alphabet alphabet output alphabet input alphabet output alphabet m e m m r f crf crf r f state state name index initial weight weight destination names label names weight names r f crf state name index initial weight weight destination names label names weight names crf state r f state serializable instance list training set state name index initial cost cost destination names label names weight names r f crf name index initial cost cost destination names label names weight names crf necessary because r f4 will r f4 transition iterator transducer transition iterator transition iterator sequence input sequence input position sequence output sequence output position input position < 0 || output position < 0 unsupported operation epsilon transitions not input sequence == unsupported operation r fs are not generative models must have an input sequence transition iterator feature vector sequence input sequence input position output sequence == ? output sequence get output position crf transition iterator r f transition iterator serializable sum transition iterator state source feature vector sequence input seq input position output r f memm source input seq input position output memm normalize costs transition iterator state source feature vector fv output r f memm source fv output memm normalize costs normalize costs normalize next state costs so they are log probabilities heart difference between locally normalized m e m m and globally normalized r f sum = transducer i m p o s s i b l e w e i g h t i = 0 i < weights length i++ sum = sum log prob sum weights i ! na n sum ! infinite sum i = 0 i < weights length i++ weights i = sum describe transition cutoff decimal format f = decimal format 0 ### describe transition cutoff + log z = +f format sum + 