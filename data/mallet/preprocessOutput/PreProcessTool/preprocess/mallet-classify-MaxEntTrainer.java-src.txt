2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e classify logging io classify classifier optimize conjugate gradient optimize invalid optimizable optimize limited memory b f g s optimize optimizable optimize optimization optimize optimizer optimize orthant wise limited memory b f g s optimize tests pipe pipe types alphabet types exp gain types feature inducer types feature selection types feature vector types gradient gain types info gain types instance types instance list types label types label alphabet types label vector types labeling types matrix ops types ranked feature vector types vector command option logger progress message logger maths does not currently handle instances that are labeled distributions instead a single label trainer a maximum entropy classifier author andrew mc callum <a href= mailto >mccallum edu< a> max ent trainer classifier trainer< max ent> classifier trainer optimization< max ent> boostable serializable logger logger = logger get logger max ent trainer get name logger progress logger = progress message logger get logger max ent trainer get name + pl num iterations = m a x v a l u e e x p g a i n = exp g r a d i e n t g a i n = grad i n f o r m a t i o n g a i n = info xxx why does test maximizable fail when variance very small? d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 d e f a u l t l1 w e i g h t = 0 0 d e f a u l t m a x i m i z e r l a s s = limited memory b f g s gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e l1 weight = d e f a u l t l1 w e i g h t maximizer = d e f a u l t m a x i m i z e r l a s s instance list training set = max ent initial classifier max ent optimizable label likelihood optimizable = optimizer optimizer = o n s t r u t o r s max ent trainer construct a max ent trainer using a trained classifier initial values max ent trainer max ent classifier to train initial classifier = classifier to train constructs a trainer a parameter to avoid overtraining 1 0 value max ent trainer gaussian prior variance gaussian prior variance = gaussian prior variance l a s s i f i e r o b j e t stores max ent get classifier optimizable != optimizable get classifier initial classifier initialize using classifier set classifier max ent classifier to train necessary? what caller about to set training set to something different? akm training set == || alphabet alphabets match classifier to train training set initial classifier != classifier to train initial classifier = classifier to train optimizable = optimizer = o p t i m i z a b l e o b j e t value and gradient functions optimizable get optimizable optimizable max ent optimizable label likelihood get optimizable instance list training set get optimizable training set get classifier max ent optimizable label likelihood get optimizable instance list training set max ent initial classifier training set != training set || initial classifier != initial classifier training set = training set initial classifier = initial classifier optimizable == || optimizable training list != training set optimizable = max ent optimizable label likelihood training set initial classifier l1 weight == 0 0 optimizable set gaussian prior variance gaussian prior variance prior term l1 regularized classifiers optimizer so t include a prior calculation in value and gradient functions optimizable use no prior optimizer = optimizable o p t i m i z e r o b j e t maximizes value function optimizer get optimizer optimizer == optimizable != optimizer = conjugate gradient optimizable optimizer called train entry point optimizable and optimizer compontents optimizer get optimizer instance list training set data not set or has changed initialize optimizable and replace optimizer training set != training set || optimizable == get optimizable training set optimizer = build a optimizer optimizer == l1 weight 0 devolves to standard l b f g s but may be faster optimizer = limited memory b f g s optimizable orthant wise limited memory b f g s optimizable l1 weight optimizer specifies maximum number iterations to run during a single call to <code>train< code> or <code>train feature induction< code> not currently functional trainer x x x since we maximize before using num iterations doesn t work that a bug? so should num iterations be higher? max ent trainer set num iterations i num iterations = i get iteration optimizable == 0 m a x v a l u e optimizer get iteration sets a parameter to prevent overtraining a smaller variance prior means that feature weights are expected to hover closer to 0 so extra evidence required to set a higher weight trainer max ent trainer set gaussian prior variance gaussian prior variance gaussian prior variance = gaussian prior variance use an l1 prior larger values mean will be closer to 0 note that setting any gaussian prior max ent trainer set l1 weight l1 weight l1 weight = l1 weight max ent train instance list training set train training set num iterations max ent train instance list training set num iterations logger fine training set size = +training set size get optimizer training set will set optimizer optimizable i = 0 i < num iterations i++ finished training = optimizer optimize 1 invalid optimizable e e print stack trace logger warning catching invalid optimizatin exception! saying converged finished training = optimization e e print stack trace logger info catching optimization saying converged finished training = finished training only any number iterations allowed num iterations == m a x v a l u e run it again because in our and sam roweis experience b f g s can still eke out more likelihood after first convergence re running without being restricted its gradient history optimizer = get optimizer training set finished training = optimizer optimize invalid optimizable e e print stack trace logger warning catching invalid optimizatin exception! saying converged finished training = optimization e e print stack trace logger info catching optimization saying converged finished training = test maximizable test value and gradient current mt progress logger info progress messages are on one line move on logger info max ent nget value calls +get value calls + max ent nget value gradient calls +get value gradient calls optimizable get classifier <p> trains a maximum entropy model using feature selection and feature induction adding conjunctions features features < p> training data a list <code> instance< code>s whose <code>data< code> fields are binary augmentable <code> feature vector< code>s and whose <code>target< code> fields are <code> label< code>s validation data not currently used <code>training data< code> or <code>null< code> testing data <code>training data< code> or <code>null< code> evaluator evaluator to track training progress and decide whether to or <code>null< code> total iterations maximum total number training iterations including those taken during feature induction num iterations between feature inductions how many iterations to train between one round feature induction and next should usually be fairly small like 5 or 10 to avoid overfitting current features num feature inductions how many rounds feature induction to run before beginning normal training num features per feature induction maximum number features to choose during each round feature induction trained <code> max ent< code> classifier added cjmaloof linc cis upenn classifier train feature induction instance list training data total iterations num iterations between feature inductions num feature inductions num features per feature induction train feature induction training data total iterations num iterations between feature inductions num feature inductions num features per feature induction e x p g a i n <p> like other <code>train feature induction< code> but allows some options to be changed < p> maxent an initial partially trained classifier <code>null< code> classifier may be modified during training gain name estimate gain log likelihood increase we want our chosen features to maximize should be one <code> max ent trainer e x p g a i n< code> <code> max ent trainer g r a d i e n t g a i n< code> or <code> max ent trainer i n f o r m a t i o n g a i n< code> <code> e x p g a i n< code> trained <code> max ent< code> classifier temporarily removed until i figure out how to handle induce features test data classifier train feature induction instance list training data total iterations num iterations between feature inductions num feature inductions num features per feature induction gain name x x x ought to be a parameter except that setting it to can crash training jump too small save during f i = alphabet input alphabet = training data get data alphabet alphabet output alphabet = training data get target alphabet training iteration = 0 num labels = output alphabet size max ent maxent = get classifier initialize feature selection feature selection global f s = training data get feature selection global f s == mask out all features some will be added later feature inducer induce features global f s = feature selection training data get data alphabet training data set feature selection global f s validation data != validation data set feature selection global f s testing data != testing data set feature selection global f s get optimizer training data will initialize me so get classifier below works maxent set feature selection global f s run feature induction feature induction iteration = 0 feature induction iteration < num feature inductions feature induction iteration++ print out some feature logger info feature induction iteration +feature induction iteration train model a little bit we t care whether it converges we execute all feature induction iterations no matter what feature induction iteration != 0 t train until we have added some features set num iterations num iterations between feature inductions train training data training iteration += num iterations between feature inductions logger info starting feature induction + 1+input alphabet size + features over +num labels+ labels create list tokens instance list instances = instance list training data get data alphabet training data get target alphabet instances feature selection will get examined feature inducer so it can know how to add singleton features instances set feature selection global f s list label vectors = list these are length 1 vectors i = 0 i < training data size i++ instance instance = training data get i feature vector input vector = feature vector instance get data label label = label instance get target having trained using just current features how we classify training data now classification classification = maxent classify instance !classification best label correct instances add input vector label label vectors add classification get label vector logger info instance list size = +error instances size s = label vectors size label vector lvs = label vector s i = 0 i < s i++ lvs i = label vector label vectors get i ranked feature vector factory gain factory = gain name equals e x p g a i n gain factory = exp gain factory lvs gaussian prior variance gain name equals g r a d i e n t g a i n gain factory = gradient gain factory lvs gain name equals i n f o r m a t i o n g a i n gain factory = info gain factory illegal argument unsupported gain name +gain name feature inducer klfi = feature inducer gain factory instances num features per feature induction 2 num features per feature induction 2 num features per feature induction note that adds features globally but not on a per transition basis klfi induce features training data testing data != klfi induce features testing data logger info max ent feature selection now includes +global f s cardinality + features klfi = = 1+input alphabet size output alphabet size x x x executing block often causes an during training i t know why save during f i keep current parameter values x x x relies on detail that most recent features added to an alphabet get highest indices count per output label old count = maxent length output alphabet size count = 1+input alphabet size copy params into proper locations i=0 i<output alphabet size i++ arraycopy maxent i old count i count old count i=0 i<old count i++ maxent i != i out maxent i + +new i exit 0 maxent = maxent feature index = input alphabet size finished feature induction logger info ended +global f s cardinality + features set num iterations total iterations training iteration train training data maxent to builder builder = builder builder append max ent trainer num iterations < m a x v a l u e builder append num iterations= + num iterations l1 weight != 0 0 builder append l1 weight= + l1 weight builder append gaussian prior variance= + gaussian prior variance builder to 