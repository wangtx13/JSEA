2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e gain obtained adding a feature to an exponential model from della pietra della pietra lafferty 1997 what right way to smooth p and q so we t get zeros and therefore zeros in alpha and na n in klgain ? i think it would be to put prior over into g q \alpha g right now i m simply doing a little m estimate smoothing p and q note that we use math log not log base 2 so units are not bits author andrew mc callum <a href= mailto >mccallum edu< a> types logging classify classification logger k l gain ranked feature vector logger logger = logger get logger k l gain get name k l gain a feature f defined in max ent type feature+class feature s f f = f k l gain a feature f g f = k l p~ ||q k l p~ ||q f where p~ empirical according to label and q from imperfect classifier and q f from imperfect classifier f added and f s weight adjusted but none other weights adjusted k l gain a feature f g f = sum g f calc k l gains instance list ilist label vector classifications num instances = ilist size num classes = ilist get target alphabet size num features = ilist get data alphabet size ilist size > 0 notation from della pietra lafferty 1997 p 4 p~ p = num classes num features q q = num classes num features alpha weight feature alphas = num classes num features flv feature location value fli feature location index logger info starting klgains #instances= +num instances label weight sum = 0 model label weight sum = 0 actually pretty lame smoothing based on ghost counts doing smoothing = num in expectation = doing smoothing ? num instances+1 0 num instances attempt some add hoc smoothing remove +1 0 in line above not doing smoothing doing smoothing i = 0 i < num classes i++ j = 0 j < num features j++ p i j = q i j = 1 0 num in expectation num features num classes label weight sum += p i j model label weight sum += q i j i = 0 i < num instances i++ classifications i get label alphabet == ilist get target alphabet instance inst = ilist get i labeling labeling = inst get labeling feature vector fv = feature vector inst get data instance weight = ilist get instance weight i below relies on label weights summing to 1 over all labels! = 0 < num classes li++ label weight = labeling value num in expectation model label weight = classifications i value num in expectation label weight sum += label weight model label weight sum += model label weight i < 500 out i= +i+ li= +li+ true= +true label weight+ model= +model label weight label weight == 0 model label weight == 0 fl = 0 fl < fv num locations fl++ fli = fv index at location fl fv value at location fl == 1 0 p fli += label weight instance weight num instances+1 q fli += model label weight instance weight num instances+1 p fli += label weight q fli += model label weight math abs label weight sum 1 0 < 0 001 label weight sum should be 1 0 it was +true label weight sum math abs model label weight sum 1 0 < 0 001 model label weight sum should be 1 0 it was +model label weight sum psum = 0 qsum = 0 i = 0 i < num classes i++ j = 0 j < num features j++ psum += p i j qsum += q i j math abs psum 1 0 < 0 0001 psum not 1 0! psum= +psum+ qsum= +qsum math abs qsum 1 0 < 0 0001 qsum not 1 0! psum= +psum+ qsum= +qsum i = 0 i < num classes i++ j = 0 j < num features j++ alphas i j = math log p i j 1 0 q i j q i j 1 0 p i j q = q e^ \alpha g p 4 out calculating qeag qeag = num classes num features model label weight sum = 0 i = 0 i < ilist size i++ classifications i get label alphabet == ilist get target alphabet instance inst = ilist get i labeling labeling = inst get labeling feature vector fv = feature vector inst get data fv max location = fv num locations 1 = 0 < num classes li++ q \omega = classifications i value num instances model label weight = classifications i value num instances model label weight sum += model label weight following line now done before outside loop over instances fi = 0 fi < num features fi++ qeag fi += model label weight 1 0 fl = 0 fl < fv num locations fl++ fli = fv index at location fl qeag fli += model label weight math exp alphas fli model label weight = 0 < num classes li++ fi = 0 fi < num features fi++ assume that feature fi does not occur in fv and thus has value 0 exp alpha 0 == 1 0 factoring possible because all features have value 1 0 qeag fi += model label weight sum 1 0 out calculating klgain values klgains = num features i = 0 i < num classes i++ j = 0 j < num features j++ alphas i j > 0 ! infinite alphas i j klgains j += alphas i j p i j math log qeag i j klgains j += math abs alphas i j p i j klgains j += math abs alphas i j logger info klgains length= +klgains length j = 0 j < num features j++ j % num features 100 == 0 i = 0 i < num classes i++ logger info c= +i+ p +ilist get data alphabet lookup j + = +p i j logger info c= +i+ q +ilist get data alphabet lookup j + = +q i j logger info c= +i+ alphas +ilist get data alphabet lookup j + = +alphas i j logger info c= +i+ qeag +ilist get data alphabet lookup j + = +qeag i j logger info klgains +ilist get data alphabet lookup j + = +klgains j klgains k l gain instance list ilist label vector classifications ilist get data alphabet calc k l gains ilist classifications label vector get label vectors from classifications classification label vector ret = label vector length i = 0 i < length i++ ret i = i get label vector ret k l gain instance list ilist classification classifications ilist get data alphabet calc k l gains ilist get label vectors from classifications classifications 