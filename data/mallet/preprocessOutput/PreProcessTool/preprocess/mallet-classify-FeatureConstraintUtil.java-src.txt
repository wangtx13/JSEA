2009 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e classify io buffered reader io io reader list arrays collections hash map iterator logging logger topics parallel topic model types alphabet types feature vector types info gain types instance types instance list types labeling types matrix ops logger maths utility functions creating feature constraints that can be used g e training author gregory druck <a href= mailto gdruck >gdruck edu< a> feature constraint logger logger = logger get logger feature constraint get name reads range constraints stored using strings from a format can be either feature name label name lower probability upper probability + or feature name label name probability + constraints are only added feature label pairs that are present filename feature constraints data instance list used alphabets constraints hash map< > read range constraints from filename instance list data hash map< > constraints = hash map< > = 0 < data get target alphabet size li++ err data get target alphabet lookup buffered reader reader = buffered reader reader filename line = reader read line line != split = line split \\s+ assume feature name has no spaces feature name = split 0 feature index = data get data alphabet lookup index feature name feature index == 1 runtime feature + feature name + not found in alphabet! probs = data get target alphabet size 2 i = 0 i < probs length i++ arrays fill probs i n e g a t i v e i n f i n i t y index = 1 index < split length index++ label split = split index split = data get target alphabet lookup index label split 0 != 1 label split 0 label split 1 contains range split = label split 1 split lower = parse range split 0 upper = parse range split 1 probs 0 = lower probs 1 = upper prob = parse label split 1 probs 0 = prob probs 1 = prob constraints put feature index probs line = reader read line e e print stack trace exit 1 constraints reads feature constraints from a whether they are stored using strings or indices filename feature constraints data instance list used alphabets constraints hash map< > read constraints from filename instance list data test constraints index based filename read constraints from index filename data read constraints from filename data reads feature constraints stored using strings from a feature name label name probability + labels that appear get probability 0 filename feature constraints data instance list used alphabets constraints hash map< > read constraints from filename instance list data hash map< > constraints = hash map< > = filename buffered reader reader = buffered reader reader line = reader read line line != split = line split \\s+ assume feature name has no spaces feature name = split 0 feature index = data get data alphabet lookup index feature name split length 1 == data get target alphabet size split length + + data get target alphabet size probs = split length 1 index = 1 index < split length index++ label split = split index split = data get target alphabet lookup index label split 0 != 1 label + label split 0 + not found prob = parse label split 1 probs = prob constraints put feature index probs line = reader read line e e print stack trace exit 1 constraints reads feature constraints stored using strings from a feature index label 0 prob label 1 prob label n prob here each label must appear filename feature constraints data instance list used alphabets constraints hash map< > read constraints from index filename instance list data hash map< > constraints = hash map< > = filename buffered reader reader = buffered reader reader line = reader read line line != split = line split \\s+ feature index = parse split 0 split length 1 == data get target alphabet size probs = split length 1 index = 1 index < split length index++ prob = parse split index probs index 1 = prob constraints put feature index probs line = reader read line e e print stack trace exit 1 constraints test constraints index based filename = filename first line = buffered reader reader = buffered reader reader first line = reader read line e e print stack trace exit 1 !first line contains select features highest gain list instance list computing gain num features number features to select list features highest gains list< integer> select features info gain instance list list num features list< integer> features = list< integer> info gain infogain = info gain list rank = 0 rank < num features rank++ features add infogain get index at rank rank features select top features in l d a topics num sel features number features to select lda est l d a estimate pr which provides an to an l d a model seq alphabet alphabet sequence dataset which may be different from vector dataset alphabet alphabet vector dataset alphabet list indices selected features list< integer> select top l d a features num sel features parallel topic model lda alphabet alphabet list< integer> features = list< integer> alphabet seq alphabet = lda get alphabet num topics = lda get num topics sorted = lda get top words seq alphabet size pos = 0 pos < seq alphabet size pos++ ti = 0 ti < num topics ti++ feat = sorted ti pos to fi = alphabet lookup index feat fi >=0 !features contains fi logger info selected feature + feat features add fi features size == num sel features features features hash map< > set targets using data instance list list list< integer> features set targets using data list features hash map< > set targets using data instance list list list< integer> features normalize set targets using data list features normalize set target distributions using estimates from data list instance list used to estimate targets features list features constraints normalize whether to normalize feature counts constraints map feature index to target targets set using estimates from supplied data hash map< > set targets using data instance list list list< integer> features use values normalize hash map< > constraints = hash map< > feature label counts = get feature label counts list use values i = 0 i < features size i++ fi = features get i fi != list get data alphabet size prob = feature label counts fi normalize smooth probability distributions adding a very small count we just need to make sure they aren t zero in which k l divergence infinite matrix ops plus equals prob 1e 8 matrix ops times equals prob 1 matrix ops sum prob constraints put fi prob constraints set target distributions using schapire heuristic described in from labeled features using generalized expectation criteria gregory druck gideon mann andrew mc callum labeled features hash map feature indices to lists label indices that feature num labels total number labels majority prob probability mass divided among majority labels constraints map feature index to target target distributions set using heuristic hash map< > set targets using heuristic hash map< list< integer>> labeled features num labels majority prob hash map< > constraints = hash map< > iterator< integer> key iter = labeled features key set iterator key iter has next fi = key iter next list< integer> labels = labeled features get fi constraints put fi get heuristic prior labels num labels majority prob constraints set target distributions using feature voting heuristic described in from labeled features using generalized expectation criteria gregory druck gideon mann andrew mc callum labeled features hash map feature indices to lists label indices that feature training data instance list to use computing expectations feature voting constraints map feature index to target target distributions set using feature voting hash map< > set targets using feature voting hash map< list< integer>> labeled features instance list training data hash map< > constraints = hash map< > num labels = training data get target alphabet size iterator< integer> key iter = labeled features key set iterator feature counts = labeled features size num labels ii = 0 ii < training data size ii++ instance instance = training data get ii feature vector fv = feature vector instance get data labeling labeling = training data get ii get labeling label dist = num labels labeling == label voting labeled features instance label dist = labeling get best index label dist = 1 key iter = labeled features key set iterator i = 0 key iter has next fi = key iter next fv location fi >= 0 = 0 < num labels li++ feature counts i += label dist fv value at location fv location fi i++ key iter = labeled features key set iterator i = 0 key iter has next fi = key iter next smoothing counts matrix ops plus equals feature counts i 1e 8 matrix ops times equals feature counts i 1 matrix ops sum feature counts i constraints put fi feature counts i i++ constraints label features using heuristic described in from labeled features using generalized expectation criteria gregory druck gideon mann andrew mc callum list instance list used to compute statistics labeling features features list features to label reject whether to reject labeling features labeled features hash map mapping feature indices to list labels hash map< list< integer>> label features instance list list list< integer> features reject hash map< list< integer>> labeled features = hash map< list< integer>> feature label counts = get feature label counts list num labels = list get target alphabet size min rank = 100 num labels info gain infogain = info gain list sum = 0 rank = 0 rank < min rank rank++ sum += infogain get value at rank rank mean = sum min rank i = 0 i < features size i++ fi = features get i reject features infogain less than cutoff reject infogain value fi < mean err oracle labeler rejected labeling + list get data alphabet lookup fi logger info oracle labeler rejected labeling + list get data alphabet lookup fi prob = feature label counts fi matrix ops plus equals prob 1e 8 matrix ops times equals prob 1 matrix ops sum prob sorted indices = get max indices prob list< integer> labels = list< integer> num labels > 2 take anything within a factor 2 best but no more than num labels 2 discard = threshold = prob sorted indices 0 2 = 0 < num labels li++ prob > threshold labels add reject labels size > num labels 2 err oracle labeler rejected labeling + list get data alphabet lookup fi logger info oracle labeler rejected labeling + list get data alphabet lookup fi discard = discard labels add sorted indices 0 labeled features put fi labels labeled features hash map< list< integer>> label features instance list list list< integer> features label features list features get feature label counts instance list list use values num features = list get data alphabet size num labels = list get target alphabet size feature label counts = num features num labels ii = 0 ii < list size ii++ instance instance = list get ii feature vector feature vector = feature vector instance get data handles distributions over labels = 0 < num labels li++ py = instance get labeling value loc = 0 loc < feature vector num locations loc++ fi = feature vector index at location loc val use values val = feature vector value at location loc val = 1 0 feature label counts fi += py val feature label counts get heuristic prior list< integer> labeled features num labels majority prob num indices = labeled features size dist = num labels num indices == num labels i = 0 i < dist length i++ dist i = 1 num labels dist keyword prob = majority prob num indices other prob = 1 majority prob num labels num indices i = 0 i < labeled features size i++ = labeled features get i dist = keyword prob = 0 < num labels li++ dist == 0 dist = other prob maths almost equals matrix ops sum dist 1 dist label voting hash map< list< integer>> labeled features instance instance scores feature vector fv = feature vector instance get data num features = instance get data alphabet size + 1 num labels = instance get target alphabet size iterator< integer> key iterator = labeled features key set iterator key iterator has next list< integer> majority list = labeled features get key iterator next i = 0 i < majority list size i++ = majority list get i num labels ++ key iterator = labeled features key set iterator key iterator has next next = key iterator next next < num features loc = fv location next loc < 0 list< integer> majority list = labeled features get next i = 0 i < majority list size i++ = majority list get i scores += 1 sum = matrix ops sum scores sum == 0 matrix ops plus equals scores 1 0 sum = matrix ops sum scores = 0 < scores length li++ scores = sum these functions are no longer needed get pr word topic l d a hyper lda num topics = lda get num topics num types = lda get alphabet size pr word topic = num topics num types ti = 0 ti < num topics ti++ wi = 0 wi < num types wi++ pr word topic ti wi = lda get count feature topic wi ti lda get count tokens per topic ti pr word topic get sorted topic pr topic word num topics = pr topic word length num types = pr topic word 0 length sorted topic idx = num topics num types ti = 0 ti < num topics ti++ topic idx = get max indices pr topic word ti arraycopy topic idx 0 sorted topic idx ti 0 topic idx length sorted topic idx get max indices x list< element> list = list< element> i = 0 i < x length i++ element element = element i x i list add element collections sort list collections reverse list sorted indices = x length i = 0 i < x length i++ sorted indices i = list get i index sorted indices element comparable< element> index value element index value index = index value = value compare to element element compare value element value 