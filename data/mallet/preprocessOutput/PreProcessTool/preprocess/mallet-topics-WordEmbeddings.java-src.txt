topics randoms command option types io concurrent word embeddings command option input = command option word embeddings input f i l e n a m e filename from which to read list training instances use stdin + instances must be feature sequence or feature sequence bigrams not feature vector command option output = command option word embeddings output f i l e n a m e weights txt filename to write text formatted word vectors command option num dimensions = command option word embeddings num dimensions i n t e g e r 50 number dimensions to fit command option window size option = command option word embeddings window size i n t e g e r 5 number adjacent words to consider command option num threads = command option word embeddings num threads i n t e g e r 1 number threads parallel training command option num samples = command option word embeddings num samples i n t e g e r 5 number negative samples to use in training command option example word = command option word embeddings example word s t r i n g defined periodically show closest vectors to word alphabet vocabulary num words num columns weights squared gradient sums stride word counts sampling sampling table sampling table size = 100000000 sampling sum = 0 0f total words = 0 max exp value = 6 0 min exp value = 6 0 sigmoid cache sigmoid cache size = 1000 window size = 5 query word = randoms random = randoms word embeddings word embeddings alphabet a num columns window size vocabulary = a num words = vocabulary size out format vocab size %d num words num columns = num columns stride = 2 num columns weights = num words stride squared gradient sums = num words stride word = 0 word < num words word++ col = 0 col < 2 num columns col++ weights word stride + col = random next 0 5f num columns word counts = num words sampling = num words sampling table = sampling table size window size = window size sigmoid cache = sigmoid cache size + 1 i = 0 i < sigmoid cache size i++ value = i sigmoid cache size max exp value min exp value + min exp value sigmoid cache i = 1 0 1 0 + math exp value count words instance list instances instance instance instances feature sequence tokens = feature sequence instance get data length = tokens get length position = 0 position < length position++ type = tokens get index at position position word counts type ++ total words += length normalizer = 1 0f total words sampling 0 = math pow normalizer word counts 0 0 75 word = 1 word < num words word++ sampling word = sampling word 1 + math pow normalizer word counts word 0 75 sampling sum = sampling num words 1 word = 0 i = 0 i < sampling table size i++ sampling sum i sampling table size > sampling word word++ sampling table i = word out done counting train instance list instances num threads num samples executor service executor = executors fixed thread pool num threads word embedding runnable runnables = word embedding runnable num threads thread = 0 thread < num threads thread++ runnables thread = word embedding runnable instances num samples num threads thread executor submit runnables thread start time = current time millis difference = 0 0 finished = ! finished thread sleep 5000 interrupted e words so far = 0 are all threads done? thread = 0 thread < num threads thread++ words so far += runnables thread words so far out format % 3f runnables thread get mean running millis = current time millis start time out format %d %d %fk w s %f loss %f avg words so far running millis words so far running millis difference 10000 average abs weight difference = 0 0 words so far > 5 total words finished = thread = 0 thread < num threads thread++ runnables thread should run = query word != find closest copy query word executor shutdown now find closest target vector sorter sorted words = sorter num words target squared sum = 0 0 col = 0 col < num columns col++ target squared sum += target vector col target vector col target normalizer = 1 0 math sqrt target squared sum out target squared sum word = 0 word < num words word++ inner product = 0 0 word squared sum = 0 0 col = 0 col < num columns col++ word squared sum += weights word stride + col weights word stride + col word normalizer = 1 0 math sqrt word squared sum col = 0 col < num columns col++ inner product += target normalizer target vector col word normalizer weights word stride + col sorted words word = sorter word inner product arrays sort sorted words i = 0 i < 10 i++ out format %f %d %s sorted words i get weight sorted words i get vocabulary lookup sorted words i get average abs weight sum = 0 0 word = 0 word < num words word++ col = 0 col < num columns col++ sum += math abs weights word stride + col sum num words num columns write print writer out word = 0 word < num words word++ formatter buffer = formatter buffer format %s vocabulary lookup word col = 0 col < num columns col++ buffer format % 6f weights word stride + col out buffer copy word copy vocabulary lookup index word copy word result = num columns col = 0 col < num columns col++ result col = weights word stride + col result add result word add result vocabulary lookup index word add result word col = 0 col < num columns col++ result col += weights word stride + col result subtract result word subtract result vocabulary lookup index word subtract result word col = 0 col < num columns col++ result col = weights word stride + col result process command line options command option set summary word embeddings train continuous word embeddings using skip gram negative sampling command option process word embeddings instance list instances = instance list load input value word embeddings matrix = word embeddings instances get data alphabet num dimensions value window size option value matrix query word = example word value matrix count words instances matrix train instances num threads value num samples value print writer out = print writer output value matrix write out out close 