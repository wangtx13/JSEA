implement different gibbs sampling based inference topics io buffered output stream io io output stream io writer io i o io print stream io print writer list arrays zip g z i p output stream types feature counter types feature sequence types feature vector types sorter types instance types instance list types label alphabet types label sequence randoms gnu trove t hash map author limin yao david mimno l d a stream l d a hyper list< topication> test test instances and their topic assignments number topics l d a stream number topics number topics t o d o auto generated constructor stub number topics alpha sum beta l d a stream number topics alpha sum beta number topics alpha sum beta t o d o auto generated constructor stub number topics alpha sum beta random l d a stream number topics alpha sum beta randoms random number topics alpha sum beta random t o d o auto generated constructor stub topic alphabet alpha sum beta random l d a stream label alphabet topic alphabet alpha sum beta randoms random topic alphabet alpha sum beta random t o d o auto generated constructor stub list< topication> get test test first training a topic model on training data inference on test data count type topic counts re sampling on all data inference all max iteration test = list< topication> initialize test initial sampling on testdata list< label sequence> topic sequences = list< label sequence> instance instance testing label sequence topic sequence = label sequence topic alphabet instance length instance not yet obeying its last argument and must be to work sample topics one doc feature sequence instance get data topic sequence randoms r = randoms feature sequence fs = feature sequence instance get data topics = topic sequence get features i = 0 i < topics length i++ type = fs get index at position i topics i = r next num topics type topic counts type adjust or put value topics i 1 1 tokens per topic topics i ++ topic sequences add topic sequence construct test testing size == topic sequences size i = 0 i < testing size i++ topication t = topication testing get i topic sequences get i test add t start time = current time millis loop iter = 0 iter <= max iteration iter++ iter%100==0 out print iteration + iter out num docs = test size t o d o di = 0 di < num docs di++ feature sequence token sequence = feature sequence test get di instance get data label sequence topic sequence = test get di topic sequence sample topics one test doc all token sequence topic sequence seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total inferencing time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds called inference all using unseen words in testdata sample topics one test doc all feature sequence token sequence label sequence topic sequence t o d o auto generated stub one doc topics = topic sequence get features t hash map current type topic counts type old topic topic tw topic weights = num topics topic weights sum doc length = token sequence get length populate topic counts local topic counts = num topics ti = 0 ti < num topics ti++ local topic counts ti = 0 position = 0 position < doc length position++ local topic counts one doc topics position ++ iterate over positions words in document si = 0 si < doc length si++ type = token sequence get index at position si old topic = one doc topics si remove token from all counts local topic counts old topic current type topic counts = type topic counts type current type topic counts get old topic >= 0 current type topic counts get old topic == 1 current type topic counts remove old topic current type topic counts adjust value old topic 1 tokens per topic old topic build a over topics token arrays fill topic weights 0 0 topic weights sum = 0 ti = 0 ti < num topics ti++ tw = current type topic counts get ti + beta tokens per topic ti + beta sum local topic counts ti + alpha ti doc len 1+t alpha constant across all topics topic weights sum += tw topic weights ti = tw sample a topic assignment from topic = random next discrete topic weights topic weights sum put that topic into counts one doc topics si = topic current type topic counts adjust or put value topic 1 1 local topic counts topic ++ tokens per topic topic ++ what we have type topic counts tokens per topic topic sequence training and test data estimate all iteration i o re gibbs sampling on all data data add all test initialize histograms and cached values estimate iteration inference on testdata one problem how to deal unseen words unseen words in alphabet but type topics count entry added limin yao max iteration inference max iteration test = list< topication> initialize test initial sampling on testdata list< label sequence> topic sequences = list< label sequence> instance instance testing label sequence topic sequence = label sequence topic alphabet instance length instance not yet obeying its last argument and must be to work sample topics one doc feature sequence instance get data topic sequence randoms r = randoms feature sequence fs = feature sequence instance get data topics = topic sequence get features i = 0 i < topics length i++ type = fs get index at position i topics i = r next num topics type topic counts type size != 0 topics i = r next num topics topics i = 1 unseen words topic sequences add topic sequence construct test testing size == topic sequences size i = 0 i < testing size i++ topication t = topication testing get i topic sequences get i test add t include sufficient statistics one doc add count on data to n k w and n k pay attention to unseen words feature sequence token sequence = feature sequence t instance get data label sequence topic sequence = t topic sequence pi = 0 pi < topic sequence get length pi++ topic = topic sequence get index at position pi type = token sequence get index at position pi topic != 1 type seen in training type topic counts type adjust or put value topic 1 1 tokens per topic topic ++ start time = current time millis loop iter = 0 iter <= max iteration iter++ iter%100==0 out print iteration + iter out num docs = test size t o d o di = 0 di < num docs di++ feature sequence token sequence = feature sequence test get di instance get data label sequence topic sequence = test get di topic sequence sample topics one test doc token sequence topic sequence seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total inferencing time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds sample topics one test doc feature sequence token sequence label sequence topic sequence t o d o auto generated stub one doc topics = topic sequence get features t hash map current type topic counts type old topic topic tw topic weights = num topics topic weights sum doc length = token sequence get length populate topic counts local topic counts = num topics ti = 0 ti < num topics ti++ local topic counts ti = 0 position = 0 position < doc length position++ one doc topics position != 1 local topic counts one doc topics position ++ iterate over positions words in document si = 0 si < doc length si++ type = token sequence get index at position si old topic = one doc topics si old topic == 1 remove token from all counts local topic counts old topic current type topic counts = type topic counts type current type topic counts get old topic >= 0 current type topic counts get old topic == 1 current type topic counts remove old topic current type topic counts adjust value old topic 1 tokens per topic old topic build a over topics token arrays fill topic weights 0 0 topic weights sum = 0 ti = 0 ti < num topics ti++ tw = current type topic counts get ti + beta tokens per topic ti + beta sum local topic counts ti + alpha ti doc len 1+t alpha constant across all topics topic weights sum += tw topic weights ti = tw sample a topic assignment from topic = random next discrete topic weights topic weights sum put that topic into counts one doc topics si = topic current type topic counts adjust or put value topic 1 1 local topic counts topic ++ tokens per topic topic ++ inference 3 each doc each iteration each word compare against inference that method2 each iter each doc each word inference one one max iteration test = list< topication> initialize test initial sampling on testdata list< label sequence> topic sequences = list< label sequence> instance instance testing label sequence topic sequence = label sequence topic alphabet instance length instance not yet obeying its last argument and must be to work sample topics one doc feature sequence instance get data topic sequence randoms r = randoms feature sequence fs = feature sequence instance get data topics = topic sequence get features i = 0 i < topics length i++ type = fs get index at position i topics i = r next num topics type topic counts type adjust or put value topics i 1 1 tokens per topic topics i ++ type topic counts type size != 0 topics i = r next num topics type topic counts type adjust or put value topics i 1 1 tokens per topic topics i ++ topics i = 1 unseen words topic sequences add topic sequence construct test testing size == topic sequences size i = 0 i < testing size i++ topication t = topication testing get i topic sequences get i test add t start time = current time millis loop iter = 0 num docs = test size t o d o di = 0 di < num docs di++ iter = 0 feature sequence token sequence = feature sequence test get di instance get data label sequence topic sequence = test get di topic sequence iter <= max iteration iter++ sample topics one test doc token sequence topic sequence di%100==0 out print docnum + di out seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total inferencing time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds inference theta max iteration instance list theta test = list< topication> initialize test initial sampling on testdata list< label sequence> topic sequences = list< label sequence> instance instance testing label sequence topic sequence = label sequence topic alphabet instance length instance not yet obeying its last argument and must be to work sample topics one doc feature sequence instance get data topic sequence randoms r = randoms feature sequence fs = feature sequence instance get data topics = topic sequence get features i = 0 i < topics length i++ type = fs get index at position i topics i = r next num topics topic sequences add topic sequence construct test testing size == topic sequences size i = 0 i < testing size i++ topication t = topication testing get i topic sequences get i test add t include sufficient statistics one doc add count on data to n k w and n k pay attention to unseen words feature sequence token sequence = feature sequence t instance get data label sequence topic sequence = t topic sequence pi = 0 pi < topic sequence get length pi++ topic = topic sequence get index at position pi type = token sequence get index at position pi topic != 1 type seen in training type topic counts type adjust or put value topic 1 1 tokens per topic topic ++ start time = current time millis loop iter = 0 iter <= max iteration iter++ iter%100==0 out print iteration + iter out num docs = test size t o d o di = 0 di < num docs di++ feature vector fv theta = feature vector theta get di get data topic = fv theta get values feature sequence token sequence = feature sequence test get di instance get data label sequence topic sequence = test get di topic sequence sample topics one doc theta token sequence topic sequence topic seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total inferencing time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds sampling known theta from maxent sample topics one doc theta feature sequence token sequence label sequence topic sequence topic t o d o auto generated stub one doc topics = topic sequence get features t hash map current type topic counts type old topic topic tw topic weights = num topics topic weights sum doc length = token sequence get length iterate over positions words in document si = 0 si < doc length si++ type = token sequence get index at position si old topic = one doc topics si old topic == 1 current type topic counts = type topic counts type current type topic counts get old topic >= 0 current type topic counts get old topic == 1 current type topic counts remove old topic current type topic counts adjust value old topic 1 tokens per topic old topic build a over topics token arrays fill topic weights 0 0 topic weights sum = 0 ti = 0 ti < num topics ti++ tw = current type topic counts get ti + beta tokens per topic ti + beta sum topic ti doc len 1+t alpha constant across all topics topic weights sum += tw topic weights ti = tw sample a topic assignment from topic = random next discrete topic weights topic weights sum put that topic into counts one doc topics si = topic current type topic counts adjust or put value topic 1 1 tokens per topic topic ++ print human readable doc topic matrix further i r use print theta list< topication> dataset f threshold max i o print writer pw = print writer writer f topic counts = num topics doc len di = 0 di < dataset size di++ label sequence topic sequence = dataset get di topic sequence current doc topics = topic sequence get features doc len = current doc topics length token=0 token < doc len token++ topic counts current doc topics token ++ pw dataset get di instance get name n t|d +alpha t doc len + alpha sum topic = 0 topic < num topics topic++ prob = topic counts topic +alpha topic doc len + alpha sum pw topic + topic + + prob pw arrays fill topic counts 0 pw close print topic word matrix further i r use print phi f threshold i o print writer pw = print writer writer f feature counter word counts per topic = feature counter num topics ti = 0 ti < num topics ti++ word counts per topic ti = feature counter alphabet fi = 0 fi < num types fi++ topics = type topic counts fi keys i = 0 i < topics length i++ word counts per topic topics i increment fi type topic counts fi get topics i ti = 0 ti < num topics ti++ pw topic + ti feature counter counter = word counts per topic ti feature vector fv = counter to feature vector pos = 0 pos < fv num locations pos++ fi = fv index at location pos word = alphabet lookup fi count = fv value at location pos prob prob = count+beta tokens per topic ti + beta sum pw word + + prob pw pw close print document topics list< topication> dataset f i o print document topics dataset print writer writer f print document topics list< topication> dataset print writer pw print document topics dataset pw 0 0 1 pw a print writer threshold only print topics proportion greater than number max print no more than many topics print document topics list< topication> dataset print writer pw threshold max pw print #doc source topic proportion doc len topic counts = num topics sorter sorted topics = sorter num topics topic = 0 topic < num topics topic++ initialize sorters dummy values sorted topics topic = sorter topic topic max < 0 || max > num topics max = num topics di = 0 di < dataset size di++ label sequence topic sequence = dataset get di topic sequence current doc topics = topic sequence get features pw print di pw print dataset get di instance get source != pw print dataset get di instance get source pw print source pw print doc len = current doc topics length count up tokens real doc len = 0 token=0 token < doc len token++ current doc topics token != 1 topic counts current doc topics token ++ real doc len ++ real doc len == doc len alpha sum=0 0 topic=0 topic < num topics topic++ alpha sum+=alpha topic and normalize and smooth dirichlet prior alpha topic = 0 topic < num topics topic++ sorted topics topic set topic topic counts topic +alpha topic doc len + alpha sum arrays sort sorted topics i = 0 i < max i++ sorted topics i get weight < threshold pw print sorted topics i get + + sorted topics i get weight + pw print arrays fill topic counts 0 pw close print state list< topication> dataset f i o print stream out = print stream g z i p output stream buffered output stream output stream f print state dataset out out close print state list< topication> dataset print stream out out #doc source pos typeindex type topic di = 0 di < dataset size di++ feature sequence token sequence = feature sequence dataset get di instance get data label sequence topic sequence = dataset get di topic sequence source = n a dataset get di instance get source != source = dataset get di instance get source to pi = 0 pi < topic sequence get length pi++ type = token sequence get index at position pi topic = topic sequence get index at position pi out print di out print out print source out print out print pi out print out print type out print out print alphabet lookup type out print out print topic out 