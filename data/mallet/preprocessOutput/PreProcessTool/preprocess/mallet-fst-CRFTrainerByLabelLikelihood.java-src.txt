io i o io input stream io output stream list random logging logger optimize limited memory b f g s optimize optimizer types exp gain types feature inducer types feature selection types feature vector types gradient gain types info gain types instance types instance list types label types label alphabet types label sequence types label vector types ranked feature vector types sequence logger unlike classifier trainer transducer trainer not stateless between calls to train a transducer trainer constructed paired a specific transducer and can only train that transducer r f stores and has feature selection and weight freezing r f trainer stores and has determining contents dimensions sparsity feature induction r f s weights determined training data <p> <b> note < b> in future may go away in favor some r f trainer value gradients r f trainer label likelihood transducer trainer transducer trainer optimization logger logger = logger get logger r f trainer label likelihood get name d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 0 d e f a u l t h y p e r b o l i p r i o r s l o p e = 0 2 d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s = 10 0 r f crf optimizable r f ocrf r f optimizable label likelihood ocrf optimizer opt iteration count = 0 converged using hyperbolic prior = gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e hyperbolic prior slope = d e f a u l t h y p e r b o l i p r i o r s l o p e hyperbolic prior sharpness = d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s use sparse weights = use no weights = t o d o remove it just debugging use some unsupported trick = various values from r f acting indicators when we need to cached value weights stamp = 1 re calculate expectations and values to get value because weights values changed cached gradient weights stamp = 1 re calculate to get value gradient because weights values changed cached weights structure stamp = 1 re allocate crf weights expectations constraints because states transitions use mcrf training set to when we need to re allocate crf weights expectations constraints because we are using a different training list than last time xxx temporary hack quite useful to have though!! cas print gradient = r f trainer label likelihood r f crf crf = crf transducer get transducer crf r f get r f crf optimizer get optimizer opt converged converged finished training converged get iteration iteration count use to specify whether or not factors are added to r f trainer you have already setup factors in your r f you may not want trainer to add additional factors flag trainer adds no factors to r f set add no factors flag use no weights = flag r f optimizable label likelihood get optimizable r f instance list training set cached weights structure stamp != crf weights structure change stamp !use no weights use sparse weights crf set weights dimension in training set use some unsupported trick crf set weights dimension densely reallocate sufficient statistics not necessary here because it done in constructor optimizable r f ocrf = cached weights structure stamp = crf weights structure change stamp ocrf == || ocrf training set != training set ocrf = optimizable r f crf training set ocrf = r f optimizable label likelihood crf training set ocrf set gaussian prior variance gaussian prior variance ocrf set hyperbolic prior sharpness hyperbolic prior sharpness ocrf set hyperbolic prior slope hyperbolic prior slope ocrf set use hyperbolic prior using hyperbolic prior opt = ocrf optimizer get optimizer instance list training set get optimizable r f training set will set mcrf necessary opt == || ocrf != opt get optimizable opt = limited memory b f g s ocrf alternative opt = conjugate gradient 0 001 opt question i make a non inner r f trainer can that subclassed in another and can that subclass still have access to all r f s instance variables? a n s w e r yes and yes but you have to use special syntax in subclass ctor dev archive cas train incremental instance list training train training m a x v a l u e train instance list training set num iterations num iterations <= 0 training set size > 0 get optimizable r f training set will set mcrf necessary get optimizer training set will set opt necessary converged = logger info r f about to train +num iterations+ iterations i = 0 i < num iterations i++ converged = opt optimize 1 iteration count++ logger info r f finished one iteration maximizer i= +i run evaluators illegal argument e e print stack trace logger info catching saying converged converged = e e print stack trace logger info catching saying converged converged = converged logger info r f training has converged i= +i converged train a r f on various sized subsets data typically used to accelerate training quickly getting to reasonable on only a subset first then on progressively more data training training instances num iterations per proportion maximum number maximizer iterations per training proportion training proportions non train on increasingly larger portions data e g 0 2 0 5 1 0 can sometimes speedup convergence be sure to end in 1 0 you want to train on all data in end training has converged train instance list training num iterations per proportion training proportions training iteration = 0 training proportions length > 0 converged = i = 0 i < training proportions length i++ training proportions i <= 1 0 logger info training on +training proportions i + % data round training proportions i == 1 0 converged = train training num iterations per proportion converged = train training split random 1 training proportions i 1 training proportions i 0 num iterations per proportion training iteration += num iterations per proportion converged train feature induction instance list training data instance list validation data instance list testing data transducer evaluator eval num iterations num iterations between feature inductions num feature inductions num features per feature induction label prob threshold clustered feature induction training proportions train feature induction training data validation data testing data eval num iterations num iterations between feature inductions num feature inductions num features per feature induction label prob threshold clustered feature induction training proportions exp train a r f using feature induction to generate conjunctions features feature induction run periodically during training features are added to improve performance on mislabeled instances specific scoring criterion given link feature inducer specified <code>gain name< code> training training instances validation validation instances testing testing instances eval evaluation during training num iterations maximum number maximizer iterations num iterations between feature inductions number maximizer iterations between each call to feature inducer num feature inductions maximum number rounds feature induction num features per feature induction maximum number features to induce at each round induction label prob threshold model s probability label an instance less than value it added an instance to link feature inducer clustered feature induction a separate link feature inducer constructed each label pair can avoid inducing a disproportionate number features a single label training proportions non train on increasingly larger portions data e g 0 2 0 5 1 0 can sometimes speedup convergence gain name type link feature inducer to use one exp grad or info link exp gain link gradient gain or link info gain training has converged train feature induction instance list training data instance list validation data instance list testing data transducer evaluator eval num iterations num iterations between feature inductions num feature inductions num features per feature induction label prob threshold clustered feature induction training proportions gain name training iteration = 0 num labels = crf output alphabet size crf global feature selection = training data get feature selection crf global feature selection == mask out all features some will be added later feature inducer induce features crf global feature selection = feature selection training data get data alphabet training data set feature selection crf global feature selection t o d o careful! validation data and testing data get removed arguments to then next two lines work will have to be done somewhere validation data != validation data set feature selection crf global feature selection testing data != testing data set feature selection crf global feature selection feature induction iteration = 0 feature induction iteration < num feature inductions feature induction iteration++ print out some feature logger info feature induction iteration +feature induction iteration train r f instance list training data = training data training proportions != feature induction iteration < training proportions length logger info training on +training proportions feature induction iteration + % data round instance list sampled training data = training data split random 1 training proportions feature induction iteration 1 training proportions feature induction iteration training data = sampled training data 0 training data set feature selection crf global feature selection xxx necessary? logger info which +the training data size + instances converged = feature induction iteration != 0 t train until we have added some features converged = train training data num iterations between feature inductions training iteration += num iterations between feature inductions logger info starting feature induction +crf input alphabet size + features create list tokens both unclustered and clustered feature induction instance list instances = instance list training data get data alphabet training data get target alphabet instances feature selection will get examined feature inducer so it can know how to add singleton features instances set feature selection crf global feature selection list label vectors = list instance list clustered instances = instance list num labels num labels list clustered label vectors = list num labels num labels i = 0 i < num labels i++ j = 0 j < num labels j++ clustered instances i j = instance list training data get data alphabet training data get target alphabet clustered instances i j set feature selection crf global feature selection clustered label vectors i j = list i = 0 i < training data size i++ logger info instance= +i instance instance = training data get i sequence input = sequence instance get data sequence output = sequence instance get target input size == output size sum lattice lattice = crf sum lattice factory sum lattice crf input sequence transducer incrementor label alphabet training data get target alphabet prev label index = 0 will put extra instances in cluster j = 0 j < output size j++ label label = label label sequence output get label at position j label != out instance= +i+ position= +j+ fv= +lattice get labeling at position j to label vector lattice labeling = lattice get labeling at position j label prob = lattice labeling value label get index label index = lattice labeling get best index out position= +j+ label prob= +true label prob label prob < label prob threshold logger info adding instance= +i+ position= +j+ prtrue= +true label prob+ label == lattice labeling get best label ? + truelabel= +label+ predlabel= +lattice labeling get best label + fv= + feature vector input get j to instances add input get j label label vectors add lattice labeling clustered instances prev label index label index add input get j label clustered label vectors prev label index label index add lattice labeling prev label index = label index logger info instance list size = +error instances size clustered feature induction feature inducer klfi = feature inducer num labels num labels i = 0 i < num labels i++ j = 0 j < num labels j++ note that we may some impossible transitions here like o > i in a o i b model because we are using lattice gammas to get predicted label not viterbi i t believe does any harm and may some good logger info doing feature induction + crf output alphabet lookup i + > +crf output alphabet lookup j + +clustered instances i j size + instances clustered instances i j size < 20 logger info skipping because only +clustered instances i j size + instances s = clustered label vectors i j size label vector lvs = label vector s k = 0 k < s k++ lvs k = label vector clustered label vectors i j get k ranked feature vector factory gain factory = gain name equals exp gain factory = exp gain factory lvs gaussian prior variance gain name equals grad gain factory = gradient gain factory lvs gain name equals info gain factory = info gain factory klfi i j = feature inducer gain factory clustered instances i j num features per feature induction 2 num features per feature induction 2 num features per feature induction crf feature inducers add klfi i j i = 0 i < num labels i++ j = 0 j < num labels j++ logger info adding induced features + crf output alphabet lookup i + > +crf output alphabet lookup j klfi i j == logger info skipping because no features induced note that adds features globally but not on a per transition basis klfi i j induce features training data testing data != klfi i j induce features testing data klfi = s = label vectors size label vector lvs = label vector s i = 0 i < s i++ lvs i = label vector label vectors get i ranked feature vector factory gain factory = gain name equals exp gain factory = exp gain factory lvs gaussian prior variance gain name equals grad gain factory = gradient gain factory lvs gain name equals info gain factory = info gain factory feature inducer klfi = feature inducer gain factory instances num features per feature induction 2 num features per feature induction 2 num features per feature induction crf feature inducers add klfi note that adds features globally but not on a per transition basis klfi induce features training data testing data != klfi induce features testing data logger info r f4 feature selection now includes +crf global feature selection cardinality + features klfi = done in r f4 train anyway set weights dimension in training data grow weights dimension to input alphabet train training data num iterations training iteration set use hyperbolic prior f using hyperbolic prior = f set hyperbolic prior slope p hyperbolic prior slope = p set hyperbolic prior sharpness p hyperbolic prior sharpness = p get use hyperbolic prior slope hyperbolic prior slope get use hyperbolic prior sharpness hyperbolic prior sharpness set gaussian prior variance p gaussian prior variance = p get gaussian prior variance gaussian prior variance get feature index feature index set use sparse weights b use sparse weights = b get use sparse weights use sparse weights sets whether to use some unsupported trick trick training a r f where some training has been done and sparse weights are used to add a few weights feaures that not occur in tainig data <p> generally leads to better accuracy at only a small memory cost b whether to use trick set use some unsupported trick b use some unsupported trick = b serialization r f trainer likelihood serial u = 1 u r r e n t s e r i a l v e r s i o n = 1 n u l l i n t e g e r = 1 need to check pointers write output stream out i o i size out write u r r e n t s e r i a l v e r s i o n out write feature index out write using hyperbolic prior out write gaussian prior variance out write hyperbolic prior slope out write hyperbolic prior sharpness out write cached gradient weights stamp out write cached value weights stamp out write cached weights structure stamp out write print gradient out write use sparse weights illegal state not yet complete read input stream in i o not found size i = in read feature index = in read using hyperbolic prior = in read gaussian prior variance = in read hyperbolic prior slope = in read hyperbolic prior sharpness = in read print gradient = in read use sparse weights = in read illegal state not yet complete 