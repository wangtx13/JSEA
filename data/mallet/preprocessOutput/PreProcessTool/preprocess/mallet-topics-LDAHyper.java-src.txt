2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics gnu trove t hash map arrays list list tree set iterator zip io text number format types randoms latent dirichlet allocation optimized hyperparameters author david mimno andrew mc callum deprecated use parallel topic model instead which uses substantially faster data structures even non parallel operation l d a hyper serializable analogous to a classify classification topication serializable instance instance l d a hyper model label sequence topic sequence labeling topic not actually constructed model fitting but could be added test documents topication instance instance l d a hyper model label sequence topic sequence instance = instance model = model topic sequence = topic sequence maintainable serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write instance out write model out write topic sequence out write topic read input stream in i o not found = in read instance = instance in read model = l d a hyper in read topic sequence = label sequence in read topic = labeling in read list< topication> data training instances and their topic assignments alphabet alphabet alphabet input data label alphabet topic alphabet alphabet topics num topics number topics to be fit num types alpha dirichlet alpha alpha over topics alpha sum beta prior on per topic multinomial over words beta sum d e f a u l t b e t a = 0 01 smoothing only mass = 0 0 cached coefficients topic term count = 0 beta topic count = 0 smoothing only count = 0 instance list empirical likelihood calculation instance list testing = an to put topic counts current document initialized locally below defined here to avoid garbage collection overhead one doc topic counts indexed <document index topic index> gnu trove t hash map type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> dirichlet estimation doc length counts histogram document sizes topic doc counts histogram document topic counts indexed <topic index sequence position index> iterations so far = 0 num iterations = 1000 burnin period = 20 was 50 was 200 save sample interval = 5 was 10 optimize interval = 20 was 50 show topics interval = 10 was 50 words per topic = 7 output model interval = 0 output model filename save state interval = 0 state filename = randoms random number format formatter print log likelihood = l d a hyper number topics number topics number topics d e f a u l t b e t a l d a hyper number topics alpha sum beta number topics alpha sum beta randoms label alphabet label alphabet num topics label alphabet ret = label alphabet i = 0 i < num topics i++ ret lookup index topic +i ret l d a hyper number topics alpha sum beta randoms random label alphabet number topics alpha sum beta random l d a hyper label alphabet topic alphabet alpha sum beta randoms random data = list< topication> topic alphabet = topic alphabet num topics = topic alphabet size alpha sum = alpha sum alpha = num topics arrays fill alpha alpha sum num topics beta = beta random = random one doc topic counts = num topics tokens per topic = num topics formatter = number format get instance formatter set maximum fraction digits 5 err l d a + num topics + topics alphabet get alphabet alphabet label alphabet get topic alphabet topic alphabet get num topics num topics list< topication> get data data get count feature topic feature index topic index type topic counts feature index get topic index get count tokens per topic topic index tokens per topic topic index held out instances empirical likelihood calculation set testing instances instance list testing testing = testing set num iterations num iterations num iterations = num iterations set burnin period burnin period burnin period = burnin period set topic display interval n show topics interval = interval words per topic = n set random seed seed random = randoms seed set optimize interval interval optimize interval = interval set model output interval filename output model interval = interval output model filename = filename define how often and where to save state interval save a copy state every <code>interval< code> iterations filename save state to iteration number a suffix set save state interval filename save state interval = interval state filename = filename instance length instance instance feature sequence instance get data size can be safely called multiple times will complain it can t handle situation initialize types alphabet alphabet alphabet == alphabet = alphabet num types = alphabet size type topic counts = t hash map num types fi = 0 fi < num types fi++ type topic counts fi = t hash map beta sum = beta num types alphabet != alphabet illegal argument cannot change alphabet alphabet size != num types num types = alphabet size t hash map type topic counts = t hash map num types i = 0 i < type topic counts length i++ type topic counts i = type topic counts i i = type topic counts length i < num types i++ type topic counts i = t hash map t o d o a k m july 18 why wasn t next line there previously? type topic counts = type topic counts beta sum = beta num types nothing changed nothing to be done initialize type topic counts t hash map type topic counts = t hash map num types i = 0 i < type topic counts length i++ type topic counts i = type topic counts i i = type topic counts length i < num types i++ type topic counts i = t hash map type topic counts = type topic counts add instances instance list training initialize types training get data alphabet list< label sequence> topic sequences = list< label sequence> instance instance training label sequence topic sequence = label sequence topic alphabet instance length instance not yet obeying its last argument and must be to work sample topics one doc feature sequence instance get data topic sequence randoms r = randoms topics = topic sequence get features i = 0 i < topics length i++ topics i = r next num topics topic sequences add topic sequence add instances training topic sequences add instances instance list training list< label sequence> topics initialize types training get data alphabet training size == topics size i = 0 i < training size i++ topication t = topication training get i topics get i data add t include sufficient statistics one doc feature sequence token sequence = feature sequence t instance get data label sequence topic sequence = t topic sequence pi = 0 pi < topic sequence get length pi++ topic = topic sequence get index at position pi type topic counts token sequence get index at position pi adjust or put value topic 1 1 tokens per topic topic ++ initialize histograms and cached values gather statistics on size documents and create histograms use in dirichlet hyperparameter optimization initialize histograms and cached values max tokens = 0 total tokens = 0 seq len doc = 0 doc < data size doc++ feature sequence fs = feature sequence data get doc instance get data seq len = fs get length seq len > max tokens max tokens = seq len total tokens += seq len initialize smoothing only sampling bucket smoothing only mass = 0 topic = 0 topic < num topics topic++ smoothing only mass += alpha topic beta tokens per topic topic + beta sum initialize cached coefficients using only smoothing cached coefficients = num topics topic=0 topic < num topics topic++ cached coefficients topic = alpha topic tokens per topic topic + beta sum err max tokens + max tokens err total tokens + total tokens doc length counts = max tokens + 1 topic doc counts = num topics max tokens + 1 estimate i o estimate num iterations estimate iterations round i o start time = current time millis max iteration = iterations so far + iterations round iterations so far <= max iteration iterations so far++ iteration start = current time millis show topics interval != 0 iterations so far != 0 iterations so far % show topics interval == 0 out print top words out words per topic testing != el = empirical likelihood 1000 testing ll = model log likelihood mi = topic label mutual out ll + + el + + mi save state interval != 0 iterations so far % save state interval == 0 print state state filename + + iterations so far output model interval != 0 iterations % output model interval == 0 write output model filename+ +iterations t o d o condition should also check that we have more than one sample to work here number samples actually obtained not yet tracked iterations so far > burnin period optimize interval != 0 iterations so far % optimize interval == 0 alpha sum = dirichlet learn alpha topic doc counts doc length counts smoothing only mass = 0 0 topic = 0 topic < num topics topic++ smoothing only mass += alpha topic beta tokens per topic topic + beta sum cached coefficients topic = alpha topic tokens per topic topic + beta sum clear histograms loop over every document in corpus topic term count = beta topic count = smoothing only count = 0 num docs = data size t o d o consider beginning sub sampling? di = 0 di < num docs di++ feature sequence token sequence = feature sequence data get di instance get data label sequence topic sequence = label sequence data get di topic sequence sample topics one doc token sequence topic sequence iterations so far >= burnin period iterations so far % save sample interval == 0 elapsed millis = current time millis iteration start elapsed millis < 1000 out print elapsed millis + ms out print elapsed millis 1000 + s out topic term count + + beta topic count + + smoothing only count iterations so far % 10 == 0 out < + iterations so far + > print log likelihood out model log likelihood out flush seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds clear histograms arrays fill doc length counts 0 topic = 0 topic < topic doc counts length topic++ arrays fill topic doc counts topic 0 topic sequence assignments are already set and accounted in sufficient statistics then readjust topics and stats should be topics will be re sampled and sufficient statistics changes operating on a or a test document and feature sequence topic sequence are not already accounted in sufficient statistics then readjust topics and stats should be current topic assignments will be ignored and sufficient statistics will not be changed you want to estimate dirichlet alpha based on per document topic multinomials sampled round then save state alpha estimation should be old sample topics one doc feature sequence feature sequence feature sequence topic sequence save state alpha estimation readjust topics and stats start time = current time millis one doc topics = topic sequence get features t hash map current type topic counts type old topic topic topic topic sum doc len = feature sequence get length adjusted value topic indices topic counts weight populate topic counts arrays fill one doc topic counts 0 readjust topics and stats token = 0 token < doc len token++ one doc topic counts one doc topics token ++ iterate over tokens words in document token = 0 token < doc len token++ type = feature sequence get index at position token old topic = one doc topics token current type topic counts = type topic counts type current type topic counts size != 0 readjust topics and stats remove token from all counts one doc topic counts old topic adjusted value = current type topic counts adjust or put value old topic 1 1 adjusted value == 0 current type topic counts remove old topic adjusted value == 1 illegal state token count in topic went negative tokens per topic old topic build a over topics token topic indices = current type topic counts keys topic counts = current type topic counts get values topic = topic indices length t o d o yipes memory allocation in inner loop! but note that keys and get values doing too topic sum = 0 i = 0 i < topic counts length i++ topic = topic indices i weight = topic counts i + beta tokens per topic topic + beta sum one doc topic counts topic + alpha topic topic sum += weight topic topic = weight sample a topic assignment from topic = topic indices random next discrete topic topic sum readjust topics and stats put that topic into counts one doc topics token = topic one doc topic counts topic ++ type topic counts type adjust or put value topic 1 1 tokens per topic topic ++ save state alpha estimation update document topic count histogram dirichlet estimation doc length counts doc len ++ topic=0 topic < num topics topic++ topic doc counts topic one doc topic counts topic ++ sample topics one doc feature sequence token sequence feature sequence topic sequence should save state readjust topics and stats currently ignored one doc topics = topic sequence get features t hash map current type topic counts type old topic topic topic weights sum doc length = token sequence get length populate topic counts t hash map local topic counts = t hash map position = 0 position < doc length position++ local topic counts adjust or put value one doc topics position 1 1 initialize topic count beta sampling bucket topic beta mass = 0 0 topic local topic counts keys n = local topic counts get topic initialize normalization constant b n t|d term topic beta mass += beta n tokens per topic topic + beta sum update coefficients non zero topics cached coefficients topic = alpha topic + n tokens per topic topic + beta sum topic term mass = 0 0 topic term scores = num topics topic term indices topic term values i score iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = one doc topics position current type topic counts = type topic counts type current type topic counts get old topic >= 0 remove token from all counts note that we actually want to remove key it goes to zero not set it to 0 current type topic counts get old topic == 1 current type topic counts remove old topic current type topic counts adjust value old topic 1 smoothing only mass = alpha old topic beta tokens per topic old topic + beta sum topic beta mass = beta local topic counts get old topic tokens per topic old topic + beta sum local topic counts get old topic == 1 local topic counts remove old topic local topic counts adjust value old topic 1 tokens per topic old topic smoothing only mass += alpha old topic beta tokens per topic old topic + beta sum topic beta mass += beta local topic counts get old topic tokens per topic old topic + beta sum cached coefficients old topic = alpha old topic + local topic counts get old topic tokens per topic old topic + beta sum topic term mass = 0 0 topic term indices = current type topic counts keys topic term values = current type topic counts get values i=0 i < topic term indices length i++ topic = topic term indices i score = cached coefficients topic topic term values i alpha topic + local topic counts get topic topic term values i tokens per topic topic + beta sum note i tried only doing next bit score > 0 but it didn t make any difference at least in first few iterations topic term mass += score topic term scores i = score topic term indices i = topic indicate that last topic topic term indices i = 1 sample = random next uniform smoothing only mass + topic beta mass + topic term mass orig sample = sample make sure it actually gets set topic = 1 sample < topic term mass topic term count++ i = 1 sample > 0 i++ sample = topic term scores i topic = topic term indices i sample = topic term mass sample < topic beta mass beta topic count++ sample = beta topic term indices = local topic counts keys topic term values = local topic counts get values i=0 i < topic term indices length i++ topic = topic term indices i sample = topic term values i tokens per topic topic + beta sum sample <= 0 0 smoothing only count++ sample = topic beta mass sample = beta topic = 0 topic < num topics topic++ sample = alpha topic tokens per topic topic + beta sum sample <= 0 0 topic = topic topic == 1 err l d a hyper sampling + orig sample + + sample + + smoothing only mass + + topic beta mass + + topic term mass topic = num topics 1 t o d o appropriate illegal state l d a hyper topic not sampled topic != 1 put that topic into counts one doc topics position = topic current type topic counts adjust or put value topic 1 1 smoothing only mass = alpha topic beta tokens per topic topic + beta sum topic beta mass = beta local topic counts get topic tokens per topic topic + beta sum local topic counts adjust or put value topic 1 1 tokens per topic topic ++ update coefficients non zero topics cached coefficients topic = alpha topic + local topic counts get topic tokens per topic topic + beta sum smoothing only mass += alpha topic beta tokens per topic topic + beta sum topic beta mass += beta local topic counts get topic tokens per topic topic + beta sum current type topic counts get topic >= 0 clean up our mess reset coefficients to values only smoothing next doc will update its own non zero topics topic local topic counts keys cached coefficients topic = alpha topic tokens per topic topic + beta sum should save state update document topic count histogram dirichlet estimation doc length counts doc length ++ topic local topic counts keys topic doc counts topic local topic counts get topic ++ sorter get sorted topic words topic sorter sorted types = sorter num types type = 0 type < num types type++ sorted types type = sorter type type topic counts type get topic arrays sort sorted types sorted types print top words num words use lines i o print stream out = print stream print top words out num words use lines out close tree set ~70x faster than ranked feature vector d m print top words print stream out num words using lines topic = 0 topic < num topics topic++ tree set< sorter> sorted words = tree set< sorter> type = 0 type < num types type++ type topic counts type contains key topic sorted words add sorter type type topic counts type get topic using lines out topic + topic word = 1 iterator< sorter> iterator = sorted words iterator iterator has next word < num words sorter info = iterator next out alphabet lookup info get + + info get weight word++ out print topic + + formatter format alpha topic + + tokens per topic topic + word = 1 iterator< sorter> iterator = sorted words iterator iterator has next word < num words sorter info = iterator next out print alphabet lookup info get + word++ out topic xml report print writer out num words out <?xml version= 1 0 ?> out <topic model> topic = 0 topic < num topics topic++ out <topic id= + topic + alpha= + alpha topic + total tokens= + tokens per topic topic + > tree set< sorter> sorted words = tree set< sorter> type = 0 type < num types type++ type topic counts type contains key topic sorted words add sorter type type topic counts type get topic word = 1 iterator< sorter> iterator = sorted words iterator iterator has next word < num words sorter info = iterator next out <word rank= + word + > + alphabet lookup info get + < word> word++ out < topic> out < topic model> topic xml report phrases print stream out num words num topics = get num topics gnu trove t hash map< string> phrases = gnu trove t hash map num topics alphabet alphabet = get alphabet get counts phrases ti = 0 ti < num topics ti++ phrases ti = gnu trove t hash map< string> di = 0 di < get data size di++ l d a hyper topication t = get data get di instance instance = t instance feature sequence fvs = feature sequence instance get data bigrams = fvs feature sequence bigrams bigrams = prevtopic = 1 prevfeature = 1 topic = 1 buffer sb = feature = 1 doclen = fvs size pi = 0 pi < doclen pi++ feature = fvs get index at position pi topic = get data get di topic sequence get index at position pi topic == prevtopic !with bigrams || feature sequence bigrams fvs get bi index at position pi != 1 sb == sb = buffer alphabet lookup prevfeature to + + alphabet lookup feature sb append sb append alphabet lookup feature sb != sbs = sb to out phrase +sbs phrases prevtopic get sbs == 0 phrases prevtopic put sbs 0 phrases prevtopic increment sbs prevtopic = prevfeature = 1 sb = prevtopic = topic prevfeature = feature phrases now filled counts now start printing xml out <?xml version= 1 0 ?> out <topics> probs = alphabet size ti = 0 ti < num topics ti++ out print <topic id=\ + ti + \ alpha=\ + alpha ti + \ total tokens=\ + tokens per topic ti + \ gathering <term> and <phrase> output temporarily so that we can get topic title before printing it to out output stream bout = output stream print stream pout = print stream bout holding candidate topic titles augmentable feature vector titles = augmentable feature vector alphabet print words type = 0 type < alphabet size type++ probs type = get count feature topic type ti get count tokens per topic ti ranked feature vector rfv = ranked feature vector alphabet probs ri = 0 ri < num words ri++ fi = rfv get index at rank ri pout <term weight=\ +probs fi + \ count=\ +this get count feature topic fi ti + \ > +alphabet lookup fi + < term> ri < 20 consider top 20 individual words candidate titles titles add alphabet lookup fi get count feature topic fi ti print phrases keys = phrases ti keys values = phrases ti get values counts = keys length i = 0 i < counts length i++ counts i = values i countssum = matrix ops sum counts alphabet alph = alphabet keys rfv = ranked feature vector alph counts out topic +ti max = rfv num locations < num words ? rfv num locations num words out topic +ti+ num phrases= +rfv num locations ri = 0 ri < max ri++ fi = rfv get index at rank ri pout <phrase weight=\ +counts fi countssum+ \ count=\ +values fi + \ > +alph lookup fi + < phrase> any phrase count less than 20 simply unreliable ri < 20 values fi > 20 titles add alph lookup fi 100 values fi prefer phrases a factor 100 select candidate titles buffer titles buffer = buffer rfv = ranked feature vector titles get alphabet titles num titles = 10 ri = 0 ri < num titles ri < rfv num locations ri++ t add redundant titles titles buffer index rfv get at rank ri to == 1 titles buffer append rfv get at rank ri ri < num titles 1 titles buffer append num titles++ out titles=\ + titles buffer to + \ > out print pout to out < topic> out < topics> print document topics f i o print document topics print writer writer f print document topics print writer pw print document topics pw 0 0 1 pw a print writer threshold only print topics proportion greater than number max print no more than many topics print document topics print writer pw threshold max pw print #doc source topic proportion doc len topic counts = num topics sorter sorted topics = sorter num topics topic = 0 topic < num topics topic++ initialize sorters dummy values sorted topics topic = sorter topic topic max < 0 || max > num topics max = num topics di = 0 di < data size di++ label sequence topic sequence = label sequence data get di topic sequence current doc topics = topic sequence get features pw print di pw print data get di instance get source != pw print data get di instance get source pw print source pw print doc len = current doc topics length count up tokens token=0 token < doc len token++ topic counts current doc topics token ++ and normalize topic = 0 topic < num topics topic++ sorted topics topic set topic topic counts topic doc len arrays sort sorted topics i = 0 i < max i++ sorted topics i get weight < threshold pw print sorted topics i get + + sorted topics i get weight + pw print arrays fill topic counts 0 print state f i o print stream out = print stream g z i p output stream buffered output stream output stream f print state out out close print state print stream out out #doc source pos typeindex type topic di = 0 di < data size di++ feature sequence token sequence = feature sequence data get di instance get data label sequence topic sequence = label sequence data get di topic sequence source = n a data get di instance get source != source = data get di instance get source to pi = 0 pi < topic sequence get length pi++ type = token sequence get index at position pi topic = topic sequence get index at position pi out print di out print out print source out print out print pi out print out print type out print out print alphabet lookup type out print out print topic out turbo topics corpus word counts alphabet unigram alphabet feature counter unigram counts = feature counter unigram alphabet corpus word counts alphabet alphabet unigram alphabet = alphabet mylog x x == 0 ? 1000000 0 math log x likelihood ratio significance test significance test unigram count next unigram count next bigram count next total count min count next bigram count < min count 1 0 next unigram count >= next bigram count log pi vu = mylog next bigram count mylog unigram count log pi vnu = mylog next unigram count next bigram count mylog next total count next bigram count log pi v old = mylog next unigram count mylog next total count log 1mp v = mylog 1 math exp log pi vnu log 1mp vu = mylog 1 math exp log pi vu 2 next bigram count log pi vu + next unigram count next bigram count log pi vnu next unigram count log pi v old + unigram count next bigram count log 1mp vu log 1mp v significant bigrams word write f output stream oos = output stream output stream f oos write oos close i o e err l d a hyper write writing l d a hyper to + f + + e l d a hyper read f l d a hyper lda = input stream ois = input stream input stream f lda = l d a hyper ois read lda initialize type topic counts to work around a bug in trove? ois close i o e err reading + f + + e not found e err reading + f + + e lda serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n instance lists out write data out write alphabet out write topic alphabet out write num topics out write alpha out write beta out write beta sum out write smoothing only mass out write cached coefficients out write iterations so far out write num iterations out write burnin period out write save sample interval out write optimize interval out write show topics interval out write words per topic out write output model interval out write output model filename out write save state interval out write state filename out write random out write formatter out write print log likelihood out write doc length counts out write topic doc counts fi = 0 fi < num types fi++ out write type topic counts fi ti = 0 ti < num topics ti++ out write tokens per topic ti read input stream in i o not found features length = in read data = list< topication> in read alphabet = alphabet in read topic alphabet = label alphabet in read num topics = in read alpha = in read beta = in read beta sum = in read smoothing only mass = in read cached coefficients = in read iterations so far = in read num iterations = in read burnin period = in read save sample interval = in read optimize interval = in read show topics interval = in read words per topic = in read output model interval = in read output model filename = in read save state interval = in read state filename = in read random = randoms in read formatter = number format in read print log likelihood = in read doc length counts = in read topic doc counts = in read num docs = data size num types = alphabet size type topic counts = t hash map num types fi = 0 fi < num types fi++ type topic counts fi = t hash map in read tokens per topic = num topics ti = 0 ti < num topics ti++ tokens per topic ti = in read topic label mutual doc level label topic token type doc topics data get 0 instance get target alphabet == 0 0 target alphabet size = data get 0 instance get target alphabet size topic label counts = num topics target alphabet size topic counts = num topics label counts = target alphabet size total = 0 doc=0 doc < data size doc++ label = data get doc instance get labeling get best index label sequence topic sequence = label sequence data get doc topic sequence doc topics = topic sequence get features token = 0 token < doc topics length token++ topic = doc topics token topic label counts topic label ++ topic counts topic ++ label counts label ++ total++ block will print out best topics each label sorter wp = sorter num types topic = 0 topic < num topics topic++ type = 0 type < num types type++ wp type = sorter type type topic counts type topic tokens per topic topic arrays sort wp buffer = buffer i = 0 i < 8 i++ append instances get data alphabet lookup wp i append out label = 0 label < topic label counts topic length label++ out topic label counts topic label + + instances get target alphabet lookup label out topic entropy = 0 0 label entropy = 0 0 joint entropy = 0 0 p log2 = math log 2 topic = 0 topic < topic counts length topic++ topic counts topic == 0 p = topic counts topic total topic entropy = p math log p log2 label = 0 label < label counts length label++ label counts label == 0 p = label counts label total label entropy = p math log p log2 topic = 0 topic < topic counts length topic++ label = 0 label < label counts length label++ topic label counts topic label == 0 p = topic label counts topic label total joint entropy = p math log p log2 topic entropy + label entropy joint entropy empirical likelihood num samples instance list testing likelihoods = testing size num samples multinomial = num types topic current sample current weights dirichlet topic prior = dirichlet alpha sample doc topic type token seq len feature sequence fs sample = 0 sample < num samples sample++ topic = topic prior next arrays fill multinomial 0 0 topic = 0 topic < num topics topic++ type=0 type<num types type++ multinomial type += topic topic beta + type topic counts type get topic beta sum + tokens per topic topic convert to log probabilities type=0 type<num types type++ multinomial type > 0 0 multinomial type = math log multinomial type doc=0 doc<testing size doc++ fs = feature sequence testing get doc get data seq len = fs get length token = 0 token < seq len token++ type = fs get index at position token adding check since testing instances may have types not found in training instances pointed out steven bethard type < num types likelihoods doc sample += multinomial type average log likelihood = 0 0 log num samples = math log num samples doc=0 doc<testing size doc++ max = n e g a t i v e i n f i n i t y sample = 0 sample < num samples sample++ likelihoods doc sample > max max = likelihoods doc sample sum = 0 0 sample = 0 sample < num samples sample++ sum += math exp likelihoods doc sample max average log likelihood += math log sum + max log num samples average log likelihood model log likelihood log likelihood = 0 0 non zero topics likelihood model a combination a dirichlet multinomial words in each topic and a dirichlet multinomial topics in each document likelihood function a dirichlet multinomial gamma sum i alpha i prod i gamma alpha i + n i prod i gamma alpha i gamma sum i alpha i + n i so log likelihood log gamma sum i alpha i log gamma sum i alpha i + n i + sum i log gamma alpha i + n i log gamma alpha i documents first topic counts = num topics topic log gammas = num topics doc topics topic=0 topic < num topics topic++ topic log gammas topic = dirichlet log gamma stirling alpha topic doc=0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence doc topics = topic sequence get features token=0 token < doc topics length token++ topic counts doc topics token ++ topic=0 topic < num topics topic++ topic counts topic > 0 log likelihood += dirichlet log gamma stirling alpha topic + topic counts topic topic log gammas topic subtract count + parameter sum term log likelihood = dirichlet log gamma stirling alpha sum + doc topics length arrays fill topic counts 0 add parameter sum term log likelihood += data size dirichlet log gamma stirling alpha sum and topics count number type topic pairs non zero type topics = 0 type=0 type < num types type++ used topics = type topic counts type keys topic used topics count = type topic counts type get topic count > 0 non zero type topics++ log likelihood += dirichlet log gamma stirling beta + count topic=0 topic < num topics topic++ log likelihood = dirichlet log gamma stirling beta num topics + tokens per topic topic log likelihood += dirichlet log gamma stirling beta num topics dirichlet log gamma stirling beta non zero type topics log likelihood recommended to use bin vectors2topics instead i o instance list training = instance list load 0 num topics = length > 1 ? parse 1 200 instance list testing = length > 2 ? instance list load 2 l d a hyper lda = l d a hyper num topics 50 0 0 01 lda print log likelihood = lda set topic display 50 7 lda add instances training lda estimate 