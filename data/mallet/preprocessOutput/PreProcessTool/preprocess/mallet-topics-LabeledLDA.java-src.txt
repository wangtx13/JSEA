topics logging zip io types pipe iterator d b instance iterator labeled l d a author david mimno labeled l d a serializable logger logger = logger get logger labeled l d a get name command option input = command option labeled l d a input f i l e n a m e filename from which to read list training instances use stdin + instances must be feature sequence not feature vector command option output prefix = command option labeled l d a output prefix s t r i n g prefix output files sampling states etc + indicating that no will be written command option input model filename = command option labeled l d a input model f i l e n a m e filename from which to read binary topic model input option ignored + indicating that no will be read command option input state filename = command option labeled l d a input state f i l e n a m e filename from which to read gzipped gibbs sampling state created output state + original input must be using input + indicating that no will be read model output options command option output model filename = command option labeled l d a output model f i l e n a m e filename in which to write binary topic model at end iterations + indicating that no will be written command option state = command option labeled l d a output state f i l e n a m e filename in which to write gibbs sampling state after at end iterations + indicating that no will be written command option output model interval = command option labeled l d a output model interval i n t e g e r 0 number iterations between writing model and its gibbs sampling state to a binary + you must also set output model to use option whose argument will be prefix filenames command option output state interval = command option labeled l d a output state interval i n t e g e r 0 number iterations between writing sampling state to a text + you must also set output state to use option whose argument will be prefix filenames command option inferencer filename = command option labeled l d a inferencer filename f i l e n a m e a topic inferencer applies a previously trained topic model to documents + indicating that no will be written command option evaluator filename = command option labeled l d a evaluator filename f i l e n a m e a held out likelihood evaluator documents + indicating that no will be written command option topic keys = command option labeled l d a output topic keys f i l e n a m e filename in which to write top words each topic and any dirichlet + indicating that no will be written command option num top words = command option labeled l d a num top words i n t e g e r 20 number most probable words to print each topic after model estimation command option show topics interval option = command option labeled l d a show topics interval i n t e g e r 50 number iterations between printing a brief summary topics so far command option topic word weights = command option labeled l d a topic word weights f i l e n a m e filename in which to write unnormalized weights every topic and word type + indicating that no will be written command option word topic counts = command option labeled l d a word topic counts f i l e n a m e filename in which to write a sparse representation topic word assignments + indicating that no will be written command option diagnostics = command option labeled l d a diagnostics f i l e n a m e filename in which to write measures topic quality in xml format + indicating that no will be written command option topic report xml = command option labeled l d a xml topic report f i l e n a m e filename in which to write top words each topic and any dirichlet in xml format + indicating that no will be written command option topic phrase report xml = command option labeled l d a xml topic phrase report f i l e n a m e filename in which to write top words and phrases each topic and any dirichlet in xml format + indicating that no will be written command option topic docs = command option labeled l d a output topic docs f i l e n a m e filename in which to write most prominent documents each topic at end iterations + indicating that no will be written command option num top docs = command option labeled l d a num top docs i n t e g e r 100 when writing topic documents output topic docs + report number top documents command option doc topics = command option labeled l d a output doc topics f i l e n a m e filename in which to write topic proportions per document at end iterations + indicating that no will be written command option doc topics threshold = command option labeled l d a doc topics threshold d e i m a l 0 0 when writing topic proportions per document output doc topics + not print topics proportions less than threshold value command option doc topics max = command option labeled l d a doc topics max i n t e g e r 1 when writing topic proportions per document output doc topics + not print more than i n t e g e r number topics + a negative value indicates that all topics should be printed model command option num iterations option = command option labeled l d a num iterations i n t e g e r 1000 number iterations gibbs sampling command option no inference = command option labeled l d a no inference true|false not perform inference just load a saved model and create a report equivalent to num iterations 0 command option random seed = command option labeled l d a random seed i n t e g e r 0 random seed gibbs sampler 0 which will use clock hyperparameters command option alpha option = command option labeled l d a alpha d e i m a l 0 1 alpha parameter smoothing over doc topic n o t sum over topics command option beta option = command option labeled l d a beta d e i m a l 0 01 beta parameter smoothing over word distributions training instances and their topic assignments list< topic assignment> data alphabet input data alphabet alphabet alphabet stores meanings labels topics alphabet label alphabet alphabet topics label alphabet topic alphabet number topics requested num topics size vocabulary num types prior alpha dirichlet alpha alpha over topics beta prior on per topic multinomial over words beta sum d e f a u l t b e t a = 0 01 an to put topic counts current document initialized locally below defined here to avoid garbage collection overhead one doc topic counts indexed <document index topic index> statistics needed sampling type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> num iterations = 1000 show topics interval = 50 words per topic = 10 randoms random print log likelihood = labeled l d a alpha beta data = list< topic assignment> alpha = alpha beta = beta random = randoms logger info labeled l d a alphabet get alphabet alphabet label alphabet get topic alphabet topic alphabet list< topic assignment> get data data set topic display interval n show topics interval = interval words per topic = n set random seed seed random = randoms seed set num iterations num iterations num iterations = num iterations get type topic counts type topic counts get topic totals tokens per topic add instances instance list training alphabet = training get data alphabet num types = alphabet size beta sum = beta num types we have one topic every possible label label alphabet = training get target alphabet num topics = label alphabet size one doc topic counts = num topics tokens per topic = num topics type topic counts = num types num topics topic alphabet = alphabet factory label alphabet size num topics doc = 0 instance instance training doc++ feature sequence tokens = feature sequence instance get data feature vector labels = feature vector instance get target label sequence topic sequence = label sequence topic alphabet tokens size topics = topic sequence get features position = 0 position < tokens size position++ topic = labels index at location random next labels num locations topics position = topic tokens per topic topic ++ type = tokens get index at position position type topic counts type topic ++ topic assignment t = topic assignment instance topic sequence data add t initialize from state state i o line fields buffered reader reader = buffered reader input stream reader g z i p input stream input stream state line = reader read line skip some lines starting # that describe format and specify hyperparameters line starts # line = reader read line fields = line split topic assignment document data feature sequence tokens = feature sequence document instance get data feature sequence topic sequence = feature sequence document topic sequence topics = topic sequence get features position = 0 position < tokens size position++ type = tokens get index at position position type == parse fields 3 topic = parse fields 5 topics position = topic difference between dense type topic representation used here and sparse used in parallel topic model type topic counts type topic ++ err instance list and state not match + line illegal state line = reader read line line != fields = line split estimate i o iteration = 1 iteration <= num iterations iteration++ iteration start = current time millis loop over every document in corpus doc = 0 doc < data size doc++ feature sequence token sequence = feature sequence data get doc instance get data feature vector labels = feature vector data get doc instance get target label sequence topic sequence = label sequence data get doc topic sequence sample topics one doc token sequence labels topic sequence elapsed millis = current time millis iteration start logger info iteration + + elapsed millis + ms occasionally print more show topics interval != 0 iteration % show topics interval == 0 logger info < + iteration + > log likelihood + model log likelihood + + top words words per topic sample topics one doc feature sequence token sequence feature vector labels feature sequence topic sequence possible topics = labels get indices num labels = labels num locations one doc topics = topic sequence get features current type topic counts type old topic topic topic weights sum doc length = token sequence get length local topic counts = num topics populate topic counts position = 0 position < doc length position++ local topic counts one doc topics position ++ score sum topic term scores = num labels iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = one doc topics position grab relevant row from our two dimensional current type topic counts = type topic counts type remove token from all counts local topic counts old topic tokens per topic old topic tokens per topic old topic >= 0 old topic + old topic + below 0 current type topic counts old topic now calculate and add up scores each topic word sum = 0 0 here s where math happens! note that overall performance dominated what you in loop label position = 0 label position < num labels label position++ topic = possible topics label position score = alpha + local topic counts topic beta + current type topic counts topic beta sum + tokens per topic topic sum += score topic term scores label position = score choose a random point between 0 and sum all topic scores sample = random next uniform sum figure out which topic contains that point label position = 1 sample > 0 0 label position++ sample = topic term scores label position make sure we actually sampled a topic label position == 1 illegal state labeled l d a topic not sampled topic = possible topics label position put that topic into counts one doc topics position = topic local topic counts topic ++ tokens per topic topic ++ current type topic counts topic ++ model log likelihood log likelihood = 0 0 non zero topics likelihood model a combination a dirichlet multinomial words in each topic and a dirichlet multinomial topics in each document likelihood function a dirichlet multinomial gamma sum i alpha i prod i gamma alpha i + n i prod i gamma alpha i gamma sum i alpha i + n i so log likelihood log gamma sum i alpha i log gamma sum i alpha i + n i + sum i log gamma alpha i + n i log gamma alpha i documents first topic counts = num topics topic log gammas = num topics doc topics topic=0 topic < num topics topic++ topic log gammas topic = dirichlet log gamma alpha doc=0 doc < data size doc++ label sequence topic sequence = label sequence data get doc topic sequence feature vector labels = feature vector data get doc instance get target doc topics = topic sequence get features token=0 token < doc topics length token++ topic counts doc topics token ++ topic=0 topic < num topics topic++ topic counts topic > 0 log likelihood += dirichlet log gamma alpha + topic counts topic topic log gammas topic add parameter sum term log likelihood += dirichlet log gamma alpha labels num locations subtract count + parameter sum term log likelihood = dirichlet log gamma alpha labels num locations + doc topics length arrays fill topic counts 0 and topics count number type topic pairs non zero type topics = 0 type=0 type < num types type++ reuse a pointer topic counts = type topic counts type topic = 0 topic < num topics topic++ topic counts topic == 0 non zero type topics++ log likelihood += dirichlet log gamma beta + topic counts topic na n log likelihood out topic counts topic exit 1 topic=0 topic < num topics topic++ log likelihood = dirichlet log gamma beta num topics + tokens per topic topic na n log likelihood out after topic + topic + + tokens per topic topic exit 1 log likelihood += dirichlet log gamma beta num topics dirichlet log gamma beta non zero type topics na n log likelihood out at end exit 1 log likelihood displaying and saving results top words num words builder output = builder sorter sorted words = sorter num types topic = 0 topic < num topics topic++ tokens per topic topic == 0 type = 0 type < num types type++ sorted words type = sorter type type topic counts type topic arrays sort sorted words output append topic + + label alphabet lookup topic + + tokens per topic topic + i=0 i < num words i++ sorted words i get weight == 0 output append alphabet lookup sorted words i get + output append output to serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write f output stream oos = output stream output stream f oos write oos close i o e err writing + f + + e labeled l d a read f labeled l d a topic model = input stream ois = input stream input stream f topic model = labeled l d a ois read ois close topic model write output stream out i o out write u r r e n t s e r i a l v e r s i o n instance lists out write data out write alphabet out write topic alphabet out write num topics out write alpha out write beta out write beta sum out write show topics interval out write words per topic out write random out write print log likelihood out write type topic counts ti = 0 ti < num topics ti++ out write tokens per topic ti read input stream in i o not found features length = in read data = list< topic assignment> in read alphabet = alphabet in read topic alphabet = label alphabet in read num topics = in read alpha = in read beta = in read beta sum = in read show topics interval = in read words per topic = in read random = randoms in read print log likelihood = in read num docs = data size num types = alphabet size type topic counts = in read tokens per topic = num topics ti = 0 ti < num topics ti++ tokens per topic ti = in read command option set summary labeled l d a sample associations between words and labels command option process labeled l d a labeled l d a labeled l d a input model filename value != labeled l d a = labeled l d a read input model filename value labeled l d a = labeled l d a alpha option value beta option value random seed value != 0 labeled l d a set random seed random seed value input value != instance list training = input value starts db training = d b instance iterator get instances input value substring 3 training = instance list load input value e logger warning unable to restore instance list + input value + + e exit 1 logger info data loaded training size > 0 training get 0 != data = training get 0 get data ! data feature sequence logger warning topic modeling currently only supports feature sequences use keep sequence option when importing data exit 1 labeled l d a add instances training input state filename value != logger info initializing from saved state labeled l d a initialize from state input state filename value labeled l d a set topic display show topics interval option value num top words value labeled l d a set num iterations num iterations option value ! no inference value labeled l d a estimate topic keys value != print stream out = print stream topic keys value out print labeled l d a top words num top words value out close output model filename value != labeled l d a != output stream oos = output stream output stream output model filename value oos write labeled l d a oos close e logger warning couldn t write topic model to filename + output model filename value i t want to directly inherit from parallel topic model because two implementations treat type topic counts differently instead simulate a standard parallel topic model copying over appropriate data structures parallel topic model topic model = parallel topic model labeled l d a topic alphabet labeled l d a alpha labeled l d a num topics labeled l d a beta topic model data = labeled l d a data topic model alphabet = labeled l d a alphabet topic model num types = labeled l d a num types topic model beta sum = labeled l d a beta sum topic model build initial type topic counts diagnostics value != print writer out = print writer diagnostics value topic model diagnostics diagnostics = topic model diagnostics topic model num top words value out diagnostics to xml out close topic report xml value != print writer out = print writer topic report xml value topic model topic xml report out num top words value out close topic phrase report xml value != print writer out = print writer topic phrase report xml value topic model topic phrase xml report out num top words value out close state value != output state interval value == 0 topic model print state state value topic docs value != print writer out = print writer writer topic docs value topic model print topic documents out num top docs value out close doc topics value != print writer out = print writer writer doc topics value doc topics threshold value == 0 0 topic model print dense document topics out topic model print document topics out doc topics threshold value doc topics max value out close topic word weights value != topic model print topic word weights topic word weights value word topic counts value != topic model print type topic counts word topic counts value inferencer filename value != output stream oos = output stream output stream inferencer filename value oos write topic model get inferencer oos close e logger warning couldn t create inferencer + e get message evaluator filename value != output stream oos = output stream output stream evaluator filename value oos write topic model get prob estimator oos close e logger warning couldn t create evaluator + e get message 