2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e gain obtained adding a feature to a conditional exponential model based on joint exponential model in della pietra della pietra lafferty 1997 we smooth using a gaussian prior note that we use math log not log base 2 so units are not bits author andrew mc callum <a href= mailto >mccallum edu< a> types logging io classify classification logger exp gain ranked feature vector logger logger = logger get logger exp gain get name exp gain a feature f defined in max ent type feature+class feature s f f = f exp gain a feature f g f = k l p~ ||q k l p~ ||q f where p~ empirical according to label and q from imperfect classifier and q f from imperfect classifier f added and f s weight adjusted but none other weights adjusted exp gain a feature f g f = sum g f it would be more accurate to a gain number each feature combination but here we simply gain feature = \sum gain feature xxx not ever used remove them using hyperbolic prior = hyperbolic slope = 0 2 hyperbolic sharpness = 10 0 calc exp gains instance list ilist label vector classifications gaussian prior variance num instances = ilist size num classes = ilist get target alphabet size num features = ilist get data alphabet size ilist size > 0 notation from della pietra lafferty 1997 p 4 p~ p = num classes num features q q = num classes num features alpha weight feature alphas = num classes num features fli feature location index flv feature location value logger info starting klgains #instances= +num instances label weight sum = 0 model label weight sum = 0 calculate p~ f and q f i = 0 i < num instances i++ classifications i get label alphabet == ilist get target alphabet instance inst = ilist get i labeling labeling = inst get labeling feature vector fv = feature vector inst get data instance weight = ilist get instance weight i below relies on label weights summing to 1 over all labels! per instance model label weight = 0 = 0 < num classes li++ label weight = labeling value model label weight = classifications i value label weight sum += label weight model label weight sum += model label weight per instance model label weight += model label weight i < 500 out i= +i+ li= +li+ true= +true label weight+ model= +model label weight label weight == 0 model label weight == 0 fl = 0 fl < fv num locations fl++ fli = fv index at location fl fv value at location fl == 1 0 xxx note that we are not attenting to instance weight here! p fli += label weight instance weight num instances+1 q fli += model label weight instance weight num instances+1 p fli += label weight q fli += model label weight math abs per instance model label weight 1 0 < 0 001 math abs label weight sum num instances 1 0 < 0 001 label weight sum should be 1 0 it was +true label weight sum math abs model label weight sum num instances 1 0 < 0 001 model label weight sum should be 1 0 it was +model label weight sum psum = 0 qsum = 0 i = 0 i < num classes i++ j = 0 j < num features j++ psum += p i j qsum += q i j math abs psum 1 0 < 0 0001 psum not 1 0! psum= +psum+ qsum= +qsum math abs qsum 1 0 < 0 0001 qsum not 1 0! psum= +psum+ qsum= +qsum determine alphas we can t it in closed form in della pietra paper because we have here a conditional max ent model so we it newton raphson initializing broken inappropriate joint closed form solution i = 0 i < num classes i++ j = 0 j < num features j++ alphas i j = math log p i j 1 0 q i j q i j 1 0 p i j dalphas = num classes num features first derivative alpha change old = num classes num features change in alpha last iteration alpha max = num classes num features change in alpha last iteration alpha min = num classes num features change in alpha last iteration ddalphas = num classes num features second derivative i = 0 i < num classes i++ j = 0 j < num features j++ alpha max i j = p o s i t i v e i n f i n i t y alpha min i j = n e g a t i v e i n f i n i t y max alphachange = 0 max dalpha = 99 max newton steps = 50 xxx change to more? alphas are initialized to zero newton = 0 max dalpha > 1 0 e 8 newton < max newton steps newton++ out newton iteration +newton using hyperbolic prior i = 0 i < num classes i++ j = 0 j < num features j++ dalphas i j = p i j alphas i j gaussian prior variance ddalphas i j = 1 gaussian prior variance gaussian prior i = 0 i < num classes i++ j = 0 j < num features j++ dalphas i j = p i j alphas i j gaussian prior variance ddalphas i j = 1 gaussian prior variance i = 0 i < ilist size i++ classifications i get label alphabet == ilist get target alphabet instance inst = ilist get i labeling labeling = inst get labeling feature vector fv = feature vector inst get data xxx assumes binary valued features what about tied weights? fl = 0 fl < fv num locations fl++ fli = fv index at location fl = 0 < num classes li++ model label weight = classifications i value expalpha = math exp alphas fli numerator = model label weight expalpha denominator = numerator + 1 0 model label weight dalphas fli = numerator denominator ddalphas fli += numerator numerator denominator denominator numerator denominator we now now first and second derivative newton step run tests on alphas and their derivatives and a newton step alphachange newalpha oldalpha max alphachange = max dalpha = 0 i = 0 i < num classes i++ j = 0 j < num features j++ alphachange = dalphas i j ddalphas i j p i j == 0 q i j == 0 i num features+j % num classes num features 2000 == 0 || na n alphas i j || na n alphachange print just a sampling them logger info alpha +i+ +j+ = +alphas i j + p= +p i j + q= +q i j + dalpha= +dalphas i j + ddalpha= +ddalphas i j + alphachange= +alphachange+ min= +alpha min i j + max= +alpha max i j na n alphas i j || na n dalphas i j || na n ddalphas i j || infinite alphas i j || infinite dalphas i j || infinite ddalphas i j alphachange = 0 ! na n alphas i j ! na n dalphas i j ! na n ddalphas i j oldalpha = alphas i j xxx ddalphas i j <= 0 math abs alphachange < 100 0 alphachange xxx arbitrary? trying to prevent a cycle math abs alphachange + alpha change old i j math abs alphachange < 0 01 newalpha = alphas i j + alphachange 2 newalpha = alphas i j + alphachange alphachange < 0 alpha max i j > alphas i j out updating alpha max +i+ +j+ = +alphas i j alpha max i j = alphas i j alphachange > 0 alpha min i j < alphas i j out updating alpha min +i+ +j+ = +alphas i j alpha min i j = alphas i j newalpha <= alpha max i j newalpha >= alpha min i j newton wants to jump to a point inside boundaries let it alphas i j = newalpha newton wants to jump to a point outside boundaries bisect instead alpha max i j != p o s i t i v e i n f i n i t y alpha min i j != n e g a t i v e i n f i n i t y alphas i j = alpha min i j + alpha max i j alpha min i j 2 out newton tried to exceed bounds bisecting dalphas +i+ +j+ = +dalphas i j + alpha min= +alpha min i j + alpha max= +alpha max i j alphachange = alphas i j oldalpha math abs alphachange > max alphachange max alphachange = math abs alphachange math abs dalphas i j > max dalpha max dalpha = math abs dalphas i j alpha change old i j = alphachange logger info after +newton+ newton iterations maximum alphachange= +max alphachange+ dalpha= +max dalpha allow some memory to be freed q = ddalphas = dalphas = alpha change old = alpha min = alpha max = q e^ \alpha g p 4 out calculating qeag note that we are using a gaussian prior so we t multiply 1 num instances qeag = num classes num features i = 0 i < ilist size i++ classifications i get label alphabet == ilist get target alphabet instance inst = ilist get i labeling labeling = inst get labeling feature vector fv = feature vector inst get data fv max location = fv num locations 1 = 0 < num classes li++ model label weight = classifications i value following line now done before outside loop over instances fi = 0 fi < num features fi++ qeag fi += model label weight 1 0 fl = 0 fl < fv num locations fl++ fli = fv index at location fl when value feature g zero a value 1 0 should be in expectation we ll actually add all these at end pre assuming that all features have value zero here we subtract assumed model label weight and put in value based on non zero valued feature g qeag fli += math log model label weight math exp alphas fli + 1 model label weight out calculating klgain values klgains = num features klgain incr alpha i = 0 i < num classes i++ j = 0 j < num features j++ ! infinite alphas i j alpha = alphas i j alpha == 0 klgain incr = alpha p i j qeag i j alpha alpha 2 gaussian prior variance klgain incr < 0 logger info w a r n i n g klgain incr +i+ +j+ = +klgain incr+ alpha= +alphas i j + feature= +ilist get data alphabet lookup j + class= +ilist get target alphabet lookup i klgains j += klgain incr logger info klgains length= +klgains length j = 0 j < num features j++ j % num features 100 == 0 i = 0 i < num classes i++ logger info c= +i+ p +ilist get data alphabet lookup j + = +p i j logger info c= +i+ q +ilist get data alphabet lookup j + = +q i j logger info c= +i+ alphas +ilist get data alphabet lookup j + = +alphas i j logger info c= +i+ qeag +ilist get data alphabet lookup j + = +qeag i j logger info klgains +ilist get data alphabet lookup j + = +klgains j klgains exp gain instance list ilist label vector classifications gaussian prior variance ilist get data alphabet calc exp gains ilist classifications gaussian prior variance label vector get label vectors from classifications classification label vector ret = label vector length i = 0 i < length i++ ret i = i get label vector ret exp gain instance list ilist classification classifications gaussian prior variance ilist get data alphabet calc exp gains ilist get label vectors from classifications classifications gaussian prior variance factory ranked feature vector factory label vector classifications gaussian prior variance = 10 0 factory label vector classifications classifications = classifications factory label vector classifications gaussian prior variance classifications = classifications gaussian prior variance = gaussian prior variance ranked feature vector ranked feature vector instance list ilist ilist get target alphabet == classifications 0 get alphabet exp gain ilist classifications gaussian prior variance serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write classifications length i = 0 i < classifications length i++ out write classifications i read input stream in i o not found = in read n = in read classifications = label vector n i = 0 i < n i++ classifications i = label vector in read 