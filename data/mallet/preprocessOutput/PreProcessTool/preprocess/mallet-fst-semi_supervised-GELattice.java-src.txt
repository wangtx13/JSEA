2010 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e semi supervised list r f transducer semi supervised constraints g e constraint types feature vector types feature vector sequence types log number runs dynamic programming algorithm mann and mc callum 08 computing gradient a generalized expectation constraint that considers a single label a linear chain r f generalized expectation criteria semi supervised conditional random fields gideon mann and andrew mc callum a l 2008 gdruck n o t e g e lattice that computes gradient all constraints simultaneously! author gregory druck author gaurav chandalia author gideon mann g e lattice input length + 1 lattice length model transducer transducer number states in f s t num states dynamic programming lattice lattice node lattice cache dot produce between violation and constraint features log number dot cache fvs input feature vector sequence gammas marginals over single states xis marginals over pairs states transducer transducer reverse trans source state indices each destination state reverse trans indices transition indices each destination state gradient gradient to increment constraints list constraints check whether to run debugging test to verify correctness will be much slower g e lattice feature vector sequence fvs gammas xis transducer transducer reverse trans reverse trans indices r f factors gradient list< g e constraint> constraints check gradient != lattice length = fvs size + 1 transducer = transducer num states = transducer num states lattice lattice = lattice node lattice length num states ip = 0 ip < lattice length ++ip a = 0 a < num states ++a lattice ip a = lattice node dot cache = log number lattice length num states num states t o d o maybe should be cached? separate lists constraints that look at one vs two states list< g e constraint> constraints1 = list< g e constraint> list< g e constraint> constraints2 = list< g e constraint> g e constraint constraint constraints constraint one state constraint constraints1 add constraint constraints2 add constraint r f crf = r f transducer dot ex = run forward crf constraints1 constraints2 gammas xis reverse trans fvs run backward crf gammas xis reverse trans reverse trans indices fvs dot ex gradient check constraints gammas xis fvs run forward pass dynamic programming algorithm crf r f constraints1 constraints that consider one state constraints2 constraints that consider two states gammas marginals over single states xis marginals over pairs states reverse trans source state indices each destination state fvs input feature vector sequence run forward r f crf list< g e constraint> constraints1 list< g e constraint> constraints2 gammas xis reverse trans feature vector sequence fvs dot ex = 0 log number one state value cache = log number num states log number nu alpha = log number transducer i m p o s s i b l e w e i g h t log number temp = log number transducer i m p o s s i b l e w e i g h t ip = 0 ip < lattice length 1 ++ip feature vector fv = fvs get ip speed things up giving constraints an opportunity to cache example which constrained input features appear in feature vector g e constraint constraint constraints1 constraint pre process fv g e constraint constraint constraints2 constraint pre process fv one state val computed = num states prev = 0 prev < num states prev++ nu alpha set transducer i m p o s s i b l e w e i g h t ip != 0 prev prevs = reverse trans prev calculate only once \sum y i 1 w a y i 1 y i ppi = 0 ppi < prev prevs length ppi++ nu alpha plus equals lattice ip 1 prev prevs ppi alpha prev ! na n nu alpha log val r f state prev state = r f state crf get state prev lattice node node = lattice ip prev xi = xis ip prev gamma = gammas ip prev ci = 0 ci < prev state num destinations ci++ curr = prev state get destination state ci get index dot = 0 g e constraint constraint constraints2 dot += constraint get composite constraint feature value fv ip prev curr avoid recomputing one state constraint features #labels times !one state val computed curr os val = 0 g e constraint constraint constraints1 os val += constraint get composite constraint feature value fv ip prev curr os val < 0 dot ex += math exp gammas ip+1 curr os val one state value cache curr = log number math log os val os val > 0 dot ex += math exp gammas ip+1 curr os val one state value cache curr = log number math log os val one state value cache curr = one state val computed curr = combine one and two state constraint feature values dot == 0 one state value cache curr == dot cache ip prev curr = dot == 0 one state value cache curr != dot cache ip prev curr = one state value cache curr dot ex += math exp xi curr dot dot < 0 dot cache ip prev curr = log number math log dot dot cache ip prev curr = log number math log dot one state value cache curr != dot cache ip prev curr plus equals one state value cache curr update dynamic programming table dot cache ip prev curr != temp set xi curr temp times equals dot cache ip prev curr node alpha curr plus equals temp gamma == transducer i m p o s s i b l e w e i g h t node alpha curr = log number transducer i m p o s s i b l e w e i g h t temp set xi curr gamma temp times equals nu alpha node alpha curr plus equals temp ! na n node alpha curr log val xi + xi curr + gamma + gamma + constraint feature + dot cache ip prev curr + nu apha + nu alpha + dot + dot dot ex run backward pass dynamic programming algorithm crf r f gammas marginals over single states xis marginals over pairs states reverse trans source state indices each destination state reverse trans indices transition indices each destination state fvs input feature vector sequence dot ex expectation constraint features dot violation gradient gradient to increment run backward r f crf gammas xis reverse trans reverse trans indices feature vector sequence fvs dot ex r f factors gradient log number nu beta = log number transducer i m p o s s i b l e w e i g h t log number dot = log number transducer i m p o s s i b l e w e i g h t log number temp = log number transducer i m p o s s i b l e w e i g h t log number temp2 = log number transducer i m p o s s i b l e w e i g h t log number next dot ip = lattice length 2 ip >= 0 ip curr = 0 curr < num states ++curr nu beta set transducer i m p o s s i b l e w e i g h t dot set transducer i m p o s s i b l e w e i g h t calculate only once \sum y i+1 w b y i y+i r f state curr state = r f state crf get state curr ni = 0 ni < curr state num destinations ni++ next= curr state get destination state ni get index nu beta plus equals lattice ip+1 curr beta next ! na n nu beta log val next dot = dot cache ip+1 curr next next dot != xi = xis ip+1 curr next temp set xi temp times equals next dot dot plus equals temp gamma = gammas ip+1 curr prev states = reverse trans curr pi = 0 pi < prev states length pi++ prev = prev states pi r f state crf state = r f state crf get state prev lattice node node = lattice ip prev xi = xis ip prev curr gamma == transducer i m p o s s i b l e w e i g h t node beta curr = log number transducer i m p o s s i b l e w e i g h t constraint feature values cached in forward pass temp set dot log val dot sign temp plus equals nu beta temp2 set xi gamma temp times equals temp2 node beta curr plus equals temp ! na n node beta curr log val xi + xi + gamma + gamma + xi + xi + log indicator feat + dot cache ip curr compute and update gradient! trans prob = math exp xi cov first term = node alpha curr exp + node beta curr exp contribution = cov first term trans prob dot ex nwi = crf state get weight names reverse trans indices curr pi length weights index wi = 0 wi < nwi wi++ weights index = r f transducer get weights index crf state get weight names reverse trans indices curr pi wi gradient weights weights index plus equals sparse fvs get ip contribution gradient weights weights index += contribution verifies correctness lattice computations check list< g e constraint> constraints gammas xis feature vector sequence fvs sum marginal probabilities ex1 = 0 0 ip = 0 ip < lattice length 1 ++ip si1 = 0 si1 < num states si1++ si2 = 0 si2 < num states si2++ dot = 0 g e constraint constraint constraints dot += constraint get composite constraint feature value fvs get ip ip si1 si2 prob = math exp xis ip si1 si2 ex1 += prob dot ex2 = 0 0 ip = 0 ip < lattice length 1 ++ip ex3 = 0 0 s1 = 0 s1 < num states ++s1 lattice node node = lattice ip s1 s2 = 0 s2 < num states ++s2 ex3 += node alpha s2 exp + node beta s2 exp should be equal to marginal prob ex1 ex3 < 1e 6 ex1 + + ex3 ex2 += ex3 ex2 = ex2 lattice length 1 should be equal to marginal prob ex1 ex2 < 1e 6 ex1 + + ex2 log number get alpha ip s1 s2 lattice ip s1 alpha s2 log number get beta ip s1 s2 lattice ip s1 beta s2 contains forward backward vectors correspoding to an input position and a state index lattice node ip > input position a vector doubles since each node we need to keep track alpha beta values state ip+1 log number alpha log number beta lattice node alpha = log number num states beta = log number num states si = 0 si < num states ++si alpha si = log number transducer i m p o s s i b l e w e i g h t beta si = log number transducer i m p o s s i b l e w e i g h t 