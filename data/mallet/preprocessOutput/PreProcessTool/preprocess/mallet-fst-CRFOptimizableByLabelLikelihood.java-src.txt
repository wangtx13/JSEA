io i o io input stream io output stream io serializable bit set logging logger types feature sequence types feature vector sequence types instance types instance list types matrix ops optimize optimizable logger an objective function r fs that label likelihood plus a gaussian or hyperbolic prior on r f optimizable label likelihood optimizable gradient value serializable logger logger = logger get logger r f optimizable label likelihood get name d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 0 d e f a u l t h y p e r b o l i p r i o r s l o p e = 0 2 d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s = 10 0 gsc changing field access to make extensible instance list training set cached value = 123456789 cached gradient bit set infinite values = r f crf r f factors constraints expectations various values from r f acting indicators when we need to cached value weights stamp = 1 re calculate expectations and values to get value because weights values changed cached gradient weights stamp = 1 re calculate to get value gradient because weights values changed using hyperbolic prior = gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e hyperbolic prior slope = d e f a u l t h y p e r b o l i p r i o r s l o p e hyperbolic prior sharpness = d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s r f optimizable label likelihood r f crf instance list ilist set up crf = crf training set = ilist cached gradient = dense vector num cached gradient = crf get num factors constraints = r f factors crf expectations = r f factors crf resets and values that may have been in expectations and constraints reallocate sufficient statistics unfortunately necessary b cached value cached value stale not in same place! cached value weights stamp = 1 cached gradient weights stamp = 1 gather constraints ilist gather constraints instance list ilist set constraints running forward backward output label sequence thus restricting it to only those paths that agree label sequence zero constraints reset constraints to zero before we fill them again constraints structure matches crf constraints zero instance instance ilist feature vector sequence input = feature vector sequence instance get data feature sequence output = feature sequence instance get target instance weight = ilist get instance weight instance out constraint gathering on instance +i+ +ilist size transducer incrementor incrementor = instance weight == 1 0 ? constraints incrementor constraints weighted incrementor instance weight sum lattice crf input output incrementor out testing value and gradient test optimizable test value and gradient current t o d o move these implementations into r f and put here stubs that call them! get num crf get num factors get buffer crf get buffer get parameter index crf get parameter index set buff crf set buff crf weights value changed set parameter index value crf set parameter index value crf weights value changed log probability training sequence labels and fill in expectations get expectation value instance values must either always or never be in total values we can t just sometimes skip a value because it infinite off total values initializing infinite values = value = 0 infinite values == infinite values = bit set initializing infinite values = reset expectations to zero before we fill them again expectations structure matches crf expectations zero count number instances that have infinite weight num inf labeled weight = 0 num inf unlabeled weight = 0 num inf weight = 0 calculate value each instance and also fill in expectations unlabeled weight labeled weight weight ii = 0 ii < training set size ii++ instance instance = training set get ii instance weight = training set get instance weight instance feature vector sequence input = feature vector sequence instance get data feature sequence output = feature sequence instance get target labeled weight = sum lattice crf input output transducer incrementor get total weight instance name = instance get name == ? instance# +ii instance get name to out labeled weight = +labeled weight infinite labeled weight ++num inf labeled weight logger warning instance name + has infinite labeled weight + instance get source != ? instance get source transducer incrementor incrementor = instance weight == 1 0 ? expectations incrementor expectations weighted incrementor instance weight unlabeled weight = sum lattice crf input incrementor get total weight out unlabeled weight = +unlabeled weight infinite unlabeled weight ++num inf unlabeled weight logger warning instance get name to + has infinite unlabeled weight + instance get source != ? instance get source here weight log conditional probability correct label sequence weight = labeled weight unlabeled weight out instance +ii+ r f maximizable r f get weight = +weight infinite weight ++num inf weight logger warning instance name + has infinite weight skipping initializing infinite values infinite values set ii !infinite values get ii illegal state instance i used to have non infinite value but now it has infinite value weights are log probabilities and we want to a log probability value += weight instance weight num inf labeled weight > 0 || num inf unlabeled weight > 0 || num inf weight > 0 logger warning number instances + infinite labeled weight + num inf labeled weight + + infinite unlabeled weight + num inf unlabeled weight + + infinite weight + num inf weight value log probability training sequence labels and prior over get value crf weights value change stamp != cached value weights stamp cached value not up to date it was calculated a different set r f weights cached value weights stamp = crf weights value change stamp cached value will soon no longer be stale starting time = current time millis crf print get value all all labels also filling in expectations at same time cached value = get expectation value incorporate prior on using hyperbolic prior hyperbolic prior cached value += crf hyberbolic prior hyperbolic prior slope hyperbolic prior sharpness gaussian prior cached value += crf gaussian prior gaussian prior variance gsc make sure prior gives a correct value ! na n cached value || infinite cached value label likelihood na n infinite logger info get value loglikelihood optimizable label likelihood = +cached value ending time = current time millis logger fine inference milliseconds = + ending time starting time cached value gsc changing from not na n to not na n or infinite not na n or infinite crf are allowed to have infinite values crf not na n expectations not na n or infinite constraints not na n or infinite get value gradient buffer prior gradient parameter gaussian prior variance gradient constraint expectation + prior gradient == expectation constraint prior gradient gradient points up hill i e in direction higher value cached gradient weights stamp != crf weights value change stamp cached gradient weights stamp = crf weights value change stamp cached gradient will soon no longer be stale will fill in expectation updating it necessary get value not na n or infinite gradient constraints expectations + prior we expectations constraints prior expectations plus equals constraints 1 0 using hyperbolic prior expectations plus equals hyperbolic prior gradient crf hyperbolic prior slope hyperbolic prior sharpness expectations plus equals gaussian prior gradient crf gaussian prior variance expectations not na n or infinite expectations get cached gradient matrix ops times equals cached gradient 1 0 in above comment xxx show feature maximum gradient t o d o something like negation still necessary????? up to now we ve been calculating weight gradient take opposite to get value gradient cached gradient times equals 1 0 point uphill what heck was this!? buffer length != num buffer = num arraycopy cached gradient 0 buffer 0 cached gradient length arrays fill buffer 0 0 arraycopy cached gradie 0 buffer 0 2 crf initial weights length t o d o now just copy state inital weights gsc adding these get set prior set use hyperbolic prior f using hyperbolic prior = f set hyperbolic prior slope p hyperbolic prior slope = p set hyperbolic prior sharpness p hyperbolic prior sharpness = p get use hyperbolic prior slope hyperbolic prior slope get use hyperbolic prior sharpness hyperbolic prior sharpness set gaussian prior variance p gaussian prior variance = p get gaussian prior variance gaussian prior variance serialization maximizable r f serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write training set out write cached value out write cached gradient out write infinite values out write crf read input stream in i o not found = in read training set = instance list in read cached value = in read cached gradient = in read infinite values = bit set in read crf = r f in read factory optimizable gradient value r f optimizable r f crf instance list training data r f optimizable label likelihood crf training data 