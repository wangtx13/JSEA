topics list arrays io types randoms gnu trove hierarchical l d a instance list instances instance list testing n r p node root node node num levels num documents num types alpha smoothing on topic distributions gamma imaginary customers at next yet unused table eta smoothing on word distributions eta sum levels indexed < doc token > n r p node document leaves currently selected path ie leaf node through n r p tree total nodes = 0 state = hlda state randoms random show progress = display topics interval = 50 num words to display = 10 hierarchical l d a alpha = 10 0 gamma = 1 0 eta = 0 1 set alpha alpha alpha = alpha set gamma gamma gamma = gamma set eta eta eta = eta set state state state = state set topic display interval words display topics interval = interval num words to display = words parameter determines whether sampler outputs shows progress outputting a character after every iteration set progress display show progress show progress = show progress initialize instance list instances instance list testing num levels randoms random instances = instances testing = testing num levels = num levels random = random ! instances get 0 get data feature sequence illegal argument input must be a feature sequence using feature sequence option when impoting data example num documents = instances size num types = instances get data alphabet size eta sum = eta num types initialize a single path n r p node path = n r p node num levels root node = n r p node num types levels = num documents document leaves = n r p node num documents initialize and fill topic pointer arrays every document set everything to single path that we added earlier doc=0 doc < num documents doc++ feature sequence fs = feature sequence instances get doc get data seq len = fs get length path 0 = root node root node customers++ level = 1 level < num levels level++ path level = path level 1 select path level customers++ node = path num levels 1 levels doc = seq len document leaves doc = node token=0 token < seq len token++ type = fs get index at position token levels doc token = random next num levels node = path levels doc token node total tokens++ node type counts type ++ estimate num iterations iteration = 1 iteration <= num iterations iteration++ doc=0 doc < num documents doc++ sample path doc iteration doc=0 doc < num documents doc++ sample topics doc show progress out print iteration % 50 == 0 out + iteration iteration % display topics interval == 0 print nodes sample path doc iteration n r p node path = n r p node num levels n r p node node level token type topic count weight node = document leaves doc level = num levels 1 level >= 0 level path level = node node = node parent document leaves doc drop path t hash map< n r p node> node weights = t hash map< n r p node> calculate p m | m calculate n r p node weights root node 0 0 add weights p w m | w m z path may have no further customers and therefore be unavailable but it should still since we haven t reset document leaves doc yet t hash map type counts = t hash map num levels doc levels level = 0 level < num levels level++ type counts level = t hash map doc levels = levels doc feature sequence fs = feature sequence instances get doc get data save counts every word at each level and remove counts from current path token = 0 token < doc levels length token++ level = doc levels token type = fs get index at position token ! type counts level contains key type type counts level put type 1 type counts level increment type path level type counts type path level type counts type >= 0 path level total tokens path level total tokens >= 0 calculate weight a path at a given level topic weights = num levels level = 1 level < num levels level++ skip root types = type counts level keys total tokens = 0 t types i=0 i<type counts level get t i++ topic weights level += math log eta + i eta sum + total tokens total tokens++ iteration > 1 out topic weights level calculate word likelihood node weights root node 0 0 type counts topic weights 0 iteration n r p node nodes = node weights keys n r p node weights = nodes length sum = 0 0 max = n e g a t i v e i n f i n i t y to avoid underflow we re using log weights and normalizing node weights so that largest weight always 1 i=0 i<nodes length i++ node weights get nodes i > max max = node weights get nodes i i=0 i<nodes length i++ weights i = math exp node weights get nodes i max iteration > 1 nodes i == document leaves doc out print out n r p node nodes i level + + weights i + + node weights get nodes i sum += weights i iteration > 1 out node = nodes random next discrete weights sum we have picked an internal node we need to add a path ! node leaf node = node get leaf node add path document leaves doc = node level = num levels 1 level >= 0 level types = type counts level keys t types node type counts t += type counts level get t node total tokens += type counts level get t node = node parent calculate n r p t hash map< n r p node> node weights n r p node node weight n r p node child node children calculate n r p node weights child weight + math log child customers node customers + gamma node weights put node weight + math log gamma node customers + gamma calculate word likelihood t hash map< n r p node> node weights n r p node node weight t hash map type counts topic weights level iteration first calculate likelihood words at level given topic node weight = 0 0 types = type counts level keys total tokens = 0 iteration > 1 out level + + node weight type types i=0 i<type counts level get type i++ node weight += math log eta + node type counts type + i eta sum + node total tokens + total tokens total tokens++ iteration > 1 out +eta + + + node type counts type + + + i + + + eta sum + + + node total tokens + + + total tokens + + + node weight iteration > 1 out level + + node weight propagate that weight to child nodes n r p node child node children calculate word likelihood node weights child weight + node weight type counts topic weights level + 1 iteration an internal node add weight a path level++ level < num levels node weight += topic weights level level++ node weights adjust value node node weight propagate a topic weight to a node and all its children weight assumed to be a log propagate topic weight t hash map< n r p node> node weights n r p node node weight ! node weights contains key node calculating n r p prior proceeds from root down ie following child links but adding word topic weights comes from bottom up following parent links and then child links it s possible that leaf node may have been removed just prior to round so current node may not have an n r p weight so it s not going to be sampled anyway so ditch it n r p node child node children propagate topic weight node weights child weight node weights adjust value node weight sample topics doc feature sequence fs = feature sequence instances get doc get data seq len = fs get length doc levels = levels doc n r p node path = n r p node num levels n r p node node level counts = num levels type token level sum get leaf node = document leaves doc level = num levels 1 level >= 0 level path level = node node = node parent level weights = num levels initialize level counts token = 0 token < seq len token++ level counts doc levels token ++ token = 0 token < seq len token++ type = fs get index at position token level counts doc levels token node = path doc levels token node type counts type node total tokens sum = 0 0 level=0 level < num levels level++ level weights level = alpha + level counts level eta + path level type counts type eta sum + path level total tokens sum += level weights level level = random next discrete level weights sum doc levels token = level level counts doc levels token ++ node = path level node type counts type ++ node total tokens++ writes current sampling state to specified in <code>state file< code> print state i o not found print state print writer buffered writer writer state write a text describing current sampling state print state print writer out i o doc = 0 alphabet alphabet = instances get data alphabet instance instance instances feature sequence fs = feature sequence instance get data seq len = fs get length doc levels = levels doc n r p node node type token level buffer path = buffer start leaf and build a describing path doc node = document leaves doc level = num levels 1 level >= 0 level path append node node + node = node parent token = 0 token < seq len token++ type = fs get index at position token level = doc levels token just tells we re not trying to add a and an out path + + type + + alphabet lookup type + + level + doc++ print nodes print node root node 0 print nodes weight print node root node 0 weight print node n r p node node indent weight buffer out = buffer i=0 i<indent i++ out append out append node total tokens + + node customers + out append node get top words num words to display weight out out n r p node child node children print node child indent + 1 weight use empirical likelihood evaluation sample a path through tree then sample a multinomial over topics in that path then a weighted sum words empirical likelihood num samples instance list testing n r p node path = n r p node num levels n r p node node weight path 0 = root node feature sequence fs sample level type token doc seq len dirichlet dirichlet = dirichlet num levels alpha level weights multinomial = num types likelihoods = testing size num samples sample = 0 sample < num samples sample++ arrays fill multinomial 0 0 level = 1 level < num levels level++ path level = path level 1 select existing level weights = dirichlet next type = 0 type < num types type++ level = 0 level < num levels level++ node = path level multinomial type += level weights level eta + node type counts type eta sum + node total tokens type = 0 type < num types type++ multinomial type = math log multinomial type doc=0 doc<testing size doc++ fs = feature sequence testing get doc get data seq len = fs get length token = 0 token < seq len token++ type = fs get index at position token likelihoods doc sample += multinomial type average log likelihood = 0 0 log num samples = math log num samples doc=0 doc<testing size doc++ max = n e g a t i v e i n f i n i t y sample = 0 sample < num samples sample++ likelihoods doc sample > max max = likelihoods doc sample sum = 0 0 sample = 0 sample < num samples sample++ sum += math exp likelihoods doc sample max average log likelihood += math log sum + max log num samples average log likelihood primarily testing purposes link topics tui hierarchical l d a t u i has a more flexible command line use instance list instances = instance list load 0 instance list testing = instance list load 1 hierarchical l d a sampler = hierarchical l d a sampler initialize instances testing 5 randoms sampler estimate 250 e e print stack trace n r p node customers list< n r p node> children n r p node parent level total tokens type counts node n r p node n r p node parent dimensions level customers = 0 parent = parent children = list< n r p node> level = level out node at level + level total tokens = 0 type counts = dimensions node = total nodes total nodes++ n r p node dimensions dimensions 0 n r p node add child n r p node node = n r p node type counts length level + 1 children add node node leaf level == num levels 1 n r p node get leaf n r p node node = l=level l<num levels 1 l++ node = node add child node drop path n r p node node = node customers node customers == 0 node parent remove node l = 1 l < num levels l++ node = node parent node customers node customers == 0 node parent remove node remove n r p node node children remove node add path n r p node node = node customers++ l = 1 l < num levels l++ node = node parent node customers++ n r p node select existing weights = children size i = 0 n r p node child children weights i = child customers gamma + customers i++ choice = random next discrete weights children get choice n r p node select weights = children size + 1 weights 0 = gamma gamma + customers i = 1 n r p node child children weights i = child customers gamma + customers i++ choice = random next discrete weights choice == 0 add child children get choice 1 get top words num words weight sorter sorted types = sorter num types type=0 type < num types type++ sorted types type = sorter type type counts type arrays sort sorted types alphabet alphabet = instances get data alphabet buffer out = buffer i = 0 i < num words i++ weight out append alphabet lookup sorted types i get + + sorted types i get weight + out append alphabet lookup sorted types i get + out to 