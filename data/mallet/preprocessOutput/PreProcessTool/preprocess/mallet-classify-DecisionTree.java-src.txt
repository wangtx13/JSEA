2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e classify io serializable logging logger pipe pipe types alphabet types augmentable feature vector types feature selection types feature vector types info gain types instance types instance list types labeling logger decision tree classifier author andrew mc callum <a href= mailto >mccallum edu< a> decision tree classifier serializable induce features serial u = 1 l logger logger = logger get logger decision tree get name node root decision tree pipe instance pipe decision tree node root instance pipe root = root node get root root node get leaf node node feature vector fv node child0 == node fv value node feature index != 0 get leaf node child1 fv get leaf node child0 fv classification classify instance instance feature vector fv = feature vector instance get data instance pipe == || fv get alphabet == instance pipe get data alphabet node leaf = get leaf root fv classification instance leaf labeling entropy 1 0 would say that it take one bit to indicate correct e g that there a 50 50 split between two classes given a particular feature add features entropy threshold = 0 7 induce features instance list ilist feature shrinkage induce per features induce per features num classes = ilist get target alphabet size num features = ilist get data alphabet size feature selection pcfs = feature selection num classes j = 0 j < num classes j++ pcfs j = feature selection ilist get per label feature selection j clone i = 0 i < ilist size i++ data = ilist get i get data augmentable feature vector afv = augmentable feature vector data root induce features afv pcfs ilist get feature selection ilist get per label feature selection feature shrinkage induce per features add features entropy threshold unsupported operation not yet node serializable serial u = 1 l feature index feature on which children would distinguish info gain gain splitting on feature instance list ilist alphabet dictionary label entropy label entropy data in unsplit node labeling labeling label in node unsplit node parent child0 child1 name xxx also calculate some sort inverted entropy feature induction in order to find one needs a feature a negative weight node instance list ilist node parent feature selection fs info gain ig = info gain ilist feature index = ig get max valued index in fs info gain = ig value feature index ilist = ilist dictionary = ilist get data alphabet parent = parent labeling = ig get base label label entropy = ig get base entropy child0 = child1 = root has depth zero depth depth = 0 node p = parent p != p = p parent depth++ depth leaf child0 == child1 == root parent == node get feature absent child child0 node get feature present child child1 get split info gain info gain get split feature ilist get data alphabet lookup feature index split feature selection fs ilist == illegal state frozen cannot split instance list ilist0 = instance list ilist get pipe instance list ilist1 = instance list ilist get pipe i = 0 i < ilist size i++ instance instance = ilist get i feature vector fv = feature vector instance get data xxx what test should be? what to negative values? whatever decided here should also go in info gain calc info gains fv value feature index != 0 out list1 add +instance get uri + weight= +ilist get instance weight i ilist1 add instance ilist get instance weight i out list0 add +instance get uri + weight= +ilist get instance weight i ilist0 add instance ilist get instance weight i logger info child0= +ilist0 size + child1= +ilist1 size child0 = node ilist0 fs child1 = node ilist1 fs saves memory allowing ilist to be garbage collected stop growth child0 != child0 stop growth child1 stop growth ilist = induce features augmentable feature vector afv feature selection features already there feature selection per features already there feature selection feature selection feature selection per feature selection interior nodes add per features entropy threshold !is root leaf || interior nodes label entropy < entropy threshold name = get name logger info trying to add feature +name conjunction index = afv get alphabet lookup index name add per features index = labeling get best index !per features already there index contains name afv add name 1 0 per feature selection index add name unsupported operation not yet feature present = afv value feature index != 0 child0 != !feature present child0 induce features afv features already there per features already there feature selection per feature selection interior nodes add per features entropy threshold child1 != feature present child1 induce features afv features already there per features already there feature selection per feature selection interior nodes add per features entropy threshold get name prefix parent == root parent parent == parent get feature present child == dictionary lookup parent feature index to dictionary != dictionary lookup parent feature index != ! + dictionary lookup parent feature index to parent get feature present child == parent get name + + dictionary lookup parent feature index to parent get name + ! + dictionary lookup parent feature index to print child0 == out get name + + labeling get best label child0 print child1 print 