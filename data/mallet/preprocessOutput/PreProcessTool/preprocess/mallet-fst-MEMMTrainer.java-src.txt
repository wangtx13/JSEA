bit set logging logger types feature sequence types feature vector types feature vector sequence types instance types instance list m e m m state m e m m transition iterator optimize limited memory b f g s optimize optimizable optimize optimizer logger trains and evaluates a link m e m m m e m m trainer transducer trainer logger logger = logger get logger m e m m trainer get name m e m m memm gathering training data = after training sets have been gathered in states record which instance list we ve gathers so we t count instances instance list training gathered gsc user supposed to set weights manually so flag not needed use sparse weights = m e m m optimizable label likelihood omemm m e m m trainer m e m m memm memm = memm m e m m optimizable label likelihood get optimizable m e m m instance list training set m e m m optimizable label likelihood memm training set m e m m trainer set use sparse weights f use sparse weights = f trains a m e m m until convergence train instance list training train training m a x v a l u e trains a m e m m specified number iterations or until convergence whichever occurs first training converged within specified iterations train instance list training num iterations num iterations <= 0 training size > 0 allocate space and place transition feature vectors in per source state instance lists here gathering training sets will be and these will result in instance list s being created in each source state and feature vectors their outgoing transitions to be added to them data field in instances training gathered != training gather training sets training gsc user has to set weights manually use sparse weights memm set weights dimension in training memm set weights dimension densely expectation based placement training data would go here i = 0 i < training size i++ instance instance = training get i feature vector sequence input = feature vector sequence instance get data feature sequence output = feature sequence instance get target it paths consistent labels gathering constraints = sum lattice input output and also it paths selected current model so we will get some negative weights gathering constraints = some training done once some training done sum lattice input gathering weights present = sparse vector weights = sparse vector weights length i = 0 i < weights length i++ num locations = weights present i cardinality logger info r f weights +weight alphabet lookup i + num features = +num locations indices = num locations j = 0 j < num locations j++ indices j = weights present i next set bit j == 0 ? 0 indices j 1 +1 out r f4 has index +indices j weights i = indexed sparse vector indices num locations num locations num locations weights i plus equals sparse weights i weights = weights omemm = m e m m optimizable label likelihood memm training gather constraints omemm gather expectations or constraints optimizer maximizer = limited memory b f g s omemm i training = converged = logger info r f about to train +num iterations+ iterations i = 0 i < num iterations i++ converged = maximizer optimize 1 logger info r f finished one iteration maximizer i= +i run evaluators illegal argument e e print stack trace logger info catching saying converged converged = converged logger info r f training has converged i= +i logger info about to set trainable converged gather training sets instance list training training gathered != it would be easy enough to support just go through all states and set training set to unsupported operation training multiple sets not supported training gathered = training i = 0 i < training size i++ instance instance = training get i feature vector sequence input = feature vector sequence instance get data feature sequence output = feature sequence instance get target it paths consistent labels sum lattice memm input output transducer incrementor increment state transducer state s count increment initial state transducer state s count increment transition transducer transition iterator ti count m e m m state source = m e m m state ti get source state count != 0 create source state s training set it doesn t yet source training set == instance list a pipe because it doesn t any processing input source training set = instance list t o d o we should make sure we t add duplicates through a second call to set weights dimenstion ! t o d o note that when training data still allows ambiguous outgoing transitions will add same f v more than once to source state s training set each >1 0 weight not incorrect but inefficient out from +source get name + > +get output + +get input source training set add instance ti get input ti get output count not yet unsupported operation train instance list training instance list validation instance list testing transducer evaluator eval num iterations num iterations per proportion training proportions unsupported operation not yet unsupported operation train feature induction instance list training data instance list validation data instance list testing data transducer evaluator eval num iterations num iterations between feature inductions num feature inductions num features per feature induction label prob threshold clustered feature induction training proportions gain name unsupported operation print instance lists i = 0 i < memm num states i++ state state = state memm get state i instance list training = state training set out state +i+ +state get name training == out no data j = 0 j < training size j++ instance inst = training get j out from +state get name + to +inst get target out instance +j out inst get target out inst get data represents in objective function <p> weights are trained matching expectations model to observations gathered from data suppress warnings serial m e m m optimizable label likelihood r f optimizable label likelihood optimizable gradient value bit set infinite values = m e m m optimizable label likelihood m e m m memm instance list training data memm training data expectations = r f factors memm constraints = r f factors memm constraints=false log probability training labels gather expectations or constraints gather constraints instance values must either always or never be in total values we can t just sometimes skip a value because it infinite off total values initializing infinite values = r f factors factors = gather constraints ? constraints expectations r f factors incrementor factor incrementor = factors incrementor infinite values == infinite values = bit set initializing infinite values = label log prob = 0 i = 0 i < memm num states i++ m e m m state s = state memm get state i s training set == out empty training set state +s name j = 0 j < s training set size j++ instance instance = s training set get j inst weight = s training set get instance weight j feature vector fv = feature vector instance get data label = instance get target transition iterator iter = transition iterator s fv gather constraints?label memm iter has next gsc iter next state advance iterator state destination = m e m m state iter next state just to advance iterator weight = iter get weight factor incrementor increment transition iter math exp weight inst weight iter increment count math exp weight inst weight !gather constraints iter get output == label ! infinite weight label log prob += inst weight weight xxx ????? logger warning state +i+ transition +j+ has infinite cost skipping initializing infinite values illegal state infinite cost transitions not yet supported infinite values set j !infinite values get j illegal state instance i used to have non infinite value + but now it has infinite value force initial weight to 0 making sure that whether factor refers to expectation or constraint they have same value i = 0 i < memm num states i++ factors initial weights i = 0 0 factors weights i = 0 0 label log prob log probability training sequence labels and fill in expectations get expectation value gather expectations or constraints override get iteration t o d o auto generated stub 0 override transducer get transducer memm override finished training t o d o auto generated stub 