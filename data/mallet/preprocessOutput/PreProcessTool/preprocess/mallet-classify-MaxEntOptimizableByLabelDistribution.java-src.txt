classify io serializable arrays iterator logging logger optimize limited memory b f g s optimize optimizable types alphabet types feature selection types feature vector types instance types instance list types label alphabet types labeling types matrix ops logger progress message logger maths max ent optimizable label optimizable gradient value serializable t o d o needs to be done? logger logger = logger get logger max ent optimizable label get name logger progress logger = progress message logger get logger max ent optimizable label get name + pl xxx why does test maximizable fail when variance very small? d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 0 d e f a u l t m a x i m i z e r l a s s = limited memory b f g s gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e maximizer = d e f a u l t m a x i m i z e r l a s s constraints cached gradient max ent classifier instance list training list expectations are temporarily stored in cached gradient cached value cached value stale cached gradient stale num labels num features feature index just clarity feature selection feature selection feature selection per label feature selection num get value calls = 0 num get value gradient calls = 0 max ent optimizable label max ent optimizable label instance list training set max ent initial classifier training list = training set alphabet fd = training set get data alphabet label alphabet ld = label alphabet training set get target alphabet t fd stop growth because someone might want to feature induction ld stop growth add one feature feature num labels = ld size num features = fd size + 1 feature index = num features 1 = num labels num features constraints = num labels num features cached gradient = num labels num features arrays fill 0 0 arrays fill constraints 0 0 arrays fill cached gradient 0 0 feature selection = training set get feature selection per label feature selection = training set get per label feature selection add feature index to selection feature selection != feature selection add feature index per label feature selection != i = 0 i < per label feature selection length i++ per label feature selection i add feature index xxx later change to allow both to be set but select which one to use a flag? feature selection == || per label feature selection == initial classifier != classifier = initial classifier = classifier feature selection = classifier feature selection per label feature selection = classifier per feature selection feature index = classifier feature index initial classifier get instance pipe == training set get pipe classifier == classifier = max ent training set get pipe feature selection per label feature selection cached value stale = cached gradient stale = initialize constraints logger fine number instances in training list = + training list size instance inst training list instance weight = training list get instance weight inst labeling labeling = inst get labeling labeling == logger fine instance +ii+ labeling= +labeling feature vector fv = feature vector inst get data alphabet fdict = fv get alphabet fv get alphabet == fd here difference between and single label rather than only picking out best index loop over all label indices labeling num locations == training set get target alphabet size pos = 0 pos < labeling num locations pos++ matrix ops row plus equals constraints num features labeling index at location pos fv instance weight labeling value at location pos ! na n instance weight instance weight na n has na n = i = 0 i < fv num locations i++ na n fv value at location i logger info na n feature + fdict lookup fv index at location i to has na n = has na n logger info na n in instance + inst get name feature whose weight 1 0 pos = 0 pos < labeling num locations pos++ constraints labeling index at location pos num features + feature index += 1 0 instance weight labeling value labeling index at location pos max ent get classifier classifier get parameter index index set parameter index v cached value stale = cached gradient stale = index = v get num length get buff buff == || buff length != length buff = length arraycopy 0 buff 0 length set buff buff != cached value stale = cached gradient stale = buff length != length = buff length arraycopy buff 0 0 buff length log probability training label distributions get value cached value stale num get value calls++ cached value = 0 we ll store expectation values in cached gradient now cached gradient stale = matrix ops set all cached gradient 0 0 incorporate likelihood data scores = training list get target alphabet size value = 0 0 iterator< instance> iter = training list iterator ii=0 iter has next ii++ instance instance = iter next instance weight = training list get instance weight instance labeling labeling = instance get labeling labeling == out l now +input alphabet size + regular features classifier get classification scores instance scores feature vector fv = feature vector instance get data value = 0 0 pos = 0 pos < labeling num locations pos++ loop added limin yao ll = labeling index at location pos scores ll == 0 labeling value at location pos > 0 logger warning instance +instance get source + has infinite value skipping value and gradient cached value = n e g a t i v e i n f i n i t y cached value stale = cached value labeling value at location pos != 0 value = instance weight labeling value at location pos math log scores ll na n value logger fine max ent optimizable label instance + instance get name + has na n value infinite value logger warning instance +instance get source + has infinite value skipping value and gradient cached value = value cached value stale = value cached value += value model expectation? added limin yao si = 0 si < scores length si++ scores si == 0 ! infinite scores si matrix ops row plus equals cached gradient num features si fv instance weight scores si cached gradient num features si + feature index += instance weight scores si logger info expectations cached gradient print incorporate prior on prior = 0 = 0 < num labels li++ fi = 0 fi < num features fi++ = num features + fi prior += 2 gaussian prior variance o value = cached value cached value += prior cached value = 1 0 m a x i m i z e n o t m i n i m i z e cached value stale = progress logger info value label prob= + o value + prior= + prior + loglikelihood = +cached value cached value get value gradient buffer gradient constraint expectation gaussian prior variance cached gradient stale num get value gradient calls++ cached value stale will fill in cached gradient expectation get value matrix ops plus equals cached gradient constraints incorporate prior on matrix ops plus equals cached gradient 1 0 gaussian prior variance a parameter may be set to infinity an external user we set gradient to 0 because parameter s value can never change anyway and it will mess up future calculations on matrix such norm matrix ops substitute cached gradient n e g a t i v e i n f i n i t y 0 0 set to zero all gradient dimensions that are not among selected features per label feature selection == label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 feature selection label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 per label feature selection label index cached gradient stale = buffer != buffer length == length arraycopy cached gradient 0 buffer 0 cached gradient length out max ent trainer gradient infinity norm = + matrix ops infinity norm cached gradient x x x should these really be public? why? counts how many times trainer has computed gradient log probability training labels get value gradient calls num get value gradient calls counts how many times trainer has computed log probability training labels get value calls num get value calls get iterations maximizer gradient get iterations max ent optimizable label use gaussian prior sets a parameter to prevent overtraining a smaller variance prior means that feature weights are expected to hover closer to 0 so extra evidence required to set a higher weight trainer max ent optimizable label set gaussian prior variance gaussian prior variance gaussian prior variance = gaussian prior variance 