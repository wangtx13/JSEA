2002 m a l l e t m achine languag e ~mccallum 1 0 furtherinformation ` l i e n s e classify logging io classify classifier optimize limited memory b f g s optimize optimizable optimize optimizer optimize tests pipe pipe types alphabet types exp gain types feature inducer types feature selection types feature vector types gradient gain types info gain types instance types instance list types label types label alphabet types label vector types labeling types matrix ops types ranked feature vector types vector command option logger progress message logger maths does not currently handle instances that are labeled distributions instead a single label trainer a maximum entropy classifier author andrew mc callum <a href= mailto >mccallum edu< a> m max ent trainer classifier trainer< m max ent> boostable serializable command option list providing logger logger = logger get logger m max ent trainer get name logger progress logger = progress message logger get logger m max ent trainer get name + pl num get value calls = 0 num get value gradient calls = 0 num iterations = 10 e x p g a i n = exp g r a d i e n t g a i n = grad i n f o r m a t i o n g a i n = info xxx why does test maximizable fail when variance very small? d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 note used to be 1 d e f a u l t h y p e r b o l i p r i o r s l o p e = 0 2 d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s = 10 0 d e f a u l t m a x i m i z e r l a s s = limited memory b f g s p a l using multi conditional training = using hyperbolic prior = gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e hyperbolic prior slope = d e f a u l t h y p e r b o l i p r i o r s l o p e hyperbolic prior sharpness = d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s maximizer = d e f a u l t m a x i m i z e r l a s s generative weighting = 1 0 maximizable trainer mt m max ent initial classifier p a l command option using multi conditional training option = command option m max ent trainer use m training true|false use multi conditional training command option using hyperbolic prior option = command option m max ent trainer use hyperbolic prior true|false use hyperbolic close to l1 penalty prior over command option gaussian prior variance option = command option m max ent trainer gaussian prior variance f l o a t 10 0 variance gaussian prior over command option hyperbolic prior slope option = command option m max ent trainer hyperbolic prior slope f l o a t 0 2 slope l1 penalty hyperbolic prior over command option hyperbolic prior sharpness option = command option m max ent trainer hyperbolic prior sharpness f l o a t 10 0 sharpness l1 penalty hyperbolic prior over command option list command options = command option list m maximum entropy classifier command option using hyperbolic prior option gaussian prior variance option hyperbolic prior slope option hyperbolic prior sharpness option using multi conditional training option p a l command option list get command option list command options m max ent trainer maximizer gradient maximizer maximizer gradient = maximizer using hyperbolic prior = m max ent trainer command option list col using hyperbolic prior = using hyperbolic prior option value gaussian prior variance = gaussian prior variance option value hyperbolic prior slope = hyperbolic prior slope option value hyperbolic prior sharpness = hyperbolic prior sharpness option value using multi conditional training = using multi conditional training option value m max ent trainer m max ent initial classifier initial classifier = initial classifier m max ent trainer m max ent trainer use hyperbolic prior using hyperbolic prior = use hyperbolic prior constructs a trainer a parameter to avoid overtraining 1 0 usually a reasonable value m max ent trainer gaussian prior variance using hyperbolic prior = gaussian prior variance = gaussian prior variance p a l added to multi conditional training m max ent trainer gaussian prior variance use multi conditional training using hyperbolic prior = using multi conditional training = use multi conditional training gaussian prior variance = gaussian prior variance m max ent trainer hyperbolic prior slope hyperbolic prior sharpness using hyperbolic prior = hyperbolic prior slope = hyperbolic prior slope hyperbolic prior sharpness = hyperbolic prior sharpness optimizable gradient value get maximizable trainer instance list ilist ilist == maximizable trainer maximizable trainer ilist specifies maximum number iterations to run during a single call to <code>train< code> or <code>train feature induction< code> not currently functional trainer x x x since we maximize before using num iterations doesn t work that a bug? so should num iterations be higher? m max ent trainer set num iterations i num iterations = i m max ent trainer set use hyperbolic prior use hyperbolic prior using hyperbolic prior = use hyperbolic prior sets a parameter to prevent overtraining a smaller variance prior means that feature weights are expected to hover closer to 0 so extra evidence required to set a higher weight trainer m max ent trainer set gaussian prior variance gaussian prior variance using hyperbolic prior = gaussian prior variance = gaussian prior variance m max ent trainer set hyperbolic prior slope hyperbolic prior slope using hyperbolic prior = hyperbolic prior slope = hyperbolic prior slope m max ent trainer set hyperbolic prior sharpness hyperbolic prior sharpness using hyperbolic prior = hyperbolic prior sharpness = hyperbolic prior sharpness m max ent get classifier mt get classifier m max ent train instance list training set logger fine training set size = +training set size mt = maximizable trainer training set m max ent initial classifier optimizer maximizer = limited memory b f g s mt p a l change tolerance large vocab experiments limited memory b f g s maximizer set tolerance 00001 std 0001 maximizer optimize x x x given loop below seems wrong logger info m max ent nget value calls +get value calls + m max ent nget value gradient calls +get value gradient calls converged i = 0 i < num iterations i++ converged = maximizer maximize mt 1 converged evaluator != !evaluator evaluate mt get classifier converged i mt get value training set validation set test set test maximizable test value and gradient mt progress logger info progess messages are on one line move on mt get classifier <p> like other <code>train feature induction< code> but allows some options to be changed < p> maxent an initial partially trained classifier <code>null< code> classifier may be modified during training gain name estimate gain log likelihood increase we want our chosen features to maximize should be one <code> max ent trainer e x p g a i n< code> <code> max ent trainer g r a d i e n t g a i n< code> or <code> max ent trainer i n f o r m a t i o n g a i n< code> <code> e x p g a i n< code> trained <code> max ent< code> classifier classifier train feature induction instance list training data instance list validation data instance list testing data classifier evaluating evaluator m max ent maxent total iterations num iterations between feature inductions num feature inductions num features per feature induction gain name x x x ought to be a parameter except that setting it to can crash training jump too small save during f i = alphabet input alphabet = training data get data alphabet alphabet output alphabet = training data get target alphabet maxent == maxent = m max ent training data get pipe 1+input alphabet size output alphabet size training iteration = 0 num labels = output alphabet size initialize feature selection feature selection global f s = training data get feature selection global f s == mask out all features some will be added later feature inducer induce features global f s = feature selection training data get data alphabet training data set feature selection global f s validation data != validation data set feature selection global f s testing data != testing data set feature selection global f s maxent = m max ent maxent get instance pipe maxent get global f s run feature induction feature induction iteration = 0 feature induction iteration < num feature inductions feature induction iteration++ print out some feature logger info feature induction iteration +feature induction iteration train model a little bit we t care whether it converges we execute all feature induction iterations no matter what feature induction iteration != 0 t train until we have added some features set num iterations num iterations between feature inductions maxent = m max ent train training data validation data testing data evaluator maxent training iteration += num iterations between feature inductions logger info starting feature induction + 1+input alphabet size + features over +num labels+ labels create list tokens instance list instances = instance list training data get data alphabet training data get target alphabet instances feature selection will get examined feature inducer so it can know how to add singleton features instances set feature selection global f s list label vectors = list these are length 1 vectors i = 0 i < training data size i++ instance instance = training data get i feature vector input vector = feature vector instance get data label label = label instance get target having trained using just current features how we classify training data now classification classification = maxent classify instance !classification best label correct instances add input vector label label vectors add classification get label vector logger info instance list size = +error instances size s = label vectors size label vector lvs = label vector s i = 0 i < s i++ lvs i = label vector label vectors get i ranked feature vector factory gain factory = gain name equals e x p g a i n gain factory = exp gain factory lvs gaussian prior variance gain name equals g r a d i e n t g a i n gain factory = gradient gain factory lvs gain name equals i n f o r m a t i o n g a i n gain factory = info gain factory illegal argument unsupported gain name +gain name feature inducer klfi = feature inducer gain factory instances num features per feature induction 2 num features per feature induction 2 num features per feature induction note that adds features globally but not on a per transition basis klfi induce features training data testing data != klfi induce features testing data logger info m max ent feature selection now includes +global f s cardinality + features klfi = = 1+input alphabet size output alphabet size x x x executing block often causes an during training i t know why save during f i keep current parameter values x x x relies on detail that most recent features added to an alphabet get highest indices count per output label old count = maxent length output alphabet size count = 1+input alphabet size copy params into proper locations i=0 i<output alphabet size i++ arraycopy maxent i old count i count old count i=0 i<old count i++ maxent i != i out maxent i + +new i exit 0 maxent = maxent feature index = input alphabet size finished feature induction logger info ended +global f s cardinality + features set num iterations total iterations training iteration train training data validation data testing data evaluator maxent x x x should these really be public? why? counts how many times trainer has computed gradient log probability training labels get value gradient calls num get value gradient calls counts how many times trainer has computed log probability training labels get value calls num get value calls get iterations maximizer gradient get iterations to m max ent trainer + +maximizer get name + + num iterations= + num iterations + using hyperbolic prior ? hyperbolic prior slope= +hyperbolic prior slope+ hyperbolic prior sharpness= +hyperbolic prior sharpness gaussian prior variance= +gaussian prior variance a inner that wraps up a m max ent classifier and its training data result a maximize maximizable function maximizable trainer optimizable gradient value constraints cached gradient m max ent classifier instance list training list expectations are temporarily stored in cached gradient cached value cached value stale cached gradient stale num labels num features feature index just clarity feature selection feature selection feature selection per label feature selection maximizable trainer maximizable trainer instance list ilist m max ent initial classifier training list = ilist alphabet fd = ilist get data alphabet label alphabet ld = label alphabet ilist get target alphabet t fd stop growth because someone might want to feature induction ld stop growth add one feature feature num labels = ld size num features = fd size + 1 feature index = num features 1 = num labels num features constraints = num labels num features cached gradient = num labels num features arrays fill 0 0 arrays fill constraints 0 0 arrays fill cached gradient 0 0 feature selection = ilist get feature selection per label feature selection = ilist get per label feature selection add feature index to selection feature selection != feature selection add feature index per label feature selection != i = 0 i < per label feature selection length i++ per label feature selection i add feature index xxx later change to allow both to be set but select which one to use a flag? feature selection == || per label feature selection == initial classifier != classifier = initial classifier = classifier feature selection = classifier feature selection per label feature selection = classifier per feature selection feature index = classifier feature index initial classifier get instance pipe == ilist get pipe classifier == classifier = m max ent ilist get pipe feature selection per label feature selection cached value stale = cached gradient stale = initialize constraints logger fine number instances in training list = + training list size instance inst training list instance weight = training list get instance weight inst labeling labeling = inst get labeling logger fine instance +ii+ labeling= +labeling feature vector fv = feature vector inst get data alphabet fdict = fv get alphabet fv get alphabet == fd = labeling get best index 2 below because there one copy p y|x and another p x|y matrix ops row plus equals constraints num features fv 2 instance weight feature whose weight 1 0 ! na n instance weight instance weight na n ! na n best index na n has na n = i = 0 i < fv num locations i++ na n fv value at location i logger info na n feature + fdict lookup fv index at location i to has na n = has na n logger info na n in instance + inst get name only p y|x uses feature p x|y doesn t use it feature value 1 0 constraints num features + feature index += instance weight test maximizable test value and gradient current m max ent get classifier classifier get parameter index index set parameter index v cached value stale = cached gradient stale = index = v get num length get buff buff == || buff length != length buff = length arraycopy 0 buff 0 length set buff buff != cached value stale = cached gradient stale = buff length != length = buff length arraycopy buff 0 0 buff length log probability training labels get value cached value stale num get value calls++ cached value = 0 we ll store expectation values in cached gradient now cached gradient stale = arrays fill cached gradient 0 0 incorporate likelihood data scores = training list get target alphabet size value = 0 0 out i now +input alphabet size + regular features iterator< instance> iter = training list iterator ii = 0 normalize to be per multinomials probs = scores length num features lprobs = scores length num features si = 0 si < scores length si++ sum = 0 max = matrix ops max fi = 0 fi < num features fi++ t o d o strongly consider some smoothing here what happens when all are zero? oh should be no problem because exp 0 == 1 probs si fi = math exp si num features+fi max sum += probs si fi sum > 0 fi = 0 fi < num features fi++ probs si fi = sum lprobs si fi = math log probs si fi iter has next instance instance = iter next instance weight = training list get instance weight instance labeling labeling = instance get labeling out l now +input alphabet size + regular features classifier get classification scores instance scores feature vector fv = feature vector instance get data = labeling get best index value = instance weight math log scores na n value logger fine m max ent trainer instance + instance get name + has na n value log scores = + math log scores + scores = + scores + has instance weight = + instance weight infinite value logger warning instance +instance get source + has infinite value skipping value and gradient cached value = value cached value stale = value cached value += value p a l a loop over classes and their scores we compute gradient taking dot product feature value and probability si = 0 si < scores length si++ scores si == 0 ! infinite scores si p a l accumulating current classifiers expectation feature vector counts label current classifier has expectation over label not over feature vector matrix ops row plus equals cached gradient num features si fv instance weight scores si cached gradient num features si + feature index += instance weight scores si p a l we wish to multiconditional training we need another term accumulated expectation using multi conditional training need something analogous to classifier get classification scores instance scores classifier get feature distributions instance note label instance get sum feature vector which number counts document we use that input ncounts = matrix ops sum fv p a l get additional term value our log probability computation amounts to dot product feature vector and probability vector cached value = instance weight fv dot product lprobs p a l get model expectation over features given fi = 0 fi < num features fi++ num features + fi != 0 matrix ops row plus equals cached gradient num features fv cached gradient num features + fi += instance weight ncounts probs fi logger info expectations cached gradient print incorporate prior on using hyperbolic prior = 0 < num labels li++ fi = 0 fi < num features fi++ cached value += hyperbolic prior slope hyperbolic prior sharpness math log maths cosh hyperbolic prior sharpness num features + fi = 0 < num labels li++ fi = 0 fi < num features fi++ = num features + fi cached value += 2 gaussian prior variance cached value = 1 0 m a x i m i z e n o t m i n i m i z e cached value stale = progress logger info value loglikelihood = +cached value cached value p a l first get value then gradient get value gradient buffer gradient constraint expectation gaussian prior variance cached gradient stale num get value gradient calls++ cached value stale will fill in cached gradient expectation get value cached gradient contains negative expectations expectations are model expectations and constraints are empirical expectations matrix ops plus equals cached gradient constraints p a l we need a second copy constraints actually we only want feature values i ve moved up into get value using multi conditional training matrix ops plus equals cached gradient constraints incorporate prior on using hyperbolic prior unsupported operation hyperbolic prior not yet matrix ops plus equals cached gradient 1 0 gaussian prior variance a parameter may be set to infinity an external user we set gradient to 0 because parameter s value can never change anyway and it will mess up future calculations on matrix such norm matrix ops substitute cached gradient n e g a t i v e i n f i n i t y 0 0 set to zero all gradient dimensions that are not among selected features per label feature selection == label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 feature selection label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 per label feature selection label index cached gradient stale = buffer != buffer length == length arraycopy cached gradient 0 buffer 0 cached gradient length sum neg log prob a b a == p o s i t i v e i n f i n i t y b == p o s i t i v e i n f i n i t y p o s i t i v e i n f i n i t y a > b b math log 1 + math exp b a a math log 1 + math exp a b 