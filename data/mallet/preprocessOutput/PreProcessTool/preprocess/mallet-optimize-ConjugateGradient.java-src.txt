2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e author andrew mc callum <a href= mailto >mccallum edu< a> optimize logging optimize line optimizer optimize optimizable optimize tests test optimizable types matrix ops logger conjugate gradient polak and ribiere from numeric recipes in section 10 6 conjugate gradient optimizer logger logger = logger get logger conjugate gradient get name converged = optimizable gradient value optimizable line optimizer gradient line maximizer initial step size = 1 tolerance = 0 0001 gradient tolerance = 0 001 max iterations = 1000 eps a small number to recitify special converging to exactly zero function value eps = 1 0e 10 optimizer evaluator gradient eval conjugate gradient optimizable gradient value function initial step size initial step size = initial step size optimizable = function line maximizer = back track line search function alternative line maximizer = gradient bracket line optimizer function conjugate gradient optimizable gradient value function function 0 01 optimizable get optimizable optimizable converged converged set evaluator optimizer evaluator gradient eval eval = eval set line maximizer line optimizer gradient line maximizer line maximizer = line maximizer set initial step size initial step size initial step size = initial step size get initial step size initial step size get step size step state a conjugate gradient search fp gg gam dgg step fret xi g h j iterations optimize optimize max iterations set tolerance t tolerance = t optimize num iterations converged n = optimizable get num xi == fp = optimizable get value xi = n g = n h = n optimizable get value gradient xi arraycopy xi 0 g 0 n arraycopy xi 0 h 0 n step = initial step size iterations = 0 iteration count = 0 iteration count < num iterations iteration count++ logger info conjugate gradient at iteration +iterations+ cost = +fp step = line maximizer optimize xi step fret = optimizable get value optimizable get value gradient xi termination numeric recipes in 2 0 math abs fret fp <= tolerance math abs fret + math abs fp +eps logger info conjugate gradient converged old value= +fp+ value= +fret+ tolerance= +tolerance converged = fp = fret termination mc callum two norm = matrix ops two norm xi two norm < gradient tolerance logger info conjugate gradient converged gradient two norm + two norm + less than + gradient tolerance converged = dgg = gg = 0 0 j = 0 j < xi length j++ gg += g j g j dgg += xi j xi j g j gam = dgg gg j = 0 j < xi length j++ g j = xi j h j = xi j + gam h j ! matrix ops na n h gdruck line search algorithms stop search whenever a step found that increases value significantly conjugate gradient assumes that line maximization finds something close to maximum in that direction in tests sometimes direction suggested g was downhill consequently here i am setting search direction to gradient slope negative or 0 matrix ops dot product xi h > 0 matrix ops set xi h logger warning reverting back to g a matrix ops set h xi iterations++ iterations > max iterations logger info too many iterations in conjugate gradient converged = eval != eval evaluate optimizable iterations reset xi = 