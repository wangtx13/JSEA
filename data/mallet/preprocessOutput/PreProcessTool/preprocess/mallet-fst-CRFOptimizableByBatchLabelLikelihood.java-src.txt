io i o io input stream io output stream io serializable list arrays collection list logging logger optimize optimizable types feature sequence types feature vector sequence types instance types instance list types matrix ops logger label likelihood gradient computations batches data can be easily parallelized <p> gradient computations are same that <tt> r f optimizable label likelihood< tt> <p> note expectations corresponding to each batch data can be computed in parallel during gradient computation prior and constraints are incorporated into expectations last batch <tt>get batch value get batch value gradient< tt> note ignores instances infinite weights <tt>get expectation value< tt> author gaurav chandalia r f optimizable batch label likelihood optimizable combining batch gradient serializable logger logger = logger get logger r f optimizable batch label likelihood get name d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 0 d e f a u l t h y p e r b o l i p r i o r s l o p e = 0 2 d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s = 10 0 r f crf instance list training set number batches training set num batches batch specific expectations list< r f factors> expectations constraints over whole training set r f factors constraints value and gradient each batch to avoid sharing cached value list<double > cached gradient using hyperbolic prior = gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e hyperbolic prior slope = d e f a u l t h y p e r b o l i p r i o r s l o p e hyperbolic prior sharpness = d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s r f optimizable batch label likelihood r f crf instance list ilist num batches set up crf = crf training set = ilist num batches = num batches cached value = num batches cached gradient = list<double > num batches expectations = list< r f factors> num batches num factors = crf get num factors i = 0 i < num batches ++i cached gradient add num factors expectations add r f factors crf constraints = r f factors crf gather constraints ilist set constraints running forward backward <i>output label sequence provided< i> thus restricting it to only those paths that agree label sequence gather constraints instance list ilist logger info gathering constraints constraints structure matches crf constraints zero instance instance ilist feature vector sequence input = feature vector sequence instance get data feature sequence output = feature sequence instance get target instance weight = ilist get instance weight instance transducer incrementor incrementor = instance weight == 1 0 ? constraints incrementor constraints weighted incrementor instance weight sum lattice crf input output incrementor constraints not na n or infinite computes log probability a batch training data fill in corresponding expectations well get expectation value batch index batch assignments reset expectations to zero before we fill them again r f factors batch expectations = expectations get batch index batch expectations zero count number instances that have infinite weight num inf labeled weight = 0 num inf unlabeled weight = 0 num inf weight = 0 value = 0 unlabeled weight labeled weight weight ii = batch assignments 0 ii < batch assignments 1 ii++ instance instance = training set get ii instance weight = training set get instance weight instance feature vector sequence input = feature vector sequence instance get data feature sequence output = feature sequence instance get target labeled weight = sum lattice crf input output get total weight infinite labeled weight ++num inf labeled weight transducer incrementor incrementor = instance weight == 1 0 ? batch expectations incrementor batch expectations weighted incrementor instance weight unlabeled weight = sum lattice crf input incrementor get total weight infinite unlabeled weight ++num inf unlabeled weight weight log conditional probability correct label sequence weight = labeled weight unlabeled weight infinite weight ++num inf weight weights are log probabilities and we want to a log probability value += weight instance weight batch expectations not na n or infinite num inf labeled weight > 0 || num inf unlabeled weight > 0 || num inf weight > 0 logger warning batch + batch index + number instances + infinite labeled weight + num inf labeled weight + + infinite unlabeled weight + num inf unlabeled weight + + infinite weight + num inf weight value log probability a batch training sequence labels and prior over last batch then incorporate prior on well get batch value batch index batch assignments batch index < num batches incorrect batch index + batch index + range 0 + num batches + batch assignments length == 2 batch assignments 0 <= batch assignments 1 invalid batch assignments + arrays to batch assignments get value all labels current batch also filling in expectations value = get expectation value batch index batch assignments batch index == num batches 1 using hyperbolic prior hyperbolic prior value += crf hyberbolic prior hyperbolic prior slope hyperbolic prior sharpness gaussian prior value += crf gaussian prior gaussian prior variance ! na n value || infinite value label likelihood na n infinite batch index + batch index + batch assignments + arrays to batch assignments update cache cached value batch index = value value get batch value gradient buffer batch index batch assignments batch index < num batches incorrect batch index + batch index + range 0 + num batches + batch assignments length == 2 batch assignments 0 <= batch assignments 1 invalid batch assignments + arrays to batch assignments r f factors batch expectations = expectations get batch index batch index == num batches 1 crf check has to be done only once infinite values are allowed crf not na n factor constraints and prior into expectations last batch gradient = constraints expectations + prior = expectations constraints prior minus sign factored in combine gradients after all gradients are computed batch expectations plus equals constraints 1 0 using hyperbolic prior batch expectations plus equals hyperbolic prior gradient crf hyperbolic prior slope hyperbolic prior sharpness batch expectations plus equals gaussian prior gradient crf gaussian prior variance batch expectations not na n or infinite gradient = cached gradient get batch index set cached gradient batch expectations get gradient arraycopy gradient 0 buffer 0 gradient length adds gradients from all batches <p> <b> note < b> assumes buffer already initialized combine gradients collection<double > batch gradients buffer buffer length == crf get num factors incorrect buffer length + buffer length + expected + crf get num factors arrays fill buffer 0 gradient batch gradients matrix ops plus equals buffer gradient from get batch value gradient matrix ops times equals buffer 1 0 get num batches num batches set use hyperbolic prior f using hyperbolic prior = f set hyperbolic prior slope p hyperbolic prior slope = p set hyperbolic prior sharpness p hyperbolic prior sharpness = p get use hyperbolic prior slope hyperbolic prior slope get use hyperbolic prior sharpness hyperbolic prior sharpness set gaussian prior variance p gaussian prior variance = p get gaussian prior variance gaussian prior variance get num crf get num factors get buffer crf get buffer get parameter index crf get parameter index set buff crf set buff crf weights value changed set parameter index value crf set parameter index value crf weights value changed serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write training set out write crf out write num batches out write cached value gradient cached gradient out write gradient read input stream in i o not found in read training set = instance list in read crf = r f in read num batches = in read cached value = in read cached gradient = list<double > num batches i = 0 i < num batches ++i cached gradient set i in read factory optimizable combining batch gradient r f optimizable r f crf instance list training data num batches r f optimizable batch label likelihood crf training data num batches 