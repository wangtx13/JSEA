list collections types feature vector sequence types instance types instance list types sequence transducer trainer instance increments trains r f stochastic gradient most effective on large training sets author kedarb r f trainer stochastic gradient instance increments r f crf t decaying factor lambda some regularization depending on training set size and gaussian prior rate t lambda iteration count = 0 converged = r f factors expectations constraints r f trainer stochastic gradient r f crf instance list training sample crf = crf expectations = r f factors crf constraints = r f factors crf set rate likelihood training sample r f trainer stochastic gradient r f crf rate crf = crf rate = rate expectations = r f factors crf constraints = r f factors crf get iteration iteration count transducer get transducer crf finished training converged best way to choose rate to run training on a sample and set it to rate that produces maximum increase in likelihood or accuracy then to be conservative just halve rate in general eta = 1 lambda t where lambda=prior variance num training instances after an initial eta 0 set t 0 = 1 lambda eta 0 after each training step eta = 1 lambda t+t 0 t=0 1 2 infinity automatically sets rate to one that would be good set rate likelihood instance list training sample num iterations = 5 was 10 akm 1 25 08 best rate = n e g a t i v e i n f i n i t y best likelihood change = n e g a t i v e i n f i n i t y curr rate = 5e 11 curr rate < 1 curr rate = 2 crf zero before likelihood = compute likelihood training sample likelihood change = train sample training sample num iterations curr rate before likelihood out likelihood change = + likelihood change + learningrate= + curr rate likelihood change > best likelihood change best likelihood change = likelihood change best rate = curr rate reset crf zero conservative estimate rate best rate = 2 out setting rate to + best rate set rate best rate train sample instance list training sample num iterations rate lambda = training sample size t = 1 lambda rate loglik = n e g a t i v e i n f i n i t y i = 0 i < num iterations i++ loglik = 0 0 j = 0 j < training sample size j++ rate = 1 lambda t loglik += train incremental likelihood training sample get j rate t += 1 0 loglik compute likelihood instance list training sample loglik = 0 0 i = 0 i < training sample size i++ instance training instance = training sample get i feature vector sequence fvs = feature vector sequence training instance get data sequence label sequence = sequence training instance get target loglik += sum lattice crf fvs label sequence get total weight loglik = sum lattice crf fvs get total weight constraints zero expectations zero loglik set rate r rate = r get rate rate train instance list training set num iterations train training set num iterations 1 train instance list training set num iterations num iterations between evaluation expectations structure matches crf constraints structure matches crf lambda = 1 0 training set size t = 1 0 lambda rate converged = list< integer> training indices = list< integer> i = 0 i < training set size i++ training indices add i old loglik = n e g a t i v e i n f i n i t y num iterations > 0 iteration count++ shuffle indices collections shuffle training indices loglik = 0 0 i = 0 i < training set size i++ rate = 1 0 lambda t loglik += train incremental likelihood training set get training indices get i t += 1 0 out loglikelihood + num iterations + = + loglik math abs loglik old loglik < 1e 3 converged = old loglik = loglik runtime get runtime gc iteration count % num iterations between evaluation == 0 run evaluators converged t o d o add some way to train batches instances where batch memberships are determined externally? or provide some easy creating batches train incremental instance list training set train training set 1 train incremental instance training instance expectations structure matches crf train incremental likelihood training instance adjust rate according to gradient single instance and label sequence likelihood train incremental likelihood instance training instance train incremental likelihood training instance rate adjust rate according to gradient single instance and label sequence likelihood train incremental likelihood instance training instance rate single loglik constraints zero expectations zero feature vector sequence fvs = feature vector sequence training instance get data sequence label sequence = sequence training instance get target single loglik = sum lattice crf fvs label sequence constraints incrementor get total weight single loglik = sum lattice crf fvs expectations incrementor get total weight calculate parameter gradient given these instances constraints expectations constraints plus equals expectations 1 change a little difference obeying weights frozen crf plus equals constraints rate single loglik 