random logging logger optimize limited memory b f g s optimize optimizer types instance list logger author gregory druck gdruck multi threaded r f trainer note that multi threaded feature induction and hyperbolic prior are not supported r f trainer threaded label likelihood transducer trainer transducer trainer optimization logger logger = logger get logger r f trainer threaded label likelihood get name d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 0 use sparse weights use no weights use some unsupported trick converged num threads iteration count gaussian prior variance r f crf r f optimizable batch label likelihood optimizable threaded optimizable threaded optimizable optimizer optimizer cached weights structure stamp r f trainer threaded label likelihood r f crf num threads crf = crf use sparse weights = use no weights = use some unsupported trick = converged = num threads = num threads iteration count = 0 gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e cached weights structure stamp = 1 transducer get transducer crf r f get r f crf optimizer get optimizer optimizer converged converged finished training converged get iteration iteration count set gaussian prior variance p gaussian prior variance = p get gaussian prior variance gaussian prior variance set use sparse weights b use sparse weights = b get use sparse weights use sparse weights sets whether to use some unsupported trick trick training a r f where some training has been done and sparse weights are used to add a few weights feaures that not occur in tainig data <p> generally leads to better accuracy at only a small memory cost b whether to use trick set use some unsupported trick b use some unsupported trick = b use to specify whether or not factors are added to r f trainer you have already setup factors in your r f you may not want trainer to add additional factors flag trainer adds no factors to r f set add no factors flag use no weights = flag shutdown threaded optimizable shutdown r f optimizable batch label likelihood get optimizable r f instance list training set cached weights structure stamp != crf weights structure change stamp !use no weights use sparse weights crf set weights dimension in training set use some unsupported trick crf set weights dimension densely optimizable = cached weights structure stamp = crf weights structure change stamp optimizable == || optimizable training set != training set optimizable = r f optimizable batch label likelihood crf training set num threads optimizable set gaussian prior variance gaussian prior variance threaded optimizable = threaded optimizable optimizable training set crf get get num factors r f cache stale indicator crf optimizer = optimizable optimizer get optimizer instance list training set get optimizable r f training set optimizer == || optimizable != optimizer get optimizable optimizer = limited memory b f g s threaded optimizable optimizer train incremental instance list training train training m a x v a l u e train instance list training set num iterations num iterations <= 0 training set size > 0 get optimizable r f training set will set mcrf necessary get optimizer training set will set opt necessary converged = logger info r f about to train +num iterations+ iterations i = 0 i < num iterations i++ converged = optimizer optimize 1 iteration count++ logger info r f finished one iteration maximizer i= +i run evaluators illegal argument e e print stack trace logger info catching saying converged converged = e e print stack trace logger info catching saying converged converged = converged logger info r f training has converged i= +i converged train a r f on various sized subsets data typically used to accelerate training quickly getting to reasonable on only a subset first then on progressively more data training training instances num iterations per proportion maximum number maximizer iterations per training proportion training proportions non train on increasingly larger portions data e g 0 2 0 5 1 0 can sometimes speedup convergence be sure to end in 1 0 you want to train on all data in end training has converged train instance list training num iterations per proportion training proportions training iteration = 0 training proportions length > 0 converged = i = 0 i < training proportions length i++ training proportions i <= 1 0 logger info training on +training proportions i + % data round training proportions i == 1 0 converged = train training num iterations per proportion converged = train training split random 1 training proportions i 1 training proportions i 0 num iterations per proportion training iteration += num iterations per proportion converged 