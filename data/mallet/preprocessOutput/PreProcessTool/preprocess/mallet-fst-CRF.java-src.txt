2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e author andrew mc callum <a href= mailto >mccallum edu< a> io io output stream io i o io input stream io output stream io output stream writer io print writer io serializable list arrays bit set hash map iterator logging logger regex pattern text decimal format types alphabet types feature inducer types feature selection types feature sequence types feature vector types feature vector sequence types indexed sparse vector types instance types instance list types matrix ops types ranked feature vector types sequence types sparse vector pipe noop pipe pipe utils logger maths there are several different kinds numeric values weights range from inf to inf high weights make a path more likely these t appear directly in transducer but appear to many subclasses such r fs weights are also often summed or combined in a dot product feature vectors unnormalized costs range from inf to inf high costs make a path less likely unnormalized costs can be obtained from negated weights or negated sums weights these are often a transition iterator s get value lattice node alpha values are unnormalized costs normalized costs range from 0 to inf high costs make a path less likely normalized costs can safely be considered log probability some event they can be obtained subtracting a negative normalizer from unnormalized costs example subtracting total cost a lattice typically initial costs and costs are examples normalized costs but they are also allowed to be unnormalized costs gammas state gammas and transition xis are all normalized costs well value lattice get value probabilities range from 0 to 1 high probabilities make a path more likely they are obtained from normalized costs taking log and negating sums probabilities range from 0 to positive numbers they are sum several probabilities these are passed to increment count represents a r f model r f transducer serializable logger logger = logger get logger r f get name l a b e l s e p a r a t o r = alphabet input alphabet alphabet output alphabet list< state> states = list< state> list< state> initial states = list< state> hash map< state> name2state = hash map< state> factors = factors sparse vector weights weights feature alphabet weight alphabet = alphabet weights frozen feature induction can fill in feature selection global feature selection feature selections on a per weights i basis and over rides permanently disabling feature inducer s and set weights dimensions in from using these features on these transitions feature selection feature selections store here induced feature conjunctions so that these conjunctions can be added to test instances before transduction list< feature inducer> feature inducers = list< feature inducer> an index that gets incremented each time r fs get changed weights value change stamp = 0 an index that gets incremented each time r fs structure get changed weights structure change stamp = 0 cached num stamp = 1 a copy weights structure change stamp last time num was calculated num a simple transparent container to hold or sufficient statistics r f factors serializable alphabet weight alphabet sparse vector weights on transitions indexed weight index weights features indexed weight index weights frozen flag indicating that weights weight index should not be changed indexed weight index initial weights indexed state index weights indexed state index construct a empty factors a empty weights alphabet 0 length initial weights and weights and other arrays factors weight alphabet = alphabet initial weights = 0 weights = 0 leave rest they will get set later add state and add weight alternatively we could create zero length arrays construct factors mimicking structure other one but zero values always simply point to other s alphabet not clone it factors factors other weight alphabet = other weight alphabet weights = sparse vector other weights length i = 0 i < weights length i++ weights i = sparse vector other weights i clone matrix zeroed weights = other weights length weights frozen = other weights frozen we t copy here because we want expectation and constraint factors to get changes to a r f factor alternatively we declare freezing to be a change structure and force reallocation expectations etc initial weights = other initial weights length weights = other weights length construct factors copying other one factors factors other clone alphabet weight alphabet = clone alphabet ? alphabet other weight alphabet clone other weight alphabet weights = sparse vector other weights length i = 0 i < weights length i++ weights i = sparse vector other weights i clone matrix weights = other weights clone weights frozen = other weights frozen initial weights = other initial weights clone weights = other weights clone construct a factors same structure crf but values initialized to zero typically used to allocate storage sufficient statistics expectations constraints etc factors r f crf t o d o change to crf weight alphabet = crf weight alphabet t o d o consider cloning instead weights = sparse vector crf weights length i = 0 i < weights length i++ weights i = sparse vector crf weights i clone matrix zeroed weights = crf weights length weights frozen = crf weights frozen crf num states == crf initial weights length crf initial weights length == crf weights length initial weights = crf initial weights length weights = crf weights length get num factors initial weights length == weights length weights length == weights length ret = initial weights length + weights length + weights length i = 0 i < weights length i++ ret += weights i num locations ret zero i = 0 i < weights length i++ weights i set all 0 arrays fill weights 0 arrays fill initial weights 0 arrays fill weights 0 structure matches factors other weight alphabet size != other weight alphabet size weights length != other weights length gsc checking each sparse vector s size within weights i = 0 i < weights length i++ weights i num locations != other weights i num locations note that we are not checking indices sparse vectors in weights weights length != other weights length initial weights length == weights length initial weights length != other initial weights length not na n i = 0 i < weights length i++ !weights i na n ! matrix ops na n weights ! matrix ops na n initial weights ! matrix ops na n weights gsc checks all weights to make sure there are no na n or infinite values can be called checking weights constraints and expectations but not crf since it can have infinite weights associated states that are not likely not na n or infinite i = 0 i < weights length i++ !weights i na n or infinite ! matrix ops na n or infinite weights ! matrix ops na n or infinite initial weights ! matrix ops na n or infinite weights plus equals factors other factor plus equals other factor plus equals factors other factor obey weights frozen i = 0 i < weights length i++ obey weights frozen weights frozen i weights i plus equals sparse other weights i factor weights i += other weights i factor i = 0 i < initial weights length i++ initial weights i += other initial weights i factor weights i += other weights i factor log p according to a zero mean gaussian given variance gaussian prior variance value = 0 prior denom = 2 variance initial weights length == weights length i = 0 i < initial weights length i++ ! infinite initial weights i value = initial weights i initial weights i prior denom ! infinite weights i value = weights i weights i prior denom w i = 0 i < weights length i++ ! infinite weights i value = weights i weights i prior denom j = 0 j < weights i num locations j++ w = weights i value at location j ! infinite w value = w w prior denom value plus equals gaussian prior gradient factors other variance initial weights length == weights length i = 0 i < initial weights length i++ gsc checking initial weights crf well since we could have a state where some states have infinite initial and or weight ! infinite initial weights i ! infinite other initial weights i initial weights i = other initial weights i variance ! infinite weights i ! infinite other weights i weights i = other weights i variance w ow i = 0 i < weights length i++ weights frozen i t o d o note that there doesn t seem to be a way to freeze initial weights and weights t o d o should we also obey feature selection here? no need it enforced creation weights ! infinite weights i weights i = other weights i variance j = 0 j < weights i num locations j++ w = weights i value at location j ow = other weights i value at location j ! infinite w weights i set value at location j w ow variance log p according to a a hyperbolic curve that a smooth approximation to an l1 prior hyberbolic prior slope sharpness value = 0 initial weights length == weights length i = 0 i < initial weights length i++ ! infinite initial weights i value = slope sharpness math log maths cosh sharpness initial weights i ! infinite weights i value = slope sharpness math log maths cosh sharpness weights i w i = 0 i < weights length i++ value = slope sharpness math log maths cosh sharpness weights i j = 0 j < weights i num locations j++ w = weights i value at location j ! infinite w value = slope sharpness math log maths cosh sharpness w value plus equals hyperbolic prior gradient factors other slope sharpness t o d o could use some careful checking over especially flipped negations initial weights length == weights length ss = slope sharpness i = 0 i < initial weights length i++ gsc checking initial weights crf well since we could have a state where some states have infinite initial and or weight ! infinite initial weights i ! infinite other initial weights i initial weights i += ss maths tanh other initial weights i ! infinite weights i ! infinite other weights i weights i += ss maths tanh other weights i w ow i = 0 i < weights length i++ weights frozen i t o d o note that there doesn t seem to be a way to freeze initial weights and weights t o d o should we also obey feature selection here? no need it enforced creation weights ! infinite weights i weights i += ss maths tanh other weights i j = 0 j < weights i num locations j++ w = weights i value at location j ow = other weights i value at location j ! infinite w weights i set value at location j w + ss maths tanh ow instances inner can be passed to various inference which can then gather increment sufficient statistics counts into containing factor instance incrementor transducer incrementor increment state transducer state s count weights s get index += count increment initial state transducer state s count initial weights s get index += count increment transition transducer transition iterator ti count index = ti get index r f state source = r f state ti get source state nwi = source weights indices index length weights index wi = 0 wi < nwi wi++ weights index = source weights indices index wi frozen weights t even gather their sufficient statistics how we ensure that gradient these will be zero weights frozen weights index t o d o should we also obey feature selection here? no need it enforced creation weights weights weights index plus equals sparse feature vector ti get input count weights weights index += count get abs norm ret = 0 i = 0 i < initial weights length i++ initial weights i > transducer i m p o s s i b l e w e i g h t ret += math abs initial weights i weights i > transducer i m p o s s i b l e w e i g h t ret += math abs weights i i = 0 i < weights length i++ ret += math abs weights i nl = weights i num locations j = 0 j < nl j++ ret += math abs weights i value at location j ret weighted incrementor transducer incrementor instance weight = 1 0 weighted incrementor instance weight instance weight = instance weight increment state transducer state s count weights s get index += count instance weight increment initial state transducer state s count initial weights s get index += count instance weight increment transition transducer transition iterator ti count index = ti get index r f state source = r f state ti get source state nwi = source weights indices index length weights index count = instance weight wi = 0 wi < nwi wi++ weights index = source weights indices index wi frozen weights t even gather their sufficient statistics how we ensure that gradient these will be zero weights frozen weights index t o d o should we also obey feature selection here? no need it enforced creation weights weights weights index plus equals sparse feature vector ti get input count weights weights index += count get buffer buffer length != get num factors illegal argument expected size buffer + get num factors + actual size + buffer length pi = 0 i = 0 i < initial weights length i++ buffer pi++ = initial weights i buffer pi++ = weights i i = 0 i < weights length i++ buffer pi++ = weights i nl = weights i num locations j = 0 j < nl j++ buffer pi++ = weights i value at location j get parameter index num state parms = 2 initial weights length index < num state parms index % 2 == 0 initial weights index 2 weights index 2 index = num state parms i = 0 i < weights length i++ index == 0 weights i index index < weights i num locations weights i value at location index index = weights i num locations illegal argument index too high = +index set buff buff length == get num factors pi = 0 i = 0 i < initial weights length i++ initial weights i = buff pi++ weights i = buff pi++ i = 0 i < weights length i++ weights i = buff pi++ nl = weights i num locations j = 0 j < nl j++ weights i set value at location j buff pi++ set parameter index value num state parms = 2 initial weights length index < num state parms index % 2 == 0 initial weights index 2 = value weights index 2 = value index = num state parms i = 0 i < weights length i++ index == 0 weights i = value index index < weights i num locations weights i set value at location index value index = weights i num locations illegal argument index too high = +index gsc serialization factors serial u = 1 u r r e n t s e r i a l v e r s i o n = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write weight alphabet out write weights out write weights out write weights frozen out write initial weights out write weights read input stream in i o not found = in read weight alphabet = alphabet in read weights = sparse vector in read weights = in read weights frozen = in read initial weights = in read weights = in read r f pipe input pipe pipe output pipe input pipe output pipe input alphabet = input pipe get data alphabet output alphabet = input pipe get target alphabet input alphabet stop growth r f alphabet input alphabet alphabet output alphabet noop input alphabet output alphabet input alphabet stop growth logger info r f input dictionary size = +input alphabet size xxx output alphabet stop growth input alphabet = input alphabet output alphabet = output alphabet create a r f whose states and weights are a copy those from another r f r f r f other assumes that other has non input pipe and output pipe we d need to add another constructor to handle not other get input pipe other get output pipe copy states and weights from other weights length copy states and weights from r f initial r f = factors initial r f will copy all transition weights weight alphabet = alphabet initial r f weight alphabet clone weight alphabet = alphabet initial r f weight alphabet clone weights = sparse vector initial r f weights length states clear clear these because they will be filled add state initial weights = 0 weights = 0 i = 0 i < initial r f states size i++ state s = state initial r f get state i weight names = s weights indices length j = 0 j < weight names length j++ w = s weights indices j weight names j = initial r f weight alphabet lookup w s weights indices j length add state s name initial r f initial weights i initial r f weights i s destination names s labels weight names feature selections = initial r f feature selections clone yyy weights frozen = initial r f weights frozen clone alphabet get input alphabet input alphabet alphabet get output alphabet output alphabet should be called whenever r fs weights have their structure arity number changed weights structure changed weights structure change stamp++ weights value change stamp++ should be called whenever r fs weights are changed weights value changed weights value change stamp++ can be over ridden in subclasses r f to subclasses r f state r f state state name index initial weight weight destination names label names weight names r f crf state name index initial weight weight destination names label names weight names crf add state name initial weight weight destination names label names weight names weight names length == destination names length label names length == destination names length weights structure changed name2state get name != illegal argument state name ` +name+ already initial weights = matrix ops append initial weights initial weight weights = matrix ops append weights weight state s = state name states size initial weight weight destination names label names weight names s print states add s initial weight > i m p o s s i b l e w e i g h t initial states add s name2state put name s add state name initial weight weight destination names label names weight names weight names = weight names length 1 i = 0 i < weight names length i++ weight names i 0 = weight names i add state name initial weight weight destination names label names weight names gives separate to each transition add state name initial weight weight destination names label names destination names length == label names length weight names = label names length i = 0 i < label names length i++ weight names i = name + > + destination names i + + label names i add state name initial weight weight destination names label names weight names add a state equal zero and labels on out going arcs same name their destination state names add state name destination names add state name 0 0 destination names destination names add a group states that are fully connected each other equal zero and labels on their out going arcs same name their destination state names add fully connected states state names i = 0 i < state names length i++ add state state names i state names add fully connected states labels labels = output alphabet size assuming entries in output alphabet are strings! i = 0 i < output alphabet size i++ logger info r f output alphabet lookup = + output alphabet lookup i get get name labels i = output alphabet lookup i add fully connected states labels add start state add start state < s t a r t> add start state name i = 0 i < num states i++ initial weights i = i m p o s s i b l e w e i g h t dests = num states i = 0 i < dests length i++ dests i = get state i get name add state name 0 0 0 dests dests initial weight 0 0 set start state state state i = 0 i < num states i++ transducer state other = get state i other == state other set initial weight 0 other set initial weight i m p o s s i b l e w e i g h t weights value changed label connections in instance list training set label connections in training set label connections in instance list training set start num labels = output alphabet size connections = num labels num labels i = 0 i < training set size i++ instance instance = training set get i feature sequence output = feature sequence instance get target j = 1 j < output size j++ source index = output alphabet lookup index output get j 1 dest index = output alphabet lookup index output get j source index >= 0 dest index >= 0 connections source index dest index = handle start state start != start index = output alphabet lookup index start j = 0 j < output alphabet size j++ connections start index j = connections add states to create a first order markov model on labels adding only those transitions occur in given training set add states labels connected in instance list training set num labels = output alphabet size connections = label connections in training set i = 0 i < num labels i++ num destinations = 0 j = 0 j < num labels j++ connections i j num destinations++ destination names = num destinations destination index = 0 j = 0 j < num labels j++ connections i j destination names destination index++ = output alphabet lookup j add state output alphabet lookup i destination names add many states there are labels but t create separate weights each source destination pair states instead have all incoming transitions to a state share same weights add states half labels connected in instance list training set num labels = output alphabet size connections = label connections in training set i = 0 i < num labels i++ num destinations = 0 j = 0 j < num labels j++ connections i j num destinations++ destination names = num destinations destination index = 0 j = 0 j < num labels j++ connections i j destination names destination index++ = output alphabet lookup j add state output alphabet lookup i 0 0 0 0 destination names destination names destination names add many states there are labels but t create separate observational test weights each source destination pair states instead have all incoming transitions to a state share same observational feature test weights however create separate feature each transition which acts an h m m style transition probability add states three quarter labels connected in instance list training set num labels = output alphabet size connections = label connections in training set i = 0 i < num labels i++ num destinations = 0 j = 0 j < num labels j++ connections i j num destinations++ destination names = num destinations weight names = num destinations destination index = 0 j = 0 j < num labels j++ connections i j label name = output alphabet lookup j destination names destination index = label name weight names destination index = 2 half labels will include all observed tests weight names destination index 0 = label name transition weights will include only feature wn = output alphabet lookup i + > + output alphabet lookup j weight names destination index 1 = wn wi = get weights index wn a empty feature selection won t allow any features here so we only get feature transitions feature selections wi = feature selection training set get data alphabet destination index++ add state output alphabet lookup i 0 0 0 0 destination names destination names weight names add fully connected states three quarter labels instance list training set num labels = output alphabet size i = 0 i < num labels i++ destination names = num labels weight names = num labels j = 0 j < num labels j++ label name = output alphabet lookup j destination names j = label name weight names j = 2 half labels will include all observational tests weight names j 0 = label name transition weights will include only feature wn = output alphabet lookup i + > + output alphabet lookup j weight names j 1 = wn wi = get weights index wn a empty feature selection won t allow any features here so we only get feature transitions feature selections wi = feature selection training set get data alphabet add state output alphabet lookup i 0 0 0 0 destination names destination names weight names add fully connected states bi labels labels = output alphabet size assuming entries in output alphabet are strings! i = 0 i < output alphabet size i++ logger info r f output alphabet lookup = + output alphabet lookup i get get name labels i = output alphabet lookup i i = 0 i < labels length i++ j = 0 j < labels length j++ destination names = labels length k = 0 k < labels length k++ destination names k = labels j + l a b e l s e p a r a t o r+labels k add state labels i + l a b e l s e p a r a t o r+labels j 0 0 0 0 destination names labels add states to create a second order markov model on labels adding only those transitions occur in given training set add states bi labels connected in instance list training set num labels = output alphabet size connections = label connections in training set i = 0 i < num labels i++ j = 0 j < num labels j++ !connections i j num destinations = 0 k = 0 k < num labels k++ connections j k num destinations++ destination names = num destinations labels = num destinations destination index = 0 k = 0 k < num labels k++ connections j k destination names destination index = output alphabet lookup j + l a b e l s e p a r a t o r+ output alphabet lookup k labels destination index = output alphabet lookup k destination index++ add state output alphabet lookup i + l a b e l s e p a r a t o r+ output alphabet lookup j 0 0 0 0 destination names labels add fully connected states tri labels labels = output alphabet size assuming entries in output alphabet are strings! i = 0 i < output alphabet size i++ logger info r f output alphabet lookup = + output alphabet lookup i get get name labels i = output alphabet lookup i i = 0 i < labels length i++ j = 0 j < labels length j++ k = 0 k < labels length k++ destination names = labels length l = 0 l < labels length l++ destination names l = labels j + l a b e l s e p a r a t o r+labels k + l a b e l s e p a r a t o r+labels l add state labels i + l a b e l s e p a r a t o r+labels j + l a b e l s e p a r a t o r+labels k 0 0 0 0 destination names labels add self transitioning state all labels name labels = output alphabet size destination names = output alphabet size assuming entries in output alphabet are strings! i = 0 i < output alphabet size i++ logger info r f output alphabet lookup = + output alphabet lookup i get get name labels i = output alphabet lookup i destination names i = name add state name 0 0 0 0 destination names labels concat labels labels sep = buffer buf = buffer i = 0 i < labels length i++ buf append sep append labels i sep = l a b e l s e p a r a t o r buf to next k gram history k next sep = buffer buf = buffer start = history length + 1 k i = start i < history length i++ buf append sep append history i sep = l a b e l s e p a r a t o r buf append sep append next buf to allowed transition prev curr pattern no pattern yes pair = concat labels prev curr no != no matcher pair matches yes != !yes matcher pair matches allowed history history pattern no pattern yes i = 1 i < history length i++ !allowed transition history i 1 history i no yes assumes that r f s output alphabet contains <code> string< code>s creates an order <em>n< em> r f input predicates and output labels given <code>training set< code> and order connectivity and weights given remaining arguments training set training instances orders an increasing non negative numbers giving orders features r f largest number <em>n< em> markov order r f states are <em>n< em> tuples output labels each other numbers <em>k< em> in <code>orders< code> represents a weight set shared all destination states whose last most recent <em>k< em> labels agree <code>orders< code> <code>null< code> an order 0 r f built defaults non it must be same length <code>orders< code> <code>true< code> positions indicating that weight set corresponding order contains only weight a feature otherwise weight set has weights all features built from input predicates start label that represents context start a sequence it may be also used sequence labels no label name one will be added connection wills be added between start label and all other labels even <tt>fully connected< tt> <tt>false< tt> argument may be in which no special start state added forbidden non specifies what pairs successive labels are not allowed both constructing <em>n< em>order states or transitions a label pair <em>u< em> <em>v< em> not allowed <em>u< em> + + <em>v< em> matches <code>forbidden< code> allowed non specifies what pairs successive labels are allowed both constructing <em>n< em>order states or transitions a label pair <em>u< em> <em>v< em> allowed only <em>u< em> + + <em>v< em> matches <code>allowed< code> fully connected whether to include all allowed transitions even those not occurring in <code>training set< code> name start state add order n states instance list training set orders defaults start pattern forbidden pattern allowed fully connected connections = start != output alphabet lookup index start !fully connected connections = label connections in training set start order = 1 defaults != defaults length != orders length illegal argument defaults must be or match orders orders == order = 0 i = 0 i < orders length i++ orders i <= order illegal argument orders must be non negative and in ascending order order = orders i order < 0 order = 0 order > 0 history indexes = order history = order label0 = output alphabet lookup 0 i = 0 i < order i++ history i = label0 num labels = output alphabet size history indexes 0 < num labels logger info preparing + concat labels history allowed history history forbidden allowed state name = concat labels history nt = 0 dest names = num labels label names = num labels weight names = num labels orders length next index = 0 next index < num labels next index++ next = output alphabet lookup next index allowed transition history order 1 next forbidden allowed fully connected || connections history indexes order 1 next index dest names nt = next k gram history order next label names nt = next i = 0 i < orders length i++ weight names nt i = next k gram history orders i +1 next defaults != defaults i wi = get weights index weight names nt i using empty feature selection gives us only features feature selections wi = feature selection training set get data alphabet nt++ nt < num labels dest names = nt label names = nt weight names = nt t = 0 t < nt t++ dest names t = dest names t label names t = label names t weight names t = weight names t dest names = dest names label names = label names weight names = weight names i = 0 i < dest names length i++ buffer b = buffer j = 0 j < orders length j++ b append append weight names i j logger info state name + > + dest names i + + label names i + + b to add state state name 0 0 0 0 dest names label names weight names o = order 1 o >= 0 o ++history indexes o < num labels history o = output alphabet lookup history indexes o o > 0 history indexes o = 0 history o = label0 i = 0 i < order i++ history i = start concat labels history state names = output alphabet size s = 0 s < output alphabet size s++ state names s = output alphabet lookup s s = 0 s < output alphabet size s++ add state state names s 0 0 0 0 state names state names state names start state get state name name2state get name set weights weights index sparse vector transition weights weights structure changed weights index >= weights length || weights index < 0 illegal argument weights index +weights index+ out bounds weights weights index = transition weights set weights weight name sparse vector transition weights set weights get weights index weight name transition weights get weights name weight index weight alphabet lookup weight index sparse vector get weights weight name weights get weights index weight name sparse vector get weights weight index weights weight index get weights weights sparse vector get weights weights set weights sparse vector m weights structure changed weights = m set weights w weights structure changed weights = w set weight widx val weights value changed weights widx = val support making optimize optimizable r fs weights frozen weights index weights frozen weights index freezes a set weights to their current values frozen weights are used labeling sequences in <tt>transduce< tt> but are not be modified <tt>train< tt> weights index index weight set to freeze freeze weights weights index weights frozen weights index = freezes a set weights to their current values frozen weights are used labeling sequences in <tt>transduce< tt> but are not be modified <tt>train< tt> weights name name weight set to freeze freeze weights weights name widx = get weights index weights name freeze weights widx unfreezes a set weights frozen weights are used labeling sequences in <tt>transduce< tt> but are not be modified <tt>train< tt> weights name name weight set to unfreeze unfreeze weights weights name widx = get weights index weights name weights frozen widx = set feature selection weight idx feature selection fs feature selections weight idx = fs weights structure changed necessary? akm 11 2007 set weights dimension in instance list training data set weights dimension in training data gsc changing to consider when training data a mix labeled and unlabeled data and we want to use unlabeled data well to set some weights using unsupported trick note target sequence an unlabeled instance either or size zero set weights dimension in instance list training data use some unsupported trick bit set weights present num weights = 0 value doesn t actually change because will have zero value but gradient changes because now have different layout weights structure changed weights present = bit set weights length i = 0 i < weights length i++ weights present i = bit set put in weights that are already there i = 0 i < weights length i++ j = weights i num locations 1 j >= 0 j weights present i set weights i index at location j put in weights in training set i = 0 i < training data size i++ instance instance = training data get i feature vector sequence input = feature vector sequence instance get data feature sequence output = feature sequence instance get target gsc training data can have unlabeled instances well output != output size > 0 it paths consistent labels sum lattice factory sum lattice input output transducer incrementor increment transition transducer transition iterator ti count state source = r f state ti get source state feature vector input = feature vector ti get input index = ti get index nwi = source weights indices index length wi = 0 wi < nwi wi++ weights index = source weights indices index wi i = 0 i < input num locations i++ feature index = input index at location i global feature selection == || global feature selection contains feature index feature selections == || feature selections weights index == || feature selections weights index contains feature index weights present weights index set feature index increment initial state transducer state s count increment state transducer state s count and also it paths selected current model so we will get some negative weights use some unsupported trick get abs norm > 0 i == 0 logger info r f incremental training detected adding weights some unsupported features once some training done sum lattice factory sum lattice input transducer incrementor increment transition transducer transition iterator ti count count < 0 2 only create features transitions probability above 0 2 0 2 somewhat arbitrary akm state source = r f state ti get source state feature vector input = feature vector ti get input index = ti get index nwi = source weights indices index length wi = 0 wi < nwi wi++ weights index = source weights indices index wi i = 0 i < input num locations i++ feature index = input index at location i global feature selection == || global feature selection contains feature index feature selections == || feature selections weights index == || feature selections weights index contains feature index weights present weights index set feature index increment initial state transducer state s count increment state transducer state s count sparse vector weights = sparse vector weights length i = 0 i < weights length i++ num locations = weights present i cardinality logger info r f weights +parameters weight alphabet lookup i + num features = +num locations indices = num locations j = 0 j < num locations j++ indices j = weights present i next set bit j == 0 ? 0 indices j 1 +1 out r f4 has index +indices j weights i = indexed sparse vector indices num locations num locations num locations weights i plus equals sparse weights i put in previous weights num weights += num locations + 1 logger info number weights = +num weights weights = weights set weights dimension densely weights structure changed sparse vector weights = sparse vector weights length max = input alphabet size num weights = 0 logger info r f using dense weights num input features = +max i = 0 i < weights length i++ nfeatures feature selections i == nfeatures = max weights i = sparse vector max max max respect feature selection feature selection fs = feature selections i nfeatures = fs get bit set cardinality idxs = nfeatures j = 0 idx = 1 idx = fs next selected index idx + 1 >= 0 idxs j++ = idx weights i = indexed sparse vector idxs nfeatures nfeatures nfeatures weights i plus equals sparse weights i num weights += nfeatures + 1 logger info number weights = +num weights weights = weights create a weight vector weight name get weights index weight name wi = weight alphabet lookup index weight name wi == 1 illegal argument alphabet frozen and no weight name + weight name weights == wi == 0 weights = sparse vector 1 weights = 1 feature selections = feature selection 1 weights frozen = 1 use initial capacity 8 weights 0 = indexed sparse vector weights 0 = 0 feature selections 0 = weights structure changed wi == weights length sparse vector weights = sparse vector weights length+1 weights = weights length+1 feature selection feature selections = feature selection weights length+1 i = 0 i < weights length i++ weights i = weights i weights i = weights i feature selections i = feature selections i weights wi = indexed sparse vector weights wi = 0 feature selections wi = weights = weights weights = weights feature selections = feature selections weights frozen = utils append weights frozen weights structure changed set trainable wi weights length weights != weights != feature selections != weights frozen != n = weights length weights length == n feature selections length == n weights frozen length == n num states states size transducer state get state index states get index iterator initial state iterator initial states iterator trainable gsc accessor get weights value change stamp weights value change stamp kedar access structure stamp get weights structure change stamp weights structure change stamp factors get gsc get abs norm ret = 0 i = 0 i < num states i++ ret += math abs initial weights i ret += math abs weights i i = 0 i < weights length i++ ret += math abs weights i ret += weights i abs norm ret only sets parameter from first group set parameter source state index dest state index feature index value set parameter source state index dest state index feature index 0 value set parameter source state index dest state index feature index weight index value weights value changed state source = state get state source state index state dest = state get state dest state index row index row index = 0 row index < source destination names length row index++ source destination names row index equals dest name row index == source destination names length illegal argument no transtition from state +source state index+ to state +dest state index+ weights index = source weights indices row index weight index feature index < 0 weights weights index = value weights weights index set value feature index value only gets parameter from first group get parameter source state index dest state index feature index get parameter source state index dest state index feature index 0 get parameter source state index dest state index feature index weight index state source = state get state source state index state dest = state get state dest state index row index row index = 0 row index < source destination names length row index++ source destination names row index equals dest name row index == source destination names length illegal argument no transtition from state +source state index+ to state +dest state index+ weights index = source weights indices row index weight index feature index < 0 weights weights index weights weights index value feature index get num cached num stamp != weights structure change stamp num = 2 num states + weights length i = 0 i < weights length i++ num += weights i num locations num deprecated but it here a reminder to something about induce features deprecated sequence predict instance list testing testing set feature selection global feature selection i = 0 i < feature inducers size i++ feature inducer klfi = feature inducer feature inducers get i klfi induce features testing sequence ret = sequence testing size i = 0 i < testing size i++ instance instance = testing get i sequence input = sequence instance get data sequence output = sequence instance get target input size == output size sequence pred output = max lattice input best output sequence pred output size == output size ret i = pred output ret deprecated deprecated evaluate transducer evaluator eval instance list testing illegal state no longer usable use r f induce features instead testing set feature selection global feature selection i = 0 i < feature inducers size i++ feature inducer klfi = feature inducer feature inducers get i klfi induce features testing eval evaluate 0 0 0 testing when r f has done feature induction these feature conjunctions must be created in test or validation data in order them to take effect induce features instance list instances instances set feature selection global feature selection i = 0 i < feature inducers size i++ feature inducer klfi = feature inducers get i klfi induce features instances t o d o put support to optimizable here including get value instance list ?? print print print writer output stream writer out print print writer out out r f s t a t e s i = 0 i < num states i++ state s = state get state i out print s t a t e n a m e=\ out print s name out print \ out print s destinations length out print outgoing transitions out print out print initial weight = out print initial weights i out print out print out print weight = out print weights i out print out transitions j = 0 j < s destinations length j++ out print out print s name out print > out s get destination state j get name k = 0 k < s weights indices j length k++ out print w e i g h t s = \ widx = s weights indices j k out print weight alphabet lookup widx to out print \ out weights == out n o w e i g h t s out r f w e i g h t s widx = 0 widx < weights length widx++ out w e i g h t s n a m e = + weight alphabet lookup widx out print < d e f a u l t f e a t u r e> = out print weights widx out print sparse vector transition weights = weights widx transition weights num locations == 0 ranked feature vector rfv = ranked feature vector input alphabet transition weights m = 0 m < rfv num locations m++ v = rfv get value at rank m index = rfv index at location rfv get index at rank m doesn t make any sense how did ever work? akm 12 2007 index = rfv get index at rank m feature = input alphabet lookup index v != 0 out print out print feature out print = out v out flush write f output stream oos = output stream output stream f oos write oos close i o e err writing + f + + e gsc serialization r f serial u = 1 u r r e n t s e r i a l v e r s i o n = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write input alphabet out write output alphabet out write states out write initial states out write name2state out write out write global feature selection out write feature selections out write feature inducers out write weights value change stamp out write weights structure change stamp out write cached num stamp out write num suppress warnings unchecked read input stream in i o not found in read input alphabet = alphabet in read output alphabet = alphabet in read states = list< state> in read initial states = list< state> in read name2state = hash map in read = factors in read global feature selection = feature selection in read feature selections = feature selection in read feature inducers = list< feature inducer> in read weights value change stamp = in read weights structure change stamp = in read cached num stamp = in read num = in read why ? couldn t it be a non inner class? in transducer also akm 12 2007 state transducer state serializable indexed destination state feature index name index destination names state destinations n b elements are until get destination state called weights indices contains indices into r f weights labels r f crf no constructor so serialization works state state name index initial weight weight destination names label names weight names r f crf destination names length == label names length destination names length == weight names length name = name index = index note setting these here actually redundant they were set already in r f add state i m considering removing initial weight and weight arguments to constructor but need to think more akm 12 2007 r f state were non then constructor could add state to list states and put it in name2state also crf initial weights index = initial weight crf weights index = weight destination names = destination names length destinations = state label names length weights indices = label names length labels = label names length crf = crf i = 0 i < label names length i++ make sure label appears in our output alphabet crf output alphabet lookup index label names i destination names i = destination names i labels i = label names i weights indices i = weight names i length j = 0 j < weight names i length j++ weights indices i j = crf get weights index weight names i j crf weights structure changed transducer get transducer crf get initial weight crf initial weights index set initial weight crf initial weights index = get weight crf weights index set weight crf weights index = print out state # +index+ \ +name+ \ out initial weight= +crf initial weights index + weight= +crf weights index out #destinations= +destinations length i = 0 i < destinations length i++ out > +destination names i num destinations destinations length get weight names index indices = weights indices index ret = indices length i=0 i < ret length i++ ret i = crf weight alphabet lookup indices i to ret add weight didx weight name widx = crf get weights index weight name weights indices didx = utils append weights indices didx widx get label name index labels index state get destination state index state ret ret = destinations index == ret = destinations index = crf name2state get destination names index ret == illegal argument name= +this name+ index= +index+ destination names index = +destination names index + name2state size = + crf name2state size ret transducer transition iterator transition iterator sequence input sequence input position sequence output sequence output position input position < 0 || output position < 0 unsupported operation epsilon transitions not input sequence == unsupported operation r fs are not generative models must have an input sequence transition iterator feature vector sequence input sequence input position output sequence == ? output sequence get output position crf transducer transition iterator transition iterator feature vector fv output transition iterator fv output crf get name name to make it efficient inside increment transition get index index serialization state serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write name out write index out write destination names out write destinations out write weights indices out write labels out write crf read input stream in i o not found in read name = in read index = in read destination names = in read destinations = r f state in read weights indices = in read labels = in read crf = r f in read transition iterator transducer transition iterator serializable state source index next index weights feature vector input r f crf transition iterator state source feature vector sequence input seq input position output r f crf source input seq get input position output crf transition iterator state source feature vector fv output r f crf source = source crf = crf input = fv weights = source destinations length nwi swi trans index = 0 trans index < source destinations length trans index++ xxx or we want output equals here? output == || output equals source labels trans index here dot product feature weights lambda weights one transition weights trans index = 0 nwi = source weights indices trans index length wi = 0 wi < nwi wi++ swi = source weights indices trans index wi weights trans index += crf weights swi dot product fv include implicit weight 1 0 feature + crf weights swi ! na n weights trans index weights trans index != p o s i t i v e i n f i n i t y weights trans index = i m p o s s i b l e w e i g h t prepare next index pointing at next non impossible transition next index = 0 next index < source destinations length weights next index == i m p o s s i b l e w e i g h t next index++ has next next index < source destinations length transducer state next state next index < source destinations length index = next index next index++ next index < source destinations length weights next index == i m p o s s i b l e w e i g h t next index++ source get destination state index these s are just to to make more efficient perhaps some them will have to go away get index index get input input get output source labels index get weight weights index transducer state get source state source transducer state get destination state source get destination state index serialization transition iterator serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write source out write index out write next index out write weights out write input out write crf read input stream in i o not found in read source = state in read index = in read next index = in read weights = in read input = feature vector in read crf = r f in read describe transition cutoff decimal format f = decimal format 0 ### buffer buf = buffer buf append value + f format get weight + <br > these weights = source weights indices index i = 0 i < these weights length i++ wi = these weights i sparse vector w = crf weights wi buf append w e i g h t s <br > + crf weight alphabet lookup wi + <br > buf append d p = +f format w dot product input + <br > vals = input num locations abs vals = input num locations k = 0 k < vals length k++ index = input index at location k vals k = w value index input value index abs vals k = math abs vals k buf append d e f a u l t + f format crf weights wi + <br > ranked feature vector rfv = ranked feature vector crf input alphabet input get indices abs vals rank = 0 rank < abs vals length rank++ fidx = rfv get index at rank rank fname = crf input alphabet lookup input index at location fidx abs vals fidx < cutoff looping over features vals fidx != 0 buf append fname + + f format vals fidx + <br > e err writing transition descriptions e print stack trace buf append e r r o r w h i l e w r i t i n g o u t p u t buf to 