topics 2005 m a l l e t m achine languag e ~mccallum 1 0 furtherinformation ` l i e n s e types randoms arrays io text number format four level pachinko allocation m l e based on andrew s latent dirichlet allocation author david mimno p a m4 l num topics number topics to be fit num sub topics alpha dirichlet alpha alpha over supertopics alpha sum sub alphas sub alpha sums beta prior on per topic multinomial over words v beta data instance list ilist data field instances expected to hold a feature sequence num types num tokens gibbs sampling state these could be shorts or we could encode both in one topics indexed <document index sequence index> sub topics indexed <document index sequence index> per document state variables sub counts # words per <super sub> counts # words per <super> weights component gibbs update that depends on topics sub weights component gibbs update that depends on sub topics sub weights unnormalized sampling cumulative weights a cache cumulative weight each topic per word type state variables type sub topic counts indexed <feature index topic index> tokens per sub topic indexed <topic index> debugging purposes tokens per topic indexed <topic index> tokens per sub topic histograms m l e topic histograms histogram # words per supertopic in documents eg 17 4 # docs 4 words in s t 17 sub topic histograms each supertopic histogram # words per subtopic runtime runtime number format formatter p a m4 l topics sub topics topics sub topics 50 0 0 001 p a m4 l topics sub topics alpha sum beta formatter = number format get instance formatter set maximum fraction digits 5 num topics = topics num sub topics = sub topics alpha sum = alpha sum alpha = topics arrays fill alpha alpha sum num topics sub alphas = topics sub topics sub alpha sums = topics initialize sub topic alphas to a symmetric dirichlet topic = 0 topic < topics topic++ arrays fill sub alphas topic 1 0 arrays fill sub alpha sums sub topics beta = beta we can t calculate v beta until we know how many word types runtime = runtime get runtime estimate instance list documents num iterations optimize interval show topics interval output model interval output model filename randoms r ilist = documents num types = ilist get data alphabet size num docs = ilist size topics = num docs sub topics = num docs allocate several arrays use within each document to cut down memory allocation and garbage collection time sub counts = num topics num sub topics counts = num topics weights = num topics sub weights = num sub topics sub weights = num topics num sub topics cumulative weights = num topics type sub topic counts = num types num sub topics tokens per sub topic = num sub topics tokens per topic = num topics tokens per sub topic = num topics num sub topics v beta = beta num types start time = current time millis max tokens = 0 initialize random assignments tokens to topics and finish allocating topics and tokens topic sub topic seq len di = 0 di < num docs di++ feature sequence fs = feature sequence ilist get di get data seq len = fs get length seq len > max tokens max tokens = seq len num tokens += seq len topics di = seq len sub topics di = seq len randomly assign tokens to topics si = 0 si < seq len si++ random topic topic = r next num topics topics di si = topic tokens per topic topic ++ random sub topic sub topic = r next num sub topics sub topics di si = sub topic sub topic we also need to update word type statistics type sub topic counts fs get index at position si sub topic ++ tokens per sub topic sub topic ++ tokens per sub topic topic sub topic ++ out max tokens + max tokens these will be initialized at first call to clear histograms in loop below topic histograms = num topics max tokens + 1 sub topic histograms = num topics num sub topics max tokens + 1 start sampler! iterations = 0 iterations < num iterations iterations++ iteration start = current time millis clear histograms sample topics all docs r there are a few things we on round numbered iterations that t make sense first iteration iterations > 0 show topics interval != 0 iterations % show topics interval == 0 out print top words 5 output model interval != 0 iterations % output model interval == 0 write output model filename+ +iterations optimize interval != 0 iterations % optimize interval == 0 optimize time = current time millis topic = 0 topic < num topics topic++ learn sub alphas topic sub topic histograms topic topic histograms topic sub alpha sums topic = 0 0 sub topic = 0 sub topic < num sub topics sub topic++ sub alpha sums topic += sub alphas topic sub topic out print o + current time millis optimize time + iterations > 1107 print word counts iterations % 10 == 0 out < + iterations + > out print current time millis iteration start + out print out flush seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds 124 5 seconds 144 8 seconds after using feature sequence instead tokens 121 6 seconds after putting on feature sequence get index at position 106 3 seconds after avoiding lookup in inner loop a temporary variable clear histograms topic = 0 topic < num topics topic++ arrays fill topic histograms topic 0 sub topic = 0 sub topic < num sub topics sub topic++ arrays fill sub topic histograms topic sub topic 0 use fixed point iteration described tom minka learn observations observation lengths i k sum = 0 initialize parameter sum k=0 k < length k++ sum += k old k current digamma denominator histogram non zero limit non zero limits = observations length arrays fill non zero limits 1 histogram arrays go up to size largest document but non zero values will almost always cluster in low end we avoid looping over empty arrays saving index largest non zero value i=0 i<observations length i++ histogram = observations i k = 0 k < histogram length k++ histogram k > 0 non zero limits i = k iteration=0 iteration<200 iteration++ calculate denominator denominator = 0 current digamma = 0 iterate over histogram i=1 i<observation lengths length i++ current digamma += 1 sum + i 1 denominator += observation lengths i current digamma na n denominator out parameter sum i=1 i < observation lengths length i++ out print observation lengths i + out calculate individual sum = 0 k=0 k<parameters length k++ what s largest non zero element in histogram? non zero limit = non zero limits k there are no tokens assigned to sub pair anywhere in corpus bail non zero limit == 1 k = 0 000001 sum += 0 000001 old k = k k = 0 current digamma = 0 histogram = observations k i=1 i <= non zero limit i++ current digamma += 1 old k + i 1 k += histogram i current digamma k = old k denominator na n k out k = + old k + + denominator i=1 i < histogram length i++ out print histogram i + out sum += k one iteration gibbs sampling across all documents sample topics all docs randoms r loop over every word in corpus di = 0 di < topics length di++ sample topics one doc feature sequence ilist get di get data topics di sub topics di r sample topics one doc feature sequence one doc tokens topics indexed seq position sub topics randoms r start time = current time millis current type sub topic counts current sub counts current sub weights current sub alpha type sub topic topic current weight cumulative weight sample doc len = one doc tokens get length t = 0 t < num topics t++ arrays fill sub counts t 0 arrays fill counts 0 populate topic counts si = 0 si < doc len si++ sub counts topics si sub topics si ++ counts topics si ++ iterate over positions words in document si = 0 si < doc len si++ type = one doc tokens get index at position si topic = topics si sub topic = sub topics si remove token from all counts sub counts topic sub topic counts topic type sub topic counts type sub topic tokens per topic topic tokens per sub topic sub topic tokens per sub topic topic sub topic build a over sub topic pairs token clear data structures t = 0 t < num topics t++ arrays fill sub weights t 0 0 arrays fill weights 0 0 arrays fill sub weights 0 0 arrays fill cumulative weights 0 0 avoid two layer ie accesses current type sub topic counts = type sub topic counts type conditional probability each sub pair proportional to an expression three parts one that depends only on topic one that depends only on sub topic and word type and one that depends on sub pair calculate each only factors first topic = 0 topic < num topics topic++ weights topic = counts topic + alpha topic counts topic + sub alpha sums topic next calculate sub only factors sub topic = 0 sub topic < num sub topics sub topic++ sub weights sub topic = current type sub topic counts sub topic + beta tokens per sub topic sub topic + v beta put them together cumulative weight = 0 0 topic = 0 topic < num topics topic++ current sub weights = sub weights topic current sub counts = sub counts topic current sub alpha = sub alphas topic current weight = weights topic sub topic = 0 sub topic < num sub topics sub topic++ current sub weights sub topic = current weight sub weights sub topic current sub counts sub topic + current sub alpha sub topic cumulative weight += current sub weights sub topic cumulative weights topic = cumulative weight sample a topic assignment from sample = r next uniform cumulative weight go over row sums to find topic topic = 0 sample > cumulative weights topic topic++ now read across to find sub topic current sub weights = sub weights topic cumulative weight = cumulative weights topic current sub weights 0 go over each sub topic until weight l e s s than sample note that we re subtracting weights in same order we added them sub topic = 0 sample < cumulative weight sub topic++ cumulative weight = current sub weights sub topic save choice into gibbs state topics si = topic sub topics si = sub topic put sub topics into counts sub counts topic sub topic ++ counts topic ++ type sub topic counts type sub topic ++ tokens per topic topic ++ tokens per sub topic sub topic ++ tokens per sub topic topic sub topic ++ update topic count histograms dirichlet estimation topic = 0 topic < num topics topic++ topic histograms topic counts topic ++ current sub counts = sub counts topic sub topic = 0 sub topic < num sub topics sub topic++ sub topic histograms topic sub topic current sub counts sub topic ++ print word counts sub topic topic buffer output = buffer topic = 0 topic < num topics topic++ sub topic = 0 sub topic < num sub topics sub topic++ output append tokens per sub topic topic sub topic + + formatter format sub alphas topic sub topic + output append out output print top words num words use lines sorter wp = sorter num types sorter sorted sub topics = sorter num sub topics sub topic = num sub topics sub topic topic sub topic = 0 sub topic < num sub topics sub topic++ wi = 0 wi < num types wi++ wp wi = sorter wi type sub topic counts wi sub topic tokens per sub topic sub topic arrays sort wp buffer topic = buffer i = 0 i < num words i++ topic append ilist get data alphabet lookup wp i wi topic append sub topic sub topic = topic to use lines out topic + sub topic i = 0 i < num words i++ out ilist get data alphabet lookup wp i wi to + + formatter format wp i p out topic + sub topic + + tokens per sub topic sub topic + + sub topic sub topic max sub topics = 10 num sub topics < 10 max sub topics = num sub topics topic = 0 topic < num topics topic++ sub topic = 0 sub topic < num sub topics sub topic++ sorted sub topics sub topic = sorter sub topic sub alphas topic sub topic arrays sort sorted sub topics out topic + topic + + tokens per topic topic + i = 0 i < max sub topics i++ sub topic = sorted sub topics i wi out sub topic + + formatter format sub alphas topic sub topic + + sub topic sub topic print document topics f i o print document topics print writer buffered writer writer f 0 0 1 looks broken d m print document topics print writer pw threshold max pw #doc source subtopic proportions supertopic proportions doc len topic dist = num topics sub topic dist = num sub topics di = 0 di < topics length di++ pw print di pw print doc len = topics di length ilist get di get source != pw print ilist get di get source to pw print source pw print doc len = sub topics di length populate per document topic counts si = 0 si < doc len si++ topic dist topics di si += 1 0 sub topic dist sub topics di si += 1 0 ti = 0 ti < num topics ti++ topic dist ti = doc len ti = 0 ti < num sub topics ti++ sub topic dist ti = doc len print subtopic prortions sorted max < 0 max = num sub topics tp = 0 tp < max tp++ maxvalue = 0 maxindex = 1 ti = 0 ti < num sub topics ti++ sub topic dist ti > maxvalue maxvalue = sub topic dist ti maxindex = ti maxindex == 1 || sub topic dist maxindex < threshold pw print maxindex+ +sub topic dist maxindex + sub topic dist maxindex = 0 pw print print supertopic prortions sorted max < 0 max = num topics tp = 0 tp < max tp++ maxvalue = 0 maxindex = 1 ti = 0 ti < num topics ti++ topic dist ti > maxvalue maxvalue = topic dist ti maxindex = ti maxindex == 1 || topic dist maxindex < threshold pw print maxindex+ +super topic dist maxindex + topic dist maxindex = 0 pw print state f i o print state print writer buffered writer writer f print state print writer pw alphabet a = ilist get data alphabet pw #doc pos typeindex type topic sub topic di = 0 di < topics length di++ feature sequence fs = feature sequence ilist get di get data si = 0 si < topics di length si++ type = fs get index at position si pw print di pw print pw print si pw print pw print type pw print pw print a lookup type pw print pw print topics di si pw print pw print sub topics di si pw pw close write f output stream oos = output stream output stream f oos write oos close i o e err writing + f + + e serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write ilist out write num topics out write alpha out write beta out write v beta di = 0 di < topics length di ++ si = 0 si < topics di length si++ out write topics di si fi = 0 fi < num types fi++ ti = 0 ti < num topics ti++ out write type topic counts fi ti ti = 0 ti < num topics ti++ out write tokens per topic ti read input stream in i o not found features length = in read ilist = instance list in read num topics = in read alpha = in read beta = in read v beta = in read num docs = ilist size topics = num docs di = 0 di < ilist size di++ doc len = feature sequence ilist get instance di get data get length topics di = doc len si = 0 si < doc len si++ topics di si = in read num types = ilist get data alphabet size type topic counts = num types num topics fi = 0 fi < num types fi++ ti = 0 ti < num topics ti++ type topic counts fi ti = in read tokens per topic = num topics ti = 0 ti < num topics ti++ tokens per topic ti = in read recommended to use bin vectors2topics instead i o instance list ilist = instance list load 0 num iterations = length > 1 ? parse 1 1000 num top words = length > 2 ? parse 2 20 num topics = length > 3 ? parse 3 10 num sub topics = length > 4 ? parse 4 10 out data loaded p a m4 l pam = p a m4 l num topics num sub topics pam estimate ilist num iterations 50 0 50 randoms should be 1100 pam print top words num top words pam print document topics 0 + pam sorter comparable wi p sorter wi p wi = wi p = p compare to o2 p > sorter o2 p 1 p == sorter o2 p 0 1 