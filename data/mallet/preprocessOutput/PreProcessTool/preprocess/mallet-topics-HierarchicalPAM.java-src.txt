topics types arrays io text number format logging hierarchical p a m where each node in d a g has a over all topics on next level and one additional node specific topic author david mimno hierarchical p a m logger logger = logger get logger hierarchical p a m get name command option input = command option hierarchical p a m input f i l e n a m e filename from which to read list training instances use stdin + instances must be feature sequence or feature sequence bigrams not feature vector command option state = command option hierarchical p a m output state f i l e n a m e filename in which to write gibbs sampling state after at end iterations + indicating that no will be written command option topic balance option = command option hierarchical p a m topic balance d e i m a l 1 0 weight in \ words\ shared over topics relative to document specific command option sub topic balance option = command option hierarchical p a m sub topic balance d e i m a l 1 0 weight in \ words\ shared over sub topics each topic relative to document specific command option num topics option = command option hierarchical p a m num topics i n t e g e r 10 number topics command option num sub topics option = command option hierarchical p a m num sub topics i n t e g e r 20 number sub topics n u m l e v e l s = 3 constants to determine level output multinomial r o o t t o p i = 0 s u p e r t o p i = 1 s u b t o p i = 2 num topics number topics to be fit num sub topics parameter controls balance between local document counts and global over topics topic balance = 1 0 parameter smoothing on that global topic smoothing = 1 0 and same sub topics sub topic balance = 1 0 sub topic smoothing = 1 0 prior on per topic multinomial over words beta beta sum data instance list instances data field instances expected to hold a feature sequence num types num tokens gibbs sampling state these could be shorts or we could encode both in one topics indexed <document index sequence index> sub topics indexed <document index sequence index> per document state variables sub counts # words per <super sub> counts # words per <super> weights component gibbs update that depends on topics sub weights component gibbs update that depends on sub topics sub weights unnormalized sampling cumulative weights a cache cumulative weight each topic document frequencies used minimal path hierarchical dirichlets topic document frequencies sub topic document frequencies and their sums sum document frequencies sum topic document frequencies note that last not same topic document frequencies cached priors topic prior weights sub topic prior weights per word type state variables type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> tokens per topic indexed <topic index> tokens per sub topic runtime runtime number format formatter hierarchical p a m topics sub topics topic balance sub topic balance formatter = number format get instance formatter set maximum fraction digits 5 topic balance = topic balance sub topic balance = sub topic balance num topics = topics num sub topics = sub topics topic document frequencies = num topics + 1 sub topic document frequencies = num topics + 1 num sub topics + 1 sum topic document frequencies = num topics beta = 0 01 we can t calculate beta sum until we know how many word types runtime = runtime get runtime estimate instance list documents instance list testing num iterations show topics interval output model interval optimize interval output model filename randoms r instances = documents num types = instances get data alphabet size num docs = instances size topics = num docs sub topics = num docs allocate several arrays use within each document to cut down memory allocation and garbage collection time sub counts = num topics + 1 num sub topics + 1 counts = num topics + 1 weights = num topics + 1 sub weights = num sub topics sub weights = num topics + 1 num sub topics + 1 cumulative weights = num topics type topic counts = num types 1 + num topics + num sub topics tokens per topic = 1 + num topics + num sub topics tokens per topic = num topics + 1 tokens per sub topic = num topics + 1 num sub topics + 1 beta sum = beta num types start time = current time millis max tokens = 0 initialize random assignments tokens to topics and finish allocating topics and tokens topic sub topic seq len doc = 0 doc < num docs doc++ local tokens per topic = num topics + 1 local tokens per sub topic = num topics + 1 num sub topics + 1 feature sequence fs = feature sequence instances get doc get data seq len = fs get length seq len > max tokens max tokens = seq len num tokens += seq len topics doc = seq len sub topics doc = seq len randomly assign tokens to topics position = 0 position < seq len position++ random topic topic = r next num topics random sub topic sub topic = r next num sub topics level = r next n u m l e v e l s level == r o o t t o p i topics doc position = num topics sub topics doc position = num sub topics type topic counts fs get index at position position 0 ++ tokens per topic 0 ++ tokens per topic num topics ++ tokens per sub topic num topics num sub topics ++ local tokens per topic num topics == 0 topic document frequencies num topics ++ sum document frequencies++ local tokens per topic num topics ++ level == s u p e r t o p i topics doc position = topic sub topics doc position = num sub topics type topic counts fs get index at position position 1 + topic ++ tokens per topic 1 + topic ++ tokens per topic topic ++ tokens per sub topic topic num sub topics ++ local tokens per topic topic == 0 topic document frequencies topic ++ sum document frequencies++ local tokens per topic topic ++ local tokens per sub topic topic num sub topics == 0 sub topic document frequencies topic num sub topics ++ sum topic document frequencies topic ++ local tokens per sub topic topic num sub topics ++ topics doc position = topic sub topics doc position = sub topic type topic counts fs get index at position position 1 + num topics + sub topic ++ tokens per topic 1 + num topics + sub topic ++ tokens per topic topic ++ tokens per sub topic topic sub topic ++ local tokens per topic topic == 0 topic document frequencies topic ++ sum document frequencies++ local tokens per topic topic ++ local tokens per sub topic topic sub topic == 0 sub topic document frequencies topic sub topic ++ sum topic document frequencies topic ++ local tokens per sub topic topic sub topic ++ initialize cached priors topic prior weights = num topics + 1 sub topic prior weights = num topics num sub topics + 1 cache topic prior topic = 0 topic < num topics topic++ cache sub topic prior topic start sampler! iterations = 1 iterations < num iterations iterations++ iteration start = current time millis loop over every word in corpus doc = 0 doc < topics length doc++ sample topics one doc feature sequence instances get doc get data topics doc sub topics doc r show topics interval != 0 iterations % show topics interval == 0 logger info print top words 8 logger fine current time millis iteration start + iterations % 10 == 0 logger info < + iterations + > l l + formatter format model log likelihood num tokens cache topic prior topic = 0 topic < num topics topic++ topic prior weights topic = topic document frequencies topic + topic smoothing sum document frequencies + num topics + 1 topic smoothing topic prior weights num topics = topic document frequencies num topics + topic smoothing sum document frequencies + num topics + 1 topic smoothing cache sub topic prior topic document frequencies = sub topic document frequencies topic sub topic = 0 sub topic < num sub topics sub topic++ sub topic prior weights topic sub topic = document frequencies sub topic + sub topic smoothing sum topic document frequencies topic + num sub topics + 1 sub topic smoothing sub topic prior weights topic num sub topics = document frequencies num sub topics + sub topic smoothing sum topic document frequencies topic + num sub topics + 1 sub topic smoothing sample topics one doc feature sequence one doc tokens topics indexed seq position sub topics randoms r start time = current time millis current type topic counts current sub counts current sub weights word weights = 1 + num topics + num sub topics type sub topic topic root weight current weight cumulative weight sample doc len = one doc tokens get length arrays fill counts 0 t = 0 t < num topics t++ arrays fill sub counts t 0 populate topic counts position = 0 position < doc len position++ sub counts topics position sub topics position ++ counts topics position ++ topic = 0 topic < num topics topic++ weights topic = counts topic + topic balance topic prior weights topic counts topic + sub topic balance weights topic != 0 0 iterate over positions words in document position = 0 position < doc len position++ type = one doc tokens get index at position position current type topic counts = type topic counts type topic = topics position sub topic = sub topics position topic == num topics current type topic counts 0 tokens per topic 0 sub topic == num sub topics current type topic counts 1 + topic tokens per topic 1 + topic current type topic counts 1 + num topics + sub topic tokens per topic 1 + num topics + sub topic remove token from all counts counts topic sub counts topic sub topic counts topic == 0 document frequencies have changed decrement and recalculate prior weights topic document frequencies topic sum document frequencies cache topic prior topic != num topics sub counts topic sub topic == 0 sub topic document frequencies topic sub topic sum topic document frequencies topic cache sub topic prior topic tokens per topic topic tokens per sub topic topic sub topic update topic weight old topic weights topic = counts topic + topic balance topic prior weights topic counts topic + sub topic balance build a over sub topic pairs token i=0 i<word weights length i++ word weights i = beta + current type topic counts i beta sum + tokens per topic i word weights i != 0 arrays fill cumulative weights 0 0 conditional probability each sub pair proportional to an expression three parts one that depends only on topic one that depends only on sub topic and word type and one that depends on sub pair cumulative weight = 0 0 topic = 0 topic < num topics topic++ current sub weights = sub weights topic current sub counts = sub counts topic current weight = weights topic document frequencies = sub topic document frequencies topic prior cache = sub topic prior weights topic sub topic = 0 sub topic < num sub topics sub topic++ current sub weights sub topic = current weight word weights 1 + num topics + sub topic current sub counts sub topic + sub topic balance prior cache sub topic cumulative weight += current sub weights sub topic current sub weights num sub topics = current weight word weights 1 + topic current sub counts num sub topics + sub topic balance prior cache num sub topics cumulative weight += current sub weights num sub topics cumulative weights topic = cumulative weight cumulative weights topic != 0 0 root weight = word weights 0 counts num topics + topic balance topic prior weights num topics sample a topic assignment from sample = r next uniform cumulative weight + root weight sample > cumulative weight we picked root topic current type topic counts 0 ++ tokens per topic 0 ++ topic = num topics sub topic = num sub topics go over row sums to find topic topic = 0 sample > cumulative weights topic topic++ now read across to find sub topic current sub weights = sub weights topic cumulative weight = cumulative weights topic go over each sub topic until weight l e s s than sample note that we re subtracting weights in same order we added them sub topic = 0 cumulative weight = current sub weights 0 sample < cumulative weight sub topic++ cumulative weight = current sub weights sub topic sub topic == num sub topics current type topic counts 1 + topic ++ tokens per topic 1 + topic ++ current type topic counts 1 + num topics + sub topic ++ tokens per topic 1 + num topics + sub topic ++ save choice into gibbs state topics position = topic sub topics position = sub topic put sub topics into counts sub counts topic sub topic ++ counts topic ++ counts topic == 1 topic document frequencies topic ++ sum document frequencies++ cache topic prior topic != num topics sub counts topic sub topic == 1 sub topic document frequencies topic sub topic ++ sum topic document frequencies topic ++ cache sub topic prior topic tokens per topic topic ++ tokens per sub topic topic sub topic ++ update weight topic weights topic = counts topic + topic balance topic prior weights topic counts topic + sub topic balance print top words num words use lines builder output = builder sorter sorted types = sorter num types sorter sorted sub topics = sorter num sub topics topic = 1 + num topics + num sub topics sub topic topic topic = 0 topic < topic length topic++ type = 0 type < num types type++ sorted types type = sorter type type topic counts type topic tokens per topic topic arrays sort sorted types builder = builder i = 0 i < num words i++ append instances get data alphabet lookup sorted types i get append topic topic = to max sub topics = 10 num sub topics < 10 max sub topics = num sub topics output append root + + tokens per topic num topics + + topic document frequencies num topics + + topic 0 + topic = 0 topic < num topics topic++ sub topic = 0 sub topic < num sub topics sub topic++ sorted sub topics sub topic = sorter sub topic tokens per sub topic topic sub topic arrays sort sorted sub topics output append topic + topic + + tokens per topic topic + + topic document frequencies topic + + tokens per sub topic topic num sub topics + + sub topic document frequencies topic num sub topics + + topic 1 + topic + i = 0 i < max sub topics i++ sub topic = sorted sub topics i get output append sub topic + + tokens per sub topic topic sub topic + + formatter format sub topic document frequencies topic sub topic + + topic 1 + num topics + sub topic + output to print state f i o print writer out = print writer buffered writer writer f print state out out close print state print writer out alphabet alphabet = instances get data alphabet out #doc pos typeindex type topic sub topic doc = 0 doc < topics length doc++ builder output = builder feature sequence fs = feature sequence instances get doc get data position = 0 position < topics doc length position++ type = fs get index at position position output append doc output append output append position output append output append type output append output append alphabet lookup type output append output append topics doc position output append output append sub topics doc position output append out print output model log likelihood log likelihood = 0 0 non zero topics likelihood model a combination a dirichlet multinomial words in each topic and a dirichlet multinomial topics in each document likelihood function a dirichlet multinomial gamma sum i alpha i prod i gamma alpha i + n i prod i gamma alpha i gamma sum i alpha i + n i so log likelihood log gamma sum i alpha i log gamma sum i alpha i + n i + sum i log gamma alpha i + n i log gamma alpha i documents first topic sub topic topic log gammas = num topics + 1 sub topic log gammas = num topics num sub topics + 1 topic=0 topic < num topics topic++ topic log gammas topic = dirichlet log gamma topic prior weights topic sub topic=0 sub topic < num sub topics sub topic++ sub topic log gammas topic sub topic = dirichlet log gamma sub topic balance sub topic prior weights topic sub topic sub topic log gammas topic num sub topics = dirichlet log gamma sub topic balance sub topic prior weights topic num sub topics topic log gammas num topics = dirichlet log gamma topic prior weights num topics topic counts = num topics + 1 sub topic counts = num topics num sub topics + 1 doc topics doc sub topics doc=0 doc < topics length doc++ doc topics = topics doc doc sub topics = sub topics doc token=0 token < doc topics length token++ topic = doc topics token sub topic = doc sub topics token topic counts topic ++ topic != num topics sub topic counts topic sub topic ++ topic=0 topic < num topics topic++ topic counts topic > 0 log likelihood += dirichlet log gamma topic balance topic prior weights topic + topic counts topic topic log gammas topic sub topic=0 sub topic < num sub topics sub topic++ sub topic counts topic sub topic > 0 log likelihood += dirichlet log gamma sub topic balance sub topic prior weights topic sub topic + sub topic counts topic sub topic sub topic log gammas topic sub topic account words assigned to topic log likelihood += dirichlet log gamma sub topic balance sub topic prior weights topic num sub topics + sub topic counts topic num sub topics sub topic log gammas topic num sub topics term sums log likelihood += dirichlet log gamma sub topic balance dirichlet log gamma sub topic balance + topic counts topic arrays fill sub topic counts topic 0 account words assigned to root topic log likelihood += dirichlet log gamma topic balance topic prior weights num topics + topic counts num topics topic log gammas num topics subtract count + parameter sum term log likelihood = dirichlet log gamma topic balance + doc topics length arrays fill topic counts 0 add parameter sum term every document all at once log likelihood += topics length dirichlet log gamma topic balance and topics count number type topic pairs non zero type topics = 0 type=0 type < num types type++ reuse a pointer topic counts = type topic counts type topic=0 topic < num topics + num sub topics + 1 topic++ topic counts topic > 0 non zero type topics++ log likelihood += dirichlet log gamma beta + topic counts topic topic=0 topic < num topics + num sub topics + 1 topic++ log likelihood = dirichlet log gamma beta num topics + num sub topics + 1 + tokens per topic topic log likelihood += dirichlet log gamma beta num topics + num sub topics + 1 dirichlet log gamma beta non zero type topics log likelihood i o command option set summary hierarchical p a m train a three level hierarchy topics command option process hierarchical p a m instance list instances = instance list load input value instance list testing = hierarchical p a m pam = hierarchical p a m num topics option value num sub topics option value topic balance option value sub topic balance option value pam estimate instances testing 1000 100 0 250 randoms state was invoked pam print state state value 