2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics arrays io types randoms like latent dirichlet allocation but integrated phrase discovery author andrew mc callum <a href= mailto >mccallum edu< a> based on xuerui wang topical n grams num topics alphabet uni alphabet alphabet bi alphabet alpha beta gamma delta t alpha v beta v gamma delta1 delta2 instance list ilist containing feature sequence bigrams in data field each instance topics 0 t 1 topic index indexed <document index sequence index> grams 0 1 bigram status indexed <document index sequence index> t o d o make boolean? num types number unique unigrams num bitypes number unique bigrams num tokens total number word occurrences total ngram bi tokens total number tokens currently generated bigrams only used progress messages doc topic doc topic counts indexed <document index topic index> used to calculate p x|w t ngram count type ngram topic counts indexed <feature index ngram status topic index> used to calculate p w|t and p w|t w topic word and topic ngram word unitype topic counts indexed <feature index topic index> bitype topic counts index <bifeature index topic index> sum words tokens per topic indexed <topic index> sum ngram words bitokens per topic indexed <feature index topic index> where later conditioned word topical n grams number topics number topics 50 0 0 01 0 01 0 03 0 2 1000 topical n grams number topics alpha sum beta gamma delta delta1 delta2 num topics = number topics alpha = alpha sum num topics smoothing over choice topic beta = beta smoothing over choice unigram words gamma = gamma smoothing over choice bigram words delta = delta smoothing over choice unigram bigram generation delta1 = delta1 t o d o clean up delta2 = delta2 out alpha +alpha sum out beta +beta out gamma +gamma out delta +delta out delta1 +delta1 out delta2 +delta2 estimate instance list documents num iterations show topics interval output model interval output model filename randoms r ilist = documents uni alphabet = ilist get data alphabet bi alphabet = feature sequence bigrams ilist get 0 get data get bi alphabet num types = uni alphabet size num bitypes = bi alphabet size num docs = ilist size topics = num docs grams = num docs doc topic counts = num docs num topics type ngram topic counts = num types 2 num topics unitype topic counts = num types num topics bitype topic counts = num bitypes num topics tokens per topic = num topics bitokens per topic = num types num topics t alpha = alpha num topics v beta = beta num types v gamma = gamma num types start time = current time millis initialize random assignments tokens to topics and finish allocating topics and tokens topic gram seq len fi di = 0 di < num docs di++ feature sequence bigrams fs = feature sequence bigrams ilist get di get data seq len = fs get length num tokens += seq len topics di = seq len grams di = seq len randomly assign tokens to topics prev fi = 1 prev topic = 1 si = 0 si < seq len si++ randomly sample a topic word at position si topic = r next num topics a bigram allowed at position si then sample a gram status it gram = fs get bi index at position si == 1 ? 0 r next 2 gram != 0 bi tokens++ topics di si = topic grams di si = gram doc topic counts di topic ++ fi = fs get index at position si prev fi != 1 type ngram topic counts prev fi gram prev topic ++ gram == 0 unitype topic counts fi topic ++ tokens per topic topic ++ bitype topic counts fs get bi index at position si topic ++ bitokens per topic prev fi topic ++ prev fi = fi prev topic = topic iterations = 0 iterations < num iterations iterations++ sample topics all docs r iterations % 10 == 0 out print iterations out print out flush show topics interval != 0 iterations % show topics interval == 0 iterations > 0 out print top words 5 output model interval != 0 iterations % output model interval == 0 iterations > 0 write output model filename+ +iterations out total time sec + current time millis start time 1000 0 one iteration gibbs sampling across all documents sample topics all docs randoms r uni topic weights = num topics bi topic weights = num topics 2 loop over every word in corpus di = 0 di < topics length di++ sample topics one doc feature sequence bigrams ilist get di get data topics di grams di doc topic counts di uni topic weights bi topic weights r sample topics one doc feature sequence bigrams one doc tokens one doc topics one doc grams one doc topic counts indexed topic index uni topic weights length==num topics bi topic weights length==num topics 2 joint topic gram sampling randoms r current type topic counts current bitype topic counts previous bitokens per topic type bitype old gram next gram gram old topic topic topic weights sum tw xxx doc len = one doc tokens length doc len = one doc tokens get length iterate over positions words in document si = 0 si < doc len si++ type = one doc tokens get index at position si bitype = one doc tokens get bi index at position si bitype == 1 out biblock +si+ at +uni alphabet lookup type old topic = one doc topics si old gram = one doc grams si next gram = si == doc len 1 ? 1 one doc grams si+1 next gram = si == doc len 1 ? 1 one doc tokens get bi index at position si+1 == 1 ? 0 1 bigram possible = bitype != 1 ! !bigram possible old gram == 1 !bigram possible remove token from all counts one doc topic counts old topic tokens per topic old topic unitype topic counts type old topic si != doc len 1 type ngram topic counts type next gram old topic type ngram topic counts type next gram old topic >= 0 one doc topic counts old topic >= 0 tokens per topic old topic >= 0 unitype topic counts type old topic >= 0 build a over topics token arrays fill uni topic weights 0 0 topic weights sum = 0 current type topic counts = unitype topic counts type ti = 0 ti < num topics ti++ tw = current type topic counts ti + beta tokens per topic ti + v beta one doc topic counts ti + alpha additional term constance across all topics topic weights sum += tw uni topic weights ti = tw sample a topic assignment from topic = r next discrete uni topic weights topic weights sum put that topic into counts one doc topics si = topic one doc topic counts topic ++ unitype topic counts type topic ++ tokens per topic topic ++ si != doc len 1 type ngram topic counts type next gram topic ++ bigram possible prev type = one doc tokens get index at position si 1 prev topic = one doc topics si 1 remove token from all counts one doc topic counts old topic type ngram topic counts prev type old gram prev topic si != doc len 1 type ngram topic counts type next gram old topic old gram == 0 unitype topic counts type old topic tokens per topic old topic bitype topic counts bitype old topic bitokens per topic prev type old topic bi tokens one doc topic counts old topic >= 0 type ngram topic counts prev type old gram prev topic >= 0 si == doc len 1 || type ngram topic counts type next gram old topic >= 0 unitype topic counts type old topic >= 0 tokens per topic old topic >= 0 bitype topic counts bitype old topic >= 0 bitokens per topic prev type old topic >= 0 bi tokens >= 0 build a joint over topics and ngram status token arrays fill bi topic weights 0 0 topic weights sum = 0 current type topic counts = unitype topic counts type current bitype topic counts = bitype topic counts bitype previous bitokens per topic = bitokens per topic prev type ti = 0 ti < num topics ti++ topic = ti << 1 just using variable an index into ti 2+gram unigram outcome tw = current type topic counts ti + beta tokens per topic ti + v beta one doc topic counts ti + alpha type ngram topic counts prev type 0 prev topic + delta1 topic weights sum += tw bi topic weights topic = tw bigram outcome topic++ tw = current bitype topic counts ti + gamma previous bitokens per topic ti + v gamma one doc topic counts ti + alpha type ngram topic counts prev type 1 prev topic + delta2 topic weights sum += tw bi topic weights topic = tw sample a topic assignment from topic = r next discrete bi topic weights topic weights sum put that topic into counts gram = topic % 2 topic = 2 put that topic into counts one doc topics si = topic one doc grams si = gram one doc topic counts topic ++ type ngram topic counts prev type gram prev topic ++ si != doc len 1 type ngram topic counts type next gram topic ++ gram == 0 unitype topic counts type topic ++ tokens per topic topic ++ bitype topic counts bitype topic ++ bitokens per topic prev type topic ++ bi tokens++ print top words num words use lines word prob comparable wi p word prob wi p wi = wi p = p compare to o2 p > word prob o2 p 1 p == word prob o2 p 0 1 ti = 0 ti < num topics ti++ unigrams word prob wp = word prob num types wi = 0 wi < num types wi++ wp wi = word prob wi unitype topic counts wi ti arrays sort wp num to print = math min wp length num words use lines out topic +ti+ unigrams i = 0 i < num to print i++ out uni alphabet lookup wp i wi to + + wp i p tokens per topic ti out print topic +ti+ i = 0 i < num to print i++ out print uni alphabet lookup wp i wi to + bigrams wp = word prob num bitypes bisum = 0 wi = 0 wi < num bitypes wi++ wp wi = word prob wi bitype topic counts wi ti bisum += bitype topic counts wi ti arrays sort wp num to print = math min wp length num words use lines out topic +ti+ bigrams i = 0 i < num to print i++ out bi alphabet lookup wp i wi to + + wp i p bisum out print i = 0 i < num to print i++ out print bi alphabet lookup wp i wi to + out ngrams augmentable feature vector afv = augmentable feature vector alphabet 10000 di = 0 di < topics length di++ feature sequence bigrams fs = feature sequence bigrams ilist get di get data si = topics di length 1 si >= 0 si topics di si == ti grams di si == 1 gram = uni alphabet lookup fs get index at position si to grams di si == 1 si >= 0 gram = uni alphabet lookup fs get index at position si to + + gram afv add gram 1 0 out pre sorting num ngrams = afv num locations out post sorting +num ngrams wp = word prob num ngrams ngram sum = 0 loc = 0 loc < num ngrams loc++ wp loc = word prob afv index at location loc afv value at location loc ngram sum += wp loc p arrays sort wp num unitype tokens = 0 num bitype tokens = 0 num unitype types = 0 num bitype types = 0 fi = 0 fi < num types fi++ num unitype tokens += unitype topic counts fi ti unitype topic counts fi ti != 0 num unitype types++ fi = 0 fi < num bitypes fi++ num bitype tokens += bitype topic counts fi ti bitype topic counts fi ti != 0 num bitype types++ use lines out topic +ti+ unigrams +num unitype tokens+ +num unitype types+ bigrams +num bitype tokens+ +num bitype types + phrases + math round afv one norm + +num ngrams i = 0 i < math min num ngrams num words i++ out afv get alphabet lookup wp i wi to + + wp i p ngram sum out print unigrams +num unitype tokens+ +num unitype types+ bigrams +num bitype tokens+ +num bitype types + phrases + math round afv one norm + +num ngrams+ out print unique ngrams= +num ngrams+ ngram count= + math round afv one norm + i = 0 i < math min num ngrams num words i++ out print afv get alphabet lookup wp i wi to + out print document topics f i o print document topics print writer writer f print document topics print writer pw print document topics print writer pw threshold max pw #doc source topic proportions doc len topic dist = topics length di = 0 di < topics length di++ pw print di pw print pw print ilist get di get source to pw print doc len = topics di length ti = 0 ti < num topics ti++ topic dist ti = doc topic counts di ti doc len max < 0 max = num topics tp = 0 tp < max tp++ maxvalue = 0 maxindex = 1 ti = 0 ti < num topics ti++ topic dist ti > maxvalue maxvalue = topic dist ti maxindex = ti maxindex == 1 || topic dist maxindex < threshold pw print maxindex+ +topic dist maxindex + topic dist maxindex = 0 pw print state f i o print writer writer = print writer writer f print state writer writer close print state print writer pw pw #doc pos typeindex type bigrampossible? topic bigram di = 0 di < topics length di++ feature sequence bigrams fs = feature sequence bigrams ilist get di get data si = 0 si < topics di length si++ type = fs get index at position si pw print di pw print pw print si pw print pw print type pw print pw print uni alphabet lookup type pw print pw print fs get bi index at position si == 1 ? 0 1 pw print pw print topics di si pw print pw print grams di si pw write f output stream oos = output stream output stream f oos write oos close i o e err writing + f + + e serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write array2 a output stream out i o out write a length d2 = a 0 length out write d2 i = 0 i < a length i++ j = 0 j < d2 j++ out write a i j read array2 input stream in i o d1 = in read d2 = in read a = d1 d2 i = 0 i < d1 i++ j = 0 j < d2 j++ a i j = in read a write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write ilist out write num topics out write alpha out write beta out write gamma out write delta out write t alpha out write v beta out write v gamma out write num types out write num bitypes out write num tokens out write bi tokens di = 0 di < topics length di ++ si = 0 si < topics di length si++ out write topics di si di = 0 di < topics length di ++ si = 0 si < topics di length si++ out write grams di si write array2 doc topic counts out fi = 0 fi < num types fi++ n = 0 n < 2 n++ ti = 0 ti < num topics ti++ out write type ngram topic counts fi n ti write array2 unitype topic counts out write array2 bitype topic counts out ti = 0 ti < num topics ti++ out write tokens per topic ti write array2 bitokens per topic out read input stream in i o not found features length = in read ilist = instance list in read num topics = in read alpha = in read beta = in read gamma = in read delta = in read t alpha = in read v beta = in read v gamma = in read num types = in read num bitypes = in read num tokens = in read bi tokens = in read num docs = ilist size topics = num docs grams = num docs di = 0 di < ilist size di++ doc len = feature sequence ilist get di get data get length topics di = doc len si = 0 si < doc len si++ topics di si = in read di = 0 di < ilist size di++ doc len = feature sequence ilist get di get data get length grams di = doc len si = 0 si < doc len si++ grams di si = in read doc topic counts = read array2 in type ngram topic counts = num types 2 num topics fi = 0 fi < num types fi++ n = 0 n < 2 n++ ti = 0 ti < num topics ti++ type ngram topic counts fi n ti = in read unitype topic counts = read array2 in bitype topic counts = read array2 in tokens per topic = num topics ti = 0 ti < num topics ti++ tokens per topic ti = in read bitokens per topic = read array2 in just testing recommend instead bin vectors2topics instance list ilist = instance list load 0 num iterations = length > 1 ? parse 1 1000 num top words = length > 2 ? parse 2 20 out data loaded topical n grams tng = topical n grams 10 tng estimate ilist 200 1 0 randoms tng print top words 60 