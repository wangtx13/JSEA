2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics logging zip io text number format topics types gnu trove a non parametric topic model that uses minimal path assumption to reduce bookkeeping author david mimno n p topic model serializable logger logger = logger get logger n p topic model get name training instances and their topic assignments list< topic assignment> data alphabet input data alphabet alphabet alphabet topics label alphabet topic alphabet largest topic seen so far max topic current number topics num topics size vocabulary num types prior alpha gamma beta prior on per topic multinomial over words beta sum d e f a u l t b e t a = 0 01 statistics needed sampling t hash map type topic counts indexed <feature index topic index> t hash map tokens per topic indexed <topic index> number documents that contain at least one token a given topic t hash map docs per topic total doc topics = 0 show topics interval = 50 words per topic = 10 randoms random number format formatter print log likelihood = alpha parameter balances local document topic counts global over topics gamma parameter weight on a completely never before seen topic in global beta parameter controls variability topic word distributions n p topic model alpha gamma beta data = list< topic assignment> topic alphabet = alphabet factory label alphabet size 1 alpha = alpha gamma = gamma beta = beta random = randoms tokens per topic = t hash map docs per topic = t hash map formatter = number format get instance formatter set maximum fraction digits 5 logger info non parametric l d a set topic display interval n show topics interval = interval words per topic = n set random seed seed random = randoms seed add instances instance list training initial topics alphabet = training get data alphabet num types = alphabet size beta sum = beta num types type topic counts = t hash map num types type=0 type < num types type++ type topic counts type = t hash map num topics = initial topics doc = 0 instance instance training doc++ t hash map topic counts = t hash map feature sequence tokens = feature sequence instance get data label sequence topic sequence = label sequence topic alphabet tokens size topics = topic sequence get features position = 0 position < tokens size position++ topic = random next num topics tokens per topic adjust or put value topic 1 1 topics position = topic keep track number docs at least one token in a given topic ! topic counts contains key topic docs per topic adjust or put value topic 1 1 total doc topics++ topic counts put topic 1 topic counts adjust value topic 1 type = tokens get index at position position type topic counts type adjust or put value topic 1 1 topic assignment t = topic assignment instance topic sequence data add t max topic = num topics 1 sample iterations i o iteration = 1 iteration <= iterations iteration++ iteration start = current time millis loop over every document in corpus doc = 0 doc < data size doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence sample topics one doc token sequence topic sequence elapsed millis = current time millis iteration start logger info iteration + + elapsed millis + ms + num topics occasionally print more show topics interval != 0 iteration % show topics interval == 0 logger info < + iteration + > # topics + num topics + + top words words per topic sample topics one doc feature sequence token sequence feature sequence topic sequence topics = topic sequence get features t hash map current type topic counts type old topic topic topic weights sum doc length = token sequence get length t hash map local topic counts = t hash map populate topic counts position = 0 position < doc length position++ local topic counts adjust or put value topics position 1 1 score sum topic term scores = num topics + 1 store a list all topics that currently all topics = docs per topic keys iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = topics position grab relevant row from our two dimensional current type topic counts = type topic counts type remove token from all counts current count = local topic counts get old topic was only token topic in doc? current count == 1 local topic counts remove old topic was only doc topic? doc count = docs per topic get old topic doc count == 1 should be very last token tokens per topic get old topic == 1 get rid topic docs per topic remove old topic total doc topics tokens per topic remove old topic num topics all topics = docs per topic keys topic term scores = num topics + 1 last in doc but topic still docs per topic adjust value old topic 1 total doc topics tokens per topic adjust value old topic 1 there at least one other token in doc topic local topic counts adjust value old topic 1 tokens per topic adjust value old topic 1 current type topic counts get old topic == 1 current type topic counts remove old topic current type topic counts adjust value old topic 1 now calculate and add up scores each topic word sum = 0 0 first topics that currently i = 0 i < num topics i++ topic = all topics i topic term scores i = local topic counts get topic + alpha docs per topic get topic total doc topics + gamma current type topic counts get topic + beta tokens per topic get topic + beta sum sum += topic term scores i add weight a topic topic term scores num topics = alpha gamma num types total doc topics + gamma sum += topic term scores num topics choose a random point between 0 and sum all topic scores sample = random next uniform sum figure out which topic contains that point topic = 1 i = 1 sample > 0 0 i++ sample = topic term scores i i < num topics topic = all topics i topics position = topic current type topic counts adjust or put value topic 1 1 tokens per topic adjust value topic 1 local topic counts contains key topic local topic counts adjust value topic 1 not a topic but it doc local topic counts put topic 1 docs per topic adjust value topic 1 total doc topics++ completely topic first generate an topic = max topic + 1 max topic = topic num topics++ topics position = topic local topic counts put topic 1 docs per topic put topic 1 total doc topics++ current type topic counts put topic 1 tokens per topic put topic 1 all topics = docs per topic keys topic term scores = num topics + 1 displaying and saving results top words num words builder output = builder sorter sorted words = sorter num types topic docs per topic keys type = 0 type < num types type++ sorted words type = sorter type type topic counts type get topic arrays sort sorted words output append topic + + tokens per topic get topic + i=0 i < num words i++ sorted words i get weight < 1 0 output append alphabet lookup sorted words i get + output append output to print state f i o print stream out = print stream g z i p output stream buffered output stream output stream f print state out out close print state print stream out out #doc source pos typeindex type topic doc = 0 doc < data size doc++ feature sequence token sequence = feature sequence data get doc instance get data label sequence topic sequence = label sequence data get doc topic sequence source = n a data get doc instance get source != source = data get doc instance get source to position = 0 position < topic sequence get length position++ type = token sequence get index at position position topic = topic sequence get index at position position out print doc out print out print source out print out print position out print out print type out print out print alphabet lookup type out print out print topic out i o instance list training = instance list load 0 num topics = length > 1 ? parse 1 200 n p topic model lda = n p topic model 5 0 10 0 0 1 lda add instances training num topics lda sample 1000 