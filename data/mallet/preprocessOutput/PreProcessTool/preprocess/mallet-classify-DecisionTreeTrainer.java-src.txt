2002 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e classify logging classify classifier pipe pipe types alphabet types feature selection types feature vector types instance types instance list types label vector types labeling types multinomial logger a decision tree learner roughly id3 but only to a fixed given depth in all branches does not yet implement splitting continuous valued features but it should in future currently a feature considered present it has positive value ftp ftp cmu project jair volume4 quinlan96a ps only set up conveniently decision stubs there no pruning or good stopping rule currently only stop reaching a maximum depth author andrew mc callum <a href= mailto >mccallum edu< a> decision tree trainer classifier trainer< decision tree> boostable logger logger = logger get logger decision tree trainer get name d e f a u l t m a x d e p t h = 5 d e f a u l t m i n i n f o g a i n s p l i t = 0 001 max depth = d e f a u l t m a x d e p t h min info gain split = 0 001 finished = decision tree classifier = decision tree trainer max depth max depth = max depth decision tree trainer 4 decision tree trainer set max depth max depth max depth = max depth decision tree trainer set min info gain split m min info gain split = m finished training finished decision tree get classifier classifier decision tree train instance list training list feature selection selected features = training list get feature selection decision tree node root = decision tree node training list selected features split tree root selected features 0 root stop growth finished = out decision tree learned root print classifier = decision tree training list get pipe root classifier split tree decision tree node node feature selection selected features depth depth == max depth || node get split info gain < min info gain split logger info splitting feature \ +node get split feature + \ infogain= +node get split info gain node split selected features split tree node get feature present child selected features depth+1 split tree node get feature absent child selected features depth+1 factory classifier trainer factory< decision tree trainer> max depth = d e f a u l t m a x d e p t h min info gain split = d e f a u l t m i n i n f o g a i n s p l i t recommended but cannot be enforced in that subclasses implement classifier train instance list training set classifier train instance list training set instance list validation set classifier train instance list training set instance list validation set classifier initial classifier which call decision tree trainer classifier trainer classifier initial classifier decision tree trainer t = decision tree trainer t max depth = max depth t min info gain split = min info gain split t decision tree trainer factory dtf = decision tree trainer factory max depth = 6 decision tree trainer factory dtf = decision tree trainer factory set max depth 6 set min info gain split 2 