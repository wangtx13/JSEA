classify io serializable arrays iterator logging logger optimize limited memory b f g s optimize optimizable types alphabet types feature selection types feature vector types instance types instance list types label alphabet types labeling types matrix ops logger progress message logger maths max ent optimizable label likelihood optimizable gradient value logger logger = logger get logger max ent optimizable label likelihood get name logger progress logger = progress message logger get logger max ent optimizable label likelihood get name + pl xxx why does test maximizable fail when variance very small? d e f a u l t g a u s s i a n p r i o r v a r i a n e = 1 d e f a u l t h y p e r b o l i p r i o r s l o p e = 0 2 d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s = 10 0 d e f a u l t m a x i m i z e r l a s s = limited memory b f g s using hyperbolic prior = using gaussian prior = gaussian prior variance = d e f a u l t g a u s s i a n p r i o r v a r i a n e hyperbolic prior slope = d e f a u l t h y p e r b o l i p r i o r s l o p e hyperbolic prior sharpness = d e f a u l t h y p e r b o l i p r i o r s h a r p n e s s maximizer = d e f a u l t m a x i m i z e r l a s s constraints cached gradient max ent classifier instance list training list expectations are temporarily stored in cached gradient cached value cached value stale cached gradient stale num labels num features feature index just clarity feature selection feature selection feature selection per label feature selection num get value calls = 0 num get value gradient calls = 0 max ent optimizable label likelihood max ent optimizable label likelihood instance list training set max ent initial classifier training list = training set alphabet fd = training set get data alphabet label alphabet ld = label alphabet training set get target alphabet t fd stop growth because someone might want to feature induction ld stop growth add one feature feature num labels = ld size num features = fd size + 1 feature index = num features 1 = num labels num features constraints = num labels num features cached gradient = num labels num features arrays fill 0 0 arrays fill constraints 0 0 arrays fill cached gradient 0 0 feature selection = training set get feature selection per label feature selection = training set get per label feature selection add feature index to selection feature selection != feature selection add feature index per label feature selection != i = 0 i < per label feature selection length i++ per label feature selection i add feature index xxx later change to allow both to be set but select which one to use a flag? feature selection == || per label feature selection == initial classifier != classifier = initial classifier = classifier feature selection = classifier feature selection per label feature selection = classifier per feature selection feature index = classifier feature index initial classifier get instance pipe == training set get pipe classifier == classifier = max ent training set get pipe feature selection per label feature selection cached value stale = cached gradient stale = initialize constraints logger fine number instances in training list = + training list size instance inst training list instance weight = training list get instance weight inst labeling labeling = inst get labeling labeling == logger fine instance +ii+ labeling= +labeling feature vector fv = feature vector inst get data alphabet fdict = fv get alphabet fv get alphabet == fd = labeling get best index matrix ops row plus equals constraints num features fv instance weight feature whose weight 1 0 ! na n instance weight instance weight na n ! na n best index na n has na n = i = 0 i < fv num locations i++ na n fv value at location i logger info na n feature + fdict lookup fv index at location i to has na n = has na n logger info na n in instance + inst get name constraints num features + feature index += 1 0 instance weight test maximizable test value and gradient current max ent get classifier classifier get parameter index index set parameter index v cached value stale = cached gradient stale = index = v get num length get buff buff == || buff length != length buff = length arraycopy 0 buff 0 length set buff buff != cached value stale = cached gradient stale = buff length != length = buff length arraycopy buff 0 0 buff length log probability training labels get value cached value stale num get value calls++ cached value = 0 we ll store expectation values in cached gradient now cached gradient stale = matrix ops set all cached gradient 0 0 incorporate likelihood data scores = training list get target alphabet size value = 0 0 iterator< instance> iter = training list iterator ii=0 iter has next ii++ instance instance = iter next instance weight = training list get instance weight instance labeling labeling = instance get labeling labeling == out l now +input alphabet size + regular features classifier get classification scores instance scores feature vector fv = feature vector instance get data = labeling get best index value = instance weight math log scores na n value logger fine max ent trainer instance + instance get name + has na n value log scores = + math log scores + scores = + scores + has instance weight = + instance weight infinite value logger warning instance +instance get source + has infinite value skipping value and gradient cached value = value cached value stale = value cached value += value si = 0 si < scores length si++ scores si == 0 ! infinite scores si matrix ops row plus equals cached gradient num features si fv instance weight scores si cached gradient num features si + feature index += instance weight scores si logger info expectations cached gradient print incorporate prior on prior = 0 using hyperbolic prior = 0 < num labels li++ fi = 0 fi < num features fi++ prior += hyperbolic prior slope hyperbolic prior sharpness math log maths cosh hyperbolic prior sharpness num features + fi using gaussian prior = 0 < num labels li++ fi = 0 fi < num features fi++ = num features + fi prior += 2 gaussian prior variance o value = cached value cached value += prior cached value = 1 0 m a x i m i z e n o t m i n i m i z e cached value stale = progress logger info value label prob= +o value+ prior= +prior+ loglikelihood = +cached value cached value get value gradient buffer gradient constraint expectation gaussian prior variance cached gradient stale num get value gradient calls++ cached value stale will fill in cached gradient expectation get value matrix ops plus equals cached gradient constraints incorporate prior on using hyperbolic prior unsupported operation hyperbolic prior not yet using gaussian prior matrix ops plus equals cached gradient 1 0 gaussian prior variance a parameter may be set to infinity an external user we set gradient to 0 because parameter s value can never change anyway and it will mess up future calculations on matrix such norm matrix ops substitute cached gradient n e g a t i v e i n f i n i t y 0 0 set to zero all gradient dimensions that are not among selected features per label feature selection == label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 feature selection label index = 0 label index < num labels label index++ matrix ops row set all cached gradient num features label index 0 0 per label feature selection label index cached gradient stale = buffer != buffer length == length arraycopy cached gradient 0 buffer 0 cached gradient length out max ent trainer gradient infinity norm = + matrix ops infinity norm cached gradient x x x should these really be public? why? counts how many times trainer has computed gradient log probability training labels get value gradient calls num get value gradient calls counts how many times trainer has computed log probability training labels get value calls num get value calls get iterations maximizer gradient get iterations max ent optimizable label likelihood use gaussian prior using gaussian prior = using hyperbolic prior = max ent optimizable label likelihood use hyperbolic prior using gaussian prior = using hyperbolic prior = in some cases a prior term in optimizer eg orthant wise l b f g s so we occasionally want to only calculate log likelihood max ent optimizable label likelihood use no prior using gaussian prior = using hyperbolic prior = sets a parameter to prevent overtraining a smaller variance prior means that feature weights are expected to hover closer to 0 so extra evidence required to set a higher weight trainer max ent optimizable label likelihood set gaussian prior variance gaussian prior variance using gaussian prior = using hyperbolic prior = gaussian prior variance = gaussian prior variance max ent optimizable label likelihood set hyperbolic prior slope hyperbolic prior slope using gaussian prior = using hyperbolic prior = hyperbolic prior slope = hyperbolic prior slope max ent optimizable label likelihood set hyperbolic prior sharpness hyperbolic prior sharpness using gaussian prior = using hyperbolic prior = hyperbolic prior sharpness = hyperbolic prior sharpness 