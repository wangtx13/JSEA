2005 m a l l e t m achine languag e ~mccallum 1 0 further ` l i e n s e topics zip io text number format types command option randoms latent dirichlet allocation loosely parallel corpora in arbitrary languages author david mimno andrew mc callum polylingual topic model serializable command option spaced strings input files = command option spaced strings polylingual topic model inputs f i l e n a m e f i l e n a m e filenames polylingual topic model each should have its own + same number instances in each a document missing in + one there should be an empty instance command option output model filename = command option polylingual topic model output model f i l e n a m e filename in which to write binary topic model at end iterations + indicating that no will be written command option input model filename = command option polylingual topic model input model f i l e n a m e filename from which to read binary topic model to which input will be appended + allowing incremental training + indicating that no will be read command option inferencer filename = command option polylingual topic model inferencer filename f i l e n a m e a topic inferencer applies a previously trained topic model to documents + indicating that no will be written command option evaluator filename = command option polylingual topic model evaluator filename f i l e n a m e a held out likelihood evaluator documents + indicating that no will be written command option state = command option polylingual topic model output state f i l e n a m e filename in which to write gibbs sampling state after at end iterations + indicating that no will be written command option topic keys = command option polylingual topic model output topic keys f i l e n a m e filename in which to write top words each topic and any dirichlet + indicating that no will be written command option doc topics = command option polylingual topic model output doc topics f i l e n a m e filename in which to write topic proportions per document at end iterations + indicating that no will be written command option doc topics threshold = command option polylingual topic model doc topics threshold d e i m a l 0 0 when writing topic proportions per document output doc topics + not print topics proportions less than threshold value command option doc topics max = command option polylingual topic model doc topics max i n t e g e r 1 when writing topic proportions per document output doc topics + not print more than i n t e g e r number topics + a negative value indicates that all topics should be printed command option output model interval option = command option polylingual topic model output model interval i n t e g e r 0 number iterations between writing model and its gibbs sampling state to a binary + you must also set output model to use option whose argument will be prefix filenames command option output state interval option = command option polylingual topic model output state interval i n t e g e r 0 number iterations between writing sampling state to a text + you must also set output state to use option whose argument will be prefix filenames command option num topics option = command option polylingual topic model num topics i n t e g e r 10 number topics to fit command option num iterations option = command option polylingual topic model num iterations i n t e g e r 1000 number iterations gibbs sampling command option random seed option = command option polylingual topic model random seed i n t e g e r 0 random seed gibbs sampler 0 which will use clock command option top words option = command option polylingual topic model num top words i n t e g e r 20 number most probable words to print each topic after model estimation command option show topics interval option = command option polylingual topic model show topics interval i n t e g e r 50 number iterations between printing a brief summary topics so far command option optimize interval option = command option polylingual topic model optimize interval i n t e g e r 0 number iterations between reestimating dirichlet hyperparameters command option optimize burn in option = command option polylingual topic model optimize burn in i n t e g e r 200 number iterations to run before first estimating dirichlet hyperparameters command option alpha option = command option polylingual topic model alpha d e i m a l 50 0 alpha parameter smoothing over topic command option beta option = command option polylingual topic model beta d e i m a l 0 01 beta parameter smoothing over unigram topic assignment serializable instance instances label sequence topic sequences labeling topic topic assignment instance instances label sequence topic sequences instances = instances topic sequences = topic sequences num languages = 1 list< topic assignment> data training instances and their topic assignments label alphabet topic alphabet alphabet topics num stopwords = 0 num topics number topics to be fit hash set< string> testing ids = these values are used to encode type topic counts count topic pairs in a single topic mask topic bits alphabet alphabets vocabulary sizes alpha dirichlet alpha alpha over topics alpha sum betas prior on per topic multinomial over words beta sums max type counts d e f a u l t b e t a = 0 01 smoothing only masses cached coefficients topic term count = 0 beta topic count = 0 smoothing only count = 0 an to put topic counts current document initialized locally below defined here to avoid garbage collection overhead one doc topic counts indexed <document index topic index> type topic counts indexed <feature index topic index> tokens per topic indexed <topic index> dirichlet estimation doc length counts histogram document sizes summed over languages topic doc counts histogram document topic counts indexed <topic index sequence position index> iterations so far = 1 num iterations = 1000 burnin period = 5 save sample interval = 5 was 10 optimize interval = 10 show topics interval = 10 was 50 words per topic = 7 save model interval = 0 model filename save state interval = 0 state filename = randoms random number format formatter print log likelihood = polylingual topic model number topics number topics number topics polylingual topic model number topics alpha sum number topics alpha sum randoms label alphabet label alphabet num topics label alphabet ret = label alphabet i = 0 i < num topics i++ ret lookup index topic +i ret polylingual topic model number topics alpha sum randoms random label alphabet number topics alpha sum random polylingual topic model label alphabet topic alphabet alpha sum randoms random data = list< topic assignment> topic alphabet = topic alphabet num topics = topic alphabet size bit count num topics == 1 exact power 2 topic mask = num topics 1 topic bits = bit count topic mask otherwise add an extra bit topic mask = highest one bit num topics 2 1 topic bits = bit count topic mask alpha sum = alpha sum alpha = num topics arrays fill alpha alpha sum num topics random = random formatter = number format get instance formatter set maximum fraction digits 5 err polylingual l d a + num topics + topics + topic bits + topic bits + to binary topic mask + topic mask load testing ids testing i o testing ids = hash set buffered reader in = buffered reader reader testing = = in read line != testing ids add in close label alphabet get topic alphabet topic alphabet get num topics num topics list< topic assignment> get data data set num iterations num iterations num iterations = num iterations set burnin period burnin period burnin period = burnin period set topic display interval n show topics interval = interval words per topic = n set random seed seed random = randoms seed set optimize interval interval optimize interval = interval set model output interval filename save model interval = interval model filename = filename define how often and where to save state interval save a copy state every <code>interval< code> iterations filename save state to iteration number a suffix set save state interval filename save state interval = interval state filename = filename add instances instance list training num languages = training length tokens per topic = num languages num topics alphabets = alphabet num languages vocabulary sizes = num languages betas = num languages beta sums = num languages max type counts = num languages type topic counts = num languages num instances = training 0 size hash set stoplists = hash set num languages = 0 < num languages language++ training size != num instances err warning + + has + training size + instances 0 has + num instances alphabets = training get data alphabet vocabulary sizes = alphabets size betas = d e f a u l t b e t a beta sums = betas vocabulary sizes type topic counts = vocabulary sizes type topic counts = type topic counts get total number occurrences each word type type totals = vocabulary sizes instance instance training testing ids != testing ids contains instance get name feature sequence tokens = feature sequence instance get data position = 0 position < tokens get length position++ type = tokens get index at position position type totals type ++ automatic stoplist creation currently disabled tree set< sorter> sorted words = tree set< sorter> type = 0 type < vocabulary sizes type++ sorted words add sorter type type totals type stoplists = hash set< integer> iterator< sorter> type iterator = sorted words iterator total stopwords = 0 type iterator has next total stopwords < num stopwords stoplists add type iterator next get allocate enough space so that we never have to worry about overflows either number topics or number times type occurs type = 0 type < vocabulary sizes type++ type totals type > max type counts max type counts = type totals type type topic counts type = math min num topics type totals type doc = 0 doc < num instances doc++ testing ids != testing ids contains training 0 get doc get name instance instances = instance num languages label sequence topic sequences = label sequence num languages = 0 < num languages language++ type topic counts = type topic counts tokens per topic = tokens per topic instances = training get doc feature sequence tokens = feature sequence instances get data topic sequences = label sequence topic alphabet tokens size topics = topic sequences get features position = 0 position < tokens size position++ type = tokens get index at position position current type topic counts = type topic counts type topic = random next num topics word one num stopwords most frequent words put it in a non sampled topic stoplists contains type topic = 1 topics position = topic tokens per topic topic ++ format these arrays topic in rightmost bits count in remaining left bits since count in high bits sorting desc numeric value guarantees that higher counts will be before lower counts start assuming that either empty or in sorted descending order here we are only adding counts so we find an existing location topic we only need to ensure that it not larger than its left neighbor index = 0 current topic = current type topic counts index topic mask current value current type topic counts index > 0 current topic != topic index++ debugging output index >= current type topic counts length i=0 i < current type topic counts length i++ out current type topic counts i topic mask + + current type topic counts i >> topic bits + out type + + type totals type current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits current value == 0 value 1 so we t have to worry about sorting except topic suffix which doesn t matter current type topic counts index = 1 << topic bits + topic current type topic counts index = current value + 1 << topic bits + topic now ensure that still sorted bubbling value up index > 0 current type topic counts index > current type topic counts index 1 temp = current type topic counts index current type topic counts index = current type topic counts index 1 current type topic counts index 1 = temp index topic assignment t = topic assignment instances topic sequences data add t initialize histograms smoothing only masses = num languages cached coefficients = num languages num topics cache values gather statistics on size documents and create histograms use in dirichlet hyperparameter optimization initialize histograms max tokens = 0 total tokens = 0 doc = 0 doc < data size doc++ length = 0 label sequence sequence data get doc topic sequences length += sequence get length length > max tokens max tokens = length total tokens += length err max tokens + max tokens err total tokens + total tokens doc length counts = max tokens + 1 topic doc counts = num topics max tokens + 1 cache values = 0 < num languages language++ smoothing only masses = 0 0 topic=0 topic < num topics topic++ smoothing only masses += alpha topic betas tokens per topic topic + beta sums cached coefficients topic = alpha topic tokens per topic topic + beta sums clear histograms arrays fill doc length counts 0 topic = 0 topic < topic doc counts length topic++ arrays fill topic doc counts topic 0 estimate i o estimate num iterations estimate iterations round i o start time = current time millis max iteration = iterations so far + iterations round total time = 0 iterations so far <= max iteration iterations so far++ iteration start = current time millis show topics interval != 0 iterations so far != 0 iterations so far % show topics interval == 0 out print top words out words per topic save state interval != 0 iterations so far % save state interval == 0 print state state filename + + iterations so far save model interval != 0 iterations % save model interval == 0 write model filename+ +iterations t o d o condition should also check that we have more than one sample to work here number samples actually obtained not yet tracked iterations so far > burnin period optimize interval != 0 iterations so far % optimize interval == 0 alpha sum = dirichlet learn alpha topic doc counts doc length counts optimize betas clear histograms cache values loop over every document in corpus topic term count = beta topic count = smoothing only count = 0 doc = 0 doc < data size doc++ sample topics one doc data get doc iterations so far >= burnin period iterations so far % save sample interval == 0 elapsed millis = current time millis iteration start total time += elapsed millis iterations so far + 1 % 10 == 0 ll = model log likelihood out elapsed millis + + total time + + ll out print elapsed millis + seconds = math round current time millis start time 1000 0 minutes = seconds 60 seconds %= 60 hours = minutes 60 minutes %= 60 days = hours 24 hours %= 24 out print total time days != 0 out print days out print days hours != 0 out print hours out print hours minutes != 0 out print minutes out print minutes out print seconds out seconds optimize betas = 0 < num languages language++ histogram starts at count 0 so all tokens most frequent type were assigned to one topic we would need to store a max type count + 1 count count histogram = max type counts + 1 now count number type topic pairs that have each number tokens type topic counts = type topic counts tokens per topic = tokens per topic index type = 0 type < vocabulary sizes type++ counts = type topic counts type index = 0 index < counts length counts index > 0 count = counts index >> topic bits count histogram count ++ index++ figure out how large we need to make observation lengths histogram max topic size = 0 topic = 0 topic < num topics topic++ tokens per topic topic > max topic size max topic size = tokens per topic topic now allocate it and populate it topic size histogram = max topic size + 1 topic = 0 topic < num topics topic++ topic size histogram tokens per topic topic ++ beta sums = dirichlet learn symmetric concentration count histogram topic size histogram vocabulary sizes beta sums betas = beta sums vocabulary sizes sample topics one doc topic assignment topic assignment should save state current type topic counts type old topic topic topic weights sum local topic counts = num topics local topic index = num topics = 0 < num languages language++ one doc topics = topic assignment topic sequences get features doc length = topic assignment topic sequences get length populate topic counts position = 0 position < doc length position++ local topic counts one doc topics position ++ build an that densely lists topics that have non zero counts dense index = 0 topic = 0 topic < num topics topic++ local topic counts topic != 0 local topic index dense index = topic dense index++ record total number non zero topics non zero topics = dense index = 0 < num languages language++ one doc topics = topic assignment topic sequences get features doc length = topic assignment topic sequences get length feature sequence token sequence = feature sequence topic assignment instances get data type topic counts = type topic counts tokens per topic = tokens per topic beta = betas beta sum = beta sums initialize smoothing only sampling bucket smoothing only mass = smoothing only masses topic = 0 topic < num topics topic++ smoothing only mass += alpha topic beta tokens per topic topic + beta sum initialize cached coefficients using only smoothing cached coefficients = num topics topic=0 topic < num topics topic++ cached coefficients topic = alpha topic tokens per topic topic + beta sum cached coefficients = cached coefficients initialize topic count beta sampling bucket topic beta mass = 0 0 initialize cached coefficients and topic beta normalizing constant dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index n = local topic counts topic initialize normalization constant b n t|d term topic beta mass += beta n tokens per topic topic + beta sum update coefficients non zero topics cached coefficients topic = alpha topic + n tokens per topic topic + beta sum topic term mass = 0 0 topic term scores = num topics topic term indices topic term values i score iterate over positions words in document position = 0 position < doc length position++ type = token sequence get index at position position old topic = one doc topics position old topic == 1 current type topic counts = type topic counts type remove token from all counts remove topic s contribution to normalizing constants smoothing only mass = alpha old topic beta tokens per topic old topic + beta sum topic beta mass = beta local topic counts old topic tokens per topic old topic + beta sum decrement local doc topic counts local topic counts old topic maintain dense index we are deleting old topic local topic counts old topic == 0 first get to dense location associated old topic dense index = 0 we know it s in there somewhere so we t need bounds checking local topic index dense index != old topic dense index++ shift all remaining dense indices to left dense index < non zero topics dense index < local topic index length 1 local topic index dense index = local topic index dense index + 1 dense index++ non zero topics decrement global topic count totals tokens per topic old topic tokens per topic old topic >= 0 old topic + old topic + below 0 add old topic s contribution back into normalizing constants smoothing only mass += alpha old topic beta tokens per topic old topic + beta sum topic beta mass += beta local topic counts old topic tokens per topic old topic + beta sum reset cached coefficient topic cached coefficients old topic = alpha old topic + local topic counts old topic tokens per topic old topic + beta sum now go over type topic counts decrementing where appropriate and calculating score each topic at same time index = 0 current topic current value already decremented = topic term mass = 0 0 index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits ! already decremented current topic == old topic we re decrementing and adding up sampling weights at same time but decrementing may require us to reorder topics so after we re done here look at cell in again current value current value == 0 current type topic counts index = 0 current type topic counts index = current value << topic bits + old topic shift reduced value to right necessary sub index = index sub index < current type topic counts length 1 current type topic counts sub index < current type topic counts sub index + 1 temp = current type topic counts sub index current type topic counts sub index = current type topic counts sub index + 1 current type topic counts sub index + 1 = temp sub index++ already decremented = score = cached coefficients current topic current value topic term mass += score topic term scores index = score index++ sample = random next uniform smoothing only mass + topic beta mass + topic term mass orig sample = sample make sure it actually gets set topic = 1 sample < topic term mass topic term count++ i = 1 sample > 0 i++ sample = topic term scores i topic = current type topic counts i topic mask current value = current type topic counts i >> topic bits current type topic counts i = current value + 1 << topic bits + topic bubble value up necessary i > 0 current type topic counts i > current type topic counts i 1 temp = current type topic counts i current type topic counts i = current type topic counts i 1 current type topic counts i 1 = temp i sample = topic term mass sample < topic beta mass beta topic count++ sample = beta dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index sample = local topic counts topic tokens per topic topic + beta sum sample <= 0 0 topic = topic smoothing only count++ sample = topic beta mass sample = beta topic = 0 sample = alpha topic tokens per topic topic + beta sum sample > 0 0 topic++ sample = alpha topic tokens per topic topic + beta sum move to position topic which may be first empty position a topic word index = 0 current type topic counts index > 0 current type topic counts index topic mask != topic index++ index should now be set to position topic which may be an empty cell at end list current type topic counts index == 0 inserting a topic guaranteed to be in order w r t count not topic current type topic counts index = 1 << topic bits + topic current value = current type topic counts index >> topic bits current type topic counts index = current value + 1 << topic bits + topic bubble increased value left necessary index > 0 current type topic counts index > current type topic counts index 1 temp = current type topic counts index current type topic counts index = current type topic counts index 1 current type topic counts index 1 = temp index topic == 1 err polylingual topic model sampling + orig sample + + sample + + smoothing only mass + + topic beta mass + + topic term mass topic = num topics 1 t o d o appropriate illegal state polylingual topic model topic not sampled topic != 1 put that topic into counts one doc topics position = topic smoothing only mass = alpha topic beta tokens per topic topic + beta sum topic beta mass = beta local topic counts topic tokens per topic topic + beta sum local topic counts topic ++ a topic document add topic to dense index local topic counts topic == 1 first find point where we should insert topic going to end which only reason we re keeping track number non zero topics and working backwards dense index = non zero topics dense index > 0 local topic index dense index 1 > topic local topic index dense index = local topic index dense index 1 dense index local topic index dense index = topic non zero topics++ tokens per topic topic ++ update coefficients non zero topics cached coefficients topic = alpha topic + local topic counts topic tokens per topic topic + beta sum smoothing only mass += alpha topic beta tokens per topic topic + beta sum topic beta mass += beta local topic counts topic tokens per topic topic + beta sum save smoothing only mass to global cache smoothing only masses = smoothing only mass should save state update document topic count histogram dirichlet estimation total length = 0 dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index topic doc counts topic local topic counts topic ++ total length += local topic counts topic doc length counts total length ++ print top words num words use lines i o print stream out = print stream print top words out num words use lines out close print top words print stream out num words using lines tree set topic sorted words = tree set num languages num topics = 0 < num languages language++ tree set topic sorted words = topic sorted words type topic counts = type topic counts topic = 0 topic < num topics topic++ topic sorted words topic = tree set< sorter> type = 0 type < vocabulary sizes type++ topic counts = type topic counts type index = 0 index < topic counts length topic counts index > 0 topic = topic counts index topic mask count = topic counts index >> topic bits topic sorted words topic add sorter type count index++ topic = 0 topic < num topics topic++ out topic + + formatter format alpha topic = 0 < num languages language++ out print + + + tokens per topic topic + + betas + tree set< sorter> sorted words = topic sorted words topic alphabet alphabet = alphabets word = 1 iterator< sorter> iterator = sorted words iterator iterator has next word < num words sorter info = iterator next out print alphabet lookup info get + word++ out print document topics f i o print document topics print writer f u t f 8 print document topics print writer pw print document topics pw 0 0 1 pw a print writer threshold only print topics proportion greater than number max print no more than many topics print document topics print writer pw threshold max pw print #doc source topic proportion doc length topic counts = num topics sorter sorted topics = sorter num topics topic = 0 topic < num topics topic++ initialize sorters dummy values sorted topics topic = sorter topic topic max < 0 || max > num topics max = num topics di = 0 di < data size di++ pw print di pw print total length = 0 = 0 < num languages language++ label sequence topic sequence = label sequence data get di topic sequences current doc topics = topic sequence get features doc length = topic sequence get length total length += doc length count up tokens token=0 token < doc length token++ topic counts current doc topics token ++ and normalize topic = 0 topic < num topics topic++ sorted topics topic set topic topic counts topic total length arrays sort sorted topics i = 0 i < max i++ sorted topics i get weight < threshold pw print sorted topics i get + + sorted topics i get weight + pw print arrays fill topic counts 0 print state f i o print stream out = print stream g z i p output stream buffered output stream output stream f u t f 8 print state out out close print state print stream out out #doc pos typeindex type topic doc = 0 doc < data size doc++ =0 < num languages language++ feature sequence token sequence = feature sequence data get doc instances get data label sequence topic sequence = label sequence data get doc topic sequences pi = 0 pi < topic sequence get length pi++ type = token sequence get index at position pi topic = topic sequence get index at position pi out print doc out print out print out print out print pi out print out print type out print out print alphabets lookup type out print out print topic out model log likelihood log likelihood = 0 0 non zero topics likelihood model a combination a dirichlet multinomial words in each topic and a dirichlet multinomial topics in each document likelihood function a dirichlet multinomial gamma sum i alpha i prod i gamma alpha i + n i prod i gamma alpha i gamma sum i alpha i + n i so log likelihood log gamma sum i alpha i log gamma sum i alpha i + n i + sum i log gamma alpha i + n i log gamma alpha i documents first topic counts = num topics topic log gammas = num topics doc topics topic=0 topic < num topics topic++ topic log gammas topic = dirichlet log gamma stirling alpha topic doc=0 doc < data size doc++ total length = 0 = 0 < num languages language++ label sequence topic sequence = label sequence data get doc topic sequences current doc topics = topic sequence get features total length += topic sequence get length count up tokens token=0 token < topic sequence get length token++ topic counts current doc topics token ++ topic=0 topic < num topics topic++ topic counts topic > 0 log likelihood += dirichlet log gamma stirling alpha topic + topic counts topic topic log gammas topic subtract count + parameter sum term log likelihood = dirichlet log gamma stirling alpha sum + total length arrays fill topic counts 0 add parameter sum term log likelihood += data size dirichlet log gamma stirling alpha sum and topics = 0 < num languages language++ type topic counts = type topic counts tokens per topic = tokens per topic beta = betas count number type topic pairs non zero type topics = 0 type=0 type < vocabulary sizes type++ reuse a pointer topic counts = type topic counts type index = 0 index < topic counts length topic counts index > 0 topic = topic counts index topic mask count = topic counts index >> topic bits non zero type topics++ log likelihood += dirichlet log gamma stirling beta + count na n log likelihood out count exit 1 index++ topic=0 topic < num topics topic++ log likelihood = dirichlet log gamma stirling beta num topics + tokens per topic topic na n log likelihood out after topic + topic + + tokens per topic topic exit 1 log likelihood += dirichlet log gamma stirling beta num topics dirichlet log gamma stirling beta non zero type topics na n log likelihood out at end exit 1 log likelihood a tool estimating topic distributions documents topic inferencer get inferencer topic inferencer type topic counts tokens per topic alphabets alpha betas beta sums serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write num languages out write data out write topic alphabet out write num topics out write testing ids out write topic mask out write topic bits out write alphabets out write vocabulary sizes out write alpha out write alpha sum out write betas out write beta sums out write max type counts out write type topic counts out write tokens per topic out write smoothing only masses out write cached coefficients out write doc length counts out write topic doc counts out write num iterations out write burnin period out write save sample interval out write optimize interval out write show topics interval out write words per topic out write save state interval out write state filename out write save model interval out write model filename out write random out write formatter out write print log likelihood read input stream in i o not found = in read num languages = in read data = list< topic assignment> in read topic alphabet = label alphabet in read num topics = in read testing ids = hash set< string> in read topic mask = in read topic bits = in read alphabets = alphabet in read vocabulary sizes = in read alpha = in read alpha sum = in read betas = in read beta sums = in read max type counts = in read type topic counts = in read tokens per topic = in read smoothing only masses = in read cached coefficients = in read doc length counts = in read topic doc counts = in read num iterations = in read burnin period = in read save sample interval = in read optimize interval = in read show topics interval = in read words per topic = in read save state interval = in read state filename = in read save model interval = in read model filename = in read random = randoms in read formatter = number format in read print log likelihood = in read write serialized model output stream oos = output stream output stream serialized model oos write oos close i o e err problem serializing polylingual topic model to + serialized model + + e polylingual topic model read f polylingual topic model topic model = input stream ois = input stream input stream f topic model = polylingual topic model ois read ois close topic model initialize histograms topic model i o command option set summary polylingual topic model a tool estimating saving and printing diagnostics topic models over comparable corpora command option process polylingual topic model polylingual topic model topic model = input model filename value != topic model = polylingual topic model read input model filename value e err unable to restore saved topic model + input model filename value + + e exit 1 num languages = input files value length instance list training = instance list num languages i=0 i < training length i++ training i = instance list load input files value i training i != out i + not out i + out data loaded historical reasons we currently only support feature sequence data not feature vector which input functions provide a warning to avoid cast exceptions training 0 size > 0 training 0 get 0 != data = training 0 get 0 get data ! data feature sequence err topic modeling currently only supports feature sequences use keep sequence option when importing data exit 1 topic model = polylingual topic model num topics option value alpha option value random seed option value != 0 topic model set random seed random seed option value topic model add instances training topic model set topic display show topics interval option value top words option value topic model set num iterations num iterations option value topic model set optimize interval optimize interval option value topic model set burnin period optimize burn in option value output state interval option value != 0 topic model set save state output state interval option value state value output model interval option value != 0 topic model set model output output model interval option value output model filename value topic model estimate topic keys value != topic model print top words topic keys value top words option value state value != topic model print state state value doc topics value != print writer out = print writer writer doc topics value topic model print document topics out doc topics threshold value doc topics max value out close inferencer filename value != = 0 < topic model num languages language++ output stream oos = output stream output stream inferencer filename value + + oos write topic model get inferencer oos close e err e get message output model filename value != topic model != topic model write output model filename value 