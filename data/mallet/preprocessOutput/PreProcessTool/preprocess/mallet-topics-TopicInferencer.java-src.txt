topics types arrays io topic inferencer serializable num topics these values are used to encode type topic counts count topic pairs in a single topic mask topic bits num types alpha beta beta sum type topic counts tokens per topic alphabet alphabet randoms random = smoothing only mass = 0 0 cached coefficients topic inferencer type topic counts tokens per topic alphabet alphabet alpha beta beta sum tokens per topic = tokens per topic type topic counts = type topic counts alphabet = alphabet num topics = tokens per topic length num types = type topic counts length bit count num topics == 1 exact power 2 topic mask = num topics 1 topic bits = bit count topic mask otherwise add an extra bit topic mask = highest one bit num topics 2 1 topic bits = bit count topic mask alpha = alpha beta = beta beta sum = beta sum cached coefficients = num topics topic=0 topic < num topics topic++ smoothing only mass += alpha topic beta tokens per topic topic + beta sum cached coefficients topic = alpha topic tokens per topic topic + beta sum random = randoms set random seed seed random = randoms seed use gibbs sampling to infer a topic topics are initialized to or a most probable topic each token using zero iterations exactly initial topic <p > does not adjust type topic counts p w|t clamped get sampled instance instance num iterations thinning burn in feature sequence tokens = feature sequence instance get data doc length = tokens size topics = doc length local topic counts = num topics local topic index = num topics type current type topic counts initialize all positions to most topic that type position = 0 position < doc length position++ type = tokens get index at position position ignore out vocabulary type < num types type topic counts type length != 0 current type topic counts = type topic counts type value should be a topic such that no other topic has more tokens type assigned to it some reason there were no tokens type in training data it will to topic 0 which no worse than random initialization topics position = current type topic counts 0 topic mask local topic counts topics position ++ build an that densely lists topics that have non zero counts dense index = 0 topic = 0 topic < num topics topic++ local topic counts topic != 0 local topic index dense index = topic dense index++ record total number non zero topics non zero topics = dense index initialize topic count beta sampling bucket topic beta mass = 0 0 initialize cached coefficients and topic beta normalizing constant dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index n = local topic counts topic initialize normalization constant b n t|d term topic beta mass += beta n tokens per topic topic + beta sum update coefficients non zero topics cached coefficients topic = alpha topic + n tokens per topic topic + beta sum topic term mass = 0 0 topic term scores = num topics topic term indices topic term values i score old topic topic result = num topics sum = 0 0 iteration = 1 iteration <= num iterations iteration++ iterate over positions words in document position = 0 position < doc length position++ type = tokens get index at position position ignore out vocabulary type >= num types || type topic counts type length == 0 old topic = topics position current type topic counts = type topic counts type prepare to sample adjusting existing counts note that we not need to change smoothing only mass since denominator clamped topic beta mass = beta local topic counts old topic tokens per topic old topic + beta sum decrement local doc topic counts local topic counts old topic local topic counts old topic >= 0 maintain dense index we are deleting old topic local topic counts old topic == 0 first get to dense location associated old topic dense index = 0 we know it s in there somewhere so we t need bounds checking local topic index dense index != old topic dense index++ shift all remaining dense indices to left dense index < non zero topics dense index < local topic index length 1 local topic index dense index = local topic index dense index + 1 dense index++ non zero topics finished maintaining local topic index topic beta mass += beta local topic counts old topic tokens per topic old topic + beta sum reset cached coefficient topic cached coefficients old topic = alpha old topic + local topic counts old topic tokens per topic old topic + beta sum cached coefficients old topic <= 0 out zero or less coefficient + old topic + = + alpha old topic + + + local topic counts old topic + + tokens per topic old topic + + + beta sum + index = 0 current topic current value already decremented = topic term mass = 0 0 index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits score = cached coefficients current topic current value topic term mass += score topic term scores index = score index++ sample = random next uniform smoothing only mass + topic beta mass + topic term mass orig sample = sample make sure it actually gets set topic = 1 sample < topic term mass topic term count++ i = 1 sample > 0 i++ sample = topic term scores i topic = current type topic counts i topic mask sample = topic term mass sample < topic beta mass beta topic count++ sample = beta dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index sample = local topic counts topic tokens per topic topic + beta sum sample <= 0 0 topic = topic sample = topic beta mass sample = beta topic = 0 sample = alpha topic tokens per topic topic + beta sum sample > 0 0 topic++ topic >= num topics index = 0 index < current type topic counts length current type topic counts index > 0 current topic = current type topic counts index topic mask current value = current type topic counts index >> topic bits out current topic + + current value + + topic term scores index + + cached coefficients current topic index++ sample = alpha topic tokens per topic topic + beta sum topics position = topic topic beta mass = beta local topic counts topic tokens per topic topic + beta sum local topic counts topic ++ a topic document add topic to dense index local topic counts topic == 1 first find point where we should insert topic going to end which only reason we re keeping track number non zero topics and working backwards dense index = non zero topics dense index > 0 local topic index dense index 1 > topic local topic index dense index = local topic index dense index 1 dense index local topic index dense index = topic non zero topics++ update coefficients non zero topics cached coefficients topic = alpha topic + local topic counts topic tokens per topic topic + beta sum topic beta mass += beta local topic counts topic tokens per topic topic + beta sum iteration > burn in iteration burn in % thinning == 0 save a sample topic=0 topic < num topics topic++ result topic += alpha topic + local topic counts topic sum += alpha topic + local topic counts topic clean up our mess reset coefficients to values only smoothing next doc will update its own non zero topics dense index = 0 dense index < non zero topics dense index++ topic = local topic index dense index cached coefficients topic = alpha topic tokens per topic topic + beta sum sum == 0 0 save at least one sample topic=0 topic < num topics topic++ result topic = alpha topic + local topic counts topic sum += result topic normalize topic=0 topic < num topics topic++ result topic = sum result infer topics instances and write distributions to instances distributions num iterations total number iterations sampling per document thinning number iterations between saved samples burn in number iterations before first saved sample threshold minimum proportion a given topic that will be written max total number topics to report per document write inferred distributions instance list instances distributions num iterations thinning burn in threshold max i o print writer out = print writer distributions out print #doc name topic proportion sorter sorted topics = sorter num topics topic = 0 topic < num topics topic++ initialize sorters dummy values sorted topics topic = sorter topic topic max < 0 || max > num topics max = num topics doc = 0 instance instance instances builder builder = builder topic = get sampled instance num iterations thinning burn in builder append doc builder append instance get name != builder append instance get name builder append no name threshold > 0 0 topic = 0 topic < num topics topic++ sorted topics topic set topic topic topic arrays sort sorted topics i = 0 i < max i++ sorted topics i get weight < threshold builder append + sorted topics i get + + sorted topics i get weight topic = 0 topic < num topics topic++ builder append + topic topic out builder doc++ out close serialization serial u = 1 u r r e n t s e r i a l v e r s i o n = 0 n u l l i n t e g e r = 1 write output stream out i o out write u r r e n t s e r i a l v e r s i o n out write alphabet out write num topics out write topic mask out write topic bits out write num types out write alpha out write beta out write beta sum out write type topic counts out write tokens per topic out write random out write smoothing only mass out write cached coefficients read input stream in i o not found = in read alphabet = alphabet in read num topics = in read topic mask = in read topic bits = in read num types = in read alpha = in read beta = in read beta sum = in read type topic counts = in read tokens per topic = in read random = randoms in read smoothing only mass = in read cached coefficients = in read topic inferencer read f topic inferencer inferencer = input stream ois = input stream input stream f inferencer = topic inferencer ois read ois close inferencer 